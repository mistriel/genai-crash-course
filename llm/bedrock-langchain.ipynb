{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# LangChain\n",
    "LangChain is a framework for developing applications powered by language models. </br>\n",
    "It provides a standard interface for working with different LLM models, as well as tools for chaining together multiple LLM calls, managing memory, and more. </br>\n",
    "\n",
    "## Key Features\n",
    "- **Standard Interface**: LangChain provides a consistent interface for working with different LLMs, making it easier to switch between models.\n",
    "- **Chaining**: You can chain together multiple LLM calls to create complex workflows.\n",
    "- **Memory Management**: LangChain provides tools for managing memory, allowing you to store and retrieve information across multiple LLM calls.\n",
    "- **Tools and Utilities**: LangChain includes a variety of tools and utilities for working with LLMs, such as tokenizers, embeddings, and more.\n",
    "\n",
    "## Architecture\n",
    "LangChain's architecture is designed to be modular and extensible. It consists of several main packages:\n",
    "- **Core**: The core package provides the basic building blocks for working with LLMs, including the standard interface and utilities for chaining and memory management.\n",
    "- **langchain**: This package contains the main LangChain functionality, including the standard interface for LLMs, tools for chaining, and memory management.\n",
    "- **langchain_community**: This package contains community-contributed tools and utilities for working with LLMs, such as additional tokenizers, embeddings, and more.\n",
    "- **Integrations**: LangChain integrates with various LLM providers, allowing you to easily switch between different vendors.\n",
    "\n",
    "\n",
    "References:\n",
    "\n",
    "- https://python.langchain.com/docs/concepts/architecture/"
   ],
   "id": "84c337a228cd5180"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prompt Templates\n",
    "Prompt templates are a way to define the structure of a prompt that will be sent to an LLM. </br>\n",
    "They allow you to create reusable templates that can be filled in with specific values at runtime.\n",
    "\n",
    "### Types of Prompt Templates\n",
    "\n",
    "- **Simple Prompt Template**: A basic template that takes a single input and generates a prompt.\n",
    "- **Chat Prompt Template**: A template designed for chat-based interactions, allowing for multiple messages and roles.\n",
    "- **Few-Shot Prompt Template**: A template that includes examples of input-output pairs to guide the LLM's response.\n",
    "- **Custom Prompt Template**: A template that allows for more complex structures and custom formatting.\n",
    "- **Prompt Template with Variables**: A template that includes variables that can be filled in with specific values at runtime.\n",
    "- **Prompt Template with Conditionals**: A template that includes conditional logic to generate different prompts based on specific conditions."
   ],
   "id": "5aaa2ce4476f696"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T07:56:58.617893Z",
     "start_time": "2025-05-29T07:56:57.574028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import necessary libraries and Initialization steps\n",
    "#!pip install langchain>=0.3 langchain-community>=0.3 langchain-aws>=0.2 boto3==1.38.15 pydantic==2.10.4 faiss-cpu==1.11.0\n",
    "from enum import Enum\n",
    "from os import times\n",
    "\n",
    "import boto3\n",
    "from langchain.retrievers.multi_query import LineListOutputParser\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "\n",
    "\n",
    "class LLMModel(Enum):\n",
    "    \"\"\"Enum for Bedrock models.\"\"\"\n",
    "    # Anthropic\n",
    "    CLAUDE_3_5_V1 = 'anthropic.claude-3-5-sonnet-20240620-v1:0'\n",
    "    CLAUDE_3_5_v2 = 'us.anthropic.claude-3-5-sonnet-20241022-v2:0' # Inference Profile ID\n",
    "    # Amazon\n",
    "    NOVA_LITE = 'amazon.nova-lite-v1:0'\n",
    "    NOVA_PRO = 'amazon.nova-pro-v1:0'\n",
    "    TITAN_LITE = 'amazon.titan-text-lite-v1'\n",
    "    TITAN_EXPRESS = 'amazon.titan-text-express-v1'\n",
    "    META_LLMA3_1B = 'meta.llama3-2-1b-instruct-v1:0'\n",
    "    META_LLMA3_3B = 'meta.llama3-2-3b-instruct-v1:0'\n",
    "\n",
    "llm = ChatBedrockConverse(\n",
    "    client=boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\"),\n",
    "    model = str(LLMModel.NOVA_PRO.value),\n",
    "    max_tokens=4096,\n",
    "    temperature=0.0,\n",
    ")"
   ],
   "id": "9faa7561651fef9d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T07:57:02.494129Z",
     "start_time": "2025-05-29T07:56:59.113841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import HumanMessagePromptTemplate, ChatPromptTemplate, AIMessagePromptTemplate\n",
    "\n",
    "import json\n",
    "\n",
    "system_message = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a helpful assistant that provides information about mobile phones available in the inventory.\n",
    "    The inventory is provided in JSON format. Use the information to answer customer queries.\n",
    "    The inventory is as follows:\n",
    "\n",
    "    {inventory}\n",
    "\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "assistant_message = AIMessagePromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Recommended guidelines for the assistant behavior:\n",
    "    - Do not recommend phones that are not available in the inventory.\n",
    "    - Do not recommend expensive phones beyond the customer's budget.\n",
    "    - Do not recommend phones that do not meet the customer's requirements.\n",
    "    - Suggest maximum 3 phones.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "human_message = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Customer: {customer_query}\n",
    "    \"\"\"\n",
    ")\n",
    "# Create a chat prompt template\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [system_message, assistant_message, human_message]\n",
    ")\n",
    "\n",
    "with open('resources/mobile-phones-inventory.json', 'r') as file:\n",
    "    inventory = json.load(file)\n",
    "\n",
    "customer_query = \"I need a Android phone with a budget of maximum $500. Camera should be 50MP Wide, 8GB RAM, What do you have available?\"\n",
    "\n",
    "final_prompt = chat_prompt_template.format_messages(\n",
    "    inventory=json.dumps(inventory, indent=2),\n",
    "    customer_query=customer_query\n",
    ")\n",
    "\n",
    "print(llm.invoke(final_prompt).content)\n",
    "\n"
   ],
   "id": "d4e9da95255f705d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the phones that are available in your budget:\n",
      "- Aura Lite\n",
      "- Luminory Glow\n",
      "- Echo Prime\n",
      "- Zenith Z\n",
      "- Echo Standard\n",
      "\n",
      "If you have any additional requirements, please let me know.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Output Parsers\n",
    "Output parsers are used to process the output from an LLM and convert it into a structured format. </br>\n",
    "The combination of Pydantic and LangChain allows you to define data models that can be used to validate and parse the output from an LLM."
   ],
   "id": "d72482c2d6985190"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "class PhoneRecommendation(BaseModel):\n",
    "    \"\"\"Model for phone recommendation.\"\"\"\n",
    "    brand: str\n",
    "    model: str\n",
    "    price: float\n",
    "    camera: str\n",
    "    ram: str\n",
    "\n",
    "parser =PydanticOutputParser(pydantic_object=PhoneRecommendation)\n",
    "\n",
    "system_message = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a helpful assistant that provides information about mobile phones available in the inventory.\n",
    "    The inventory is provided in JSON format. Use the information to answer customer queries.\n",
    "    The inventory is as follows:\n",
    "\n",
    "    {inventory}\n",
    "\n",
    "    ---------------------\n",
    "\n",
    "    The output should be a JSON array of phone recommendations, each recommendation should be a JSON object with the following fields:\n",
    "\n",
    "    {format_instructions}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    ")\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [system_message, assistant_message, human_message]\n",
    ")\n",
    "final_prompt = chat_prompt_template.format_messages(\n",
    "    inventory=json.dumps(inventory, indent=2),\n",
    "    customer_query=customer_query,\n",
    "    format_instructions=parser.get_format_instructions()\n",
    ")\n",
    "\n",
    "print(llm.invoke(final_prompt).content)\n",
    "\n"
   ],
   "id": "4eb61c2ac04b45d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Memory\n",
    "Memory in LangChain allows you to store historical messages and context across multiple interactions with an LLM. </br>\n",
    "This is particularly useful for chat-based applications where you want to maintain **context** & **continuity** in conversations.\n",
    "\n",
    "### Types of Memory\n",
    "- **In-Memory Memory**: Stores messages in memory, suitable for short-lived applications.\n",
    "- **Persistent Memory**: Stores messages in a database or file system, suitable for long-lived applications.\n",
    "- **Custom Memory**: Allows you to implement your own memory management logic, suitable for complex applications.\n"
   ],
   "id": "5aa79b5fa74f7635"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "import uuid\n",
    "\n",
    "# MessagesPlaceholder is used to dynamically include the message history in the prompt.\n",
    "history_message = MessagesPlaceholder(variable_name=\"history\")\n",
    "\n",
    "# Create a chat prompt template with system, history, and human messages\n",
    "# And the partial method is used to fill in the inventory data that is known at the time of creating the prompt template.\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([system_message, assistant_message, history_message, human_message]).partial(\n",
    "    inventory=json.dumps(inventory, indent=2)\n",
    ")\n",
    "\n",
    "# Combine the chat prompt template with the LLM to create a runnable\n",
    "# The pipe operator (|) is used to chain operations, where the output of chat_prompt_template (partial formatted messages) becomes the input for llm.\n",
    "# This allows seamless integration of the prompt template with the language model for generating responses.\n",
    "runnable = chat_prompt_template | llm\n",
    "\n",
    "# Create a message history store (in memory for this example)\n",
    "sessions = {}\n",
    "\n",
    "# create sessions with unique session IDs to manage conversation history\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in sessions:\n",
    "        sessions[session_id] = InMemoryChatMessageHistory()\n",
    "    return sessions[session_id]\n",
    "\n",
    "# Use the runnable with message history\n",
    "chat_with_history = RunnableWithMessageHistory(\n",
    "    runnable,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"customer_query\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "def interactive_chat():\n",
    "    session_id = str(uuid.uuid4())\n",
    "\n",
    "    response = None\n",
    "    while True:\n",
    "        # Set the AI message to the initial greeting or the last response\n",
    "        ai_message = response.content if response else \"Sales Assistant: Hello! I'm your phone sales representative. How can I help you today?\"\n",
    "        # Get the user query\n",
    "        user_input = input(ai_message)\n",
    "\n",
    "        # Check if user wants to exit\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"\\nSales Assistant: Thank you for shopping with us! Have a great day!\")\n",
    "            break\n",
    "\n",
    "        # Get response using the runnable with history\n",
    "        response = chat_with_history.invoke(\n",
    "            {\"customer_query\": user_input},\n",
    "            {\"configurable\": {\"session_id\": session_id}}\n",
    "        )\n",
    "\n",
    "        print(\"Tokens used so far:\", response.usage_metadata['total_tokens'])\n",
    "\n",
    "# Run the interactive conversation\n",
    "interactive_chat()\n"
   ],
   "id": "da205dc17e830940",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Retrieval-Augmented Generation (RAG)\n",
    "Retrieval-Augmented Generation (RAG) is a technique that combines the power of large language models (LLMs) with external knowledge sources to improve the quality and relevance of generated responses. </br>\n",
    "\n",
    "The process of creating a RAG application typically involves the following steps:\n",
    "1. **Document Loading**: Load documents from various sources (e.g., web pages, documents, databases) that contain relevant information.\n",
    "2. **Chunking**: Split the loaded documents into smaller, manageable chunks to facilitate efficient retrieval.\n",
    "3. **Embedding Creation**: Generate embeddings for the document chunks using a suitable embedding model. These embeddings capture the semantic meaning of the text.\n",
    "4. **Vector Store Creation**: Store the embeddings in a vector store (e.g., FAISS, Pinecone) to enable efficient similarity search.\n",
    "5. **Retriever Creation**: Create a retriever that can query the vector store to find relevant document chunks based on user queries.\n",
    "\n",
    "\n",
    "### Embeddings and Vector Stores\n",
    "Embeddings are numerical representations of text that capture semantic meaning. </br>\n",
    "In order to create an embedding vector, we use a LLM to convert text into a high-dimensional vector space where similar texts are closer together. </br>\n",
    "Once we have the embeddings, we store them in a vector store for future retrival. </br>\n",
    "\n",
    "![RAG Indexing Process](../resources/images/rag_indexing.png)\n",
    "\n",
    "### Retrieval and generation\n",
    "\n",
    "When using trying to find similar documents, we use a retriever to search the vector store for relevant chunks based on the user's query. </br>\n",
    "For that the query is also converted into an embedding vector using the same embedding model. </br>\n",
    "Now that we have both chunks and query in the same vector space, we can find the most similar chunks to the query. </br>\n",
    "\n",
    "![RAG Indexing Process](../resources/images/rag_retrieval_generation.png)\n",
    "\n",
    "References:\n",
    "- https://python.langchain.com/docs/tutorials/rag/"
   ],
   "id": "7038623de3d03018"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T08:12:06.395454Z",
     "start_time": "2025-05-29T08:10:29.729220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_aws import BedrockEmbeddings  # Changed LLM import\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# --- 1. Load and Chunk Documents ---\n",
    "print(\"Step 1: Loading webpage content by known element ids\")\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "if not docs:\n",
    "    print(\"No documents loaded. Check the URL or SoupStrainer.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Step 2: Splitting documents into chunks...\")\n",
    "\n",
    "# RecursiveCharacterTextSplitter Is Recommended for general text:\n",
    "# This is a more sophisticated and generally recommended splitter for generic text.\n",
    "# It attempts to keep semantically related pieces of text together by using a list of separators in a hierarchical order.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "print(f\"Step 2: Document was split into {len(all_splits)} chunks.\")\n",
    "\n",
    "if not all_splits:\n",
    "    print(\"No text splits generated. Check the document content or splitter settings.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Create Vector Store and Index Chunks ---\n",
    "print(\"\\nStep 3: Creating vector store and indexing chunks with AWS Bedrock Embeddings...\")\n",
    "# Instantiate BedrockEmbeddings\n",
    "embeddings = BedrockEmbeddings(model_id='amazon.titan-embed-text-v2:0')\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "# Or use FAISS for larger datasets\n",
    "# vector_store = FAISS.from_documents(documents=all_splits, embedding=embeddings)\n",
    "print(\"Vector store created and documents indexed successfully using AWS Bedrock Embeddings.\")\n",
    "\n"
   ],
   "id": "a3d7aeac90046757",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading webpage content by known element ids\n",
      "Step 2: Splitting documents into chunks...\n",
      "Split into 61 chunks.\n",
      "\n",
      "Step 3: Creating vector store and indexing chunks with AWS Bedrock Embeddings...\n",
      "Vector store created and documents indexed successfully using AWS Bedrock Embeddings.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Lets explore the process for:\n",
    "1. Chunks of text Created from the document\n",
    "2. Embedding vectors created for each chunk\n",
    "3. Vector Store created with the embeddings\n"
   ],
   "id": "e58aa5b823751740"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T09:01:05.107624Z",
     "start_time": "2025-05-29T09:01:05.100435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Lets see the raw document content\n",
    "print(docs[0])\n",
    "\n",
    "# Display the first few chunks of text\n",
    "print(\"\\n--- Text Chunks ---\")\n",
    "for i, chunk in enumerate(all_splits[:5]):\n",
    "    print(f\"Chunk {i+1}:\")\n",
    "    print(\"MetaData: \\n\", chunk.metadata)\n",
    "    print(\"Content: \\n\",chunk.page_content)\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# The question will be converted into an embedding vector and used to search the vector store.\n",
    "## Print the first few embedding vectors\n",
    "print('The embedding vector of the question', embeddings.embed_query(\"What is an LLM agent?\") )\n",
    "# vector_store.similarity_search(\"What is an LLM agent?\", k=3)  # Example search to see if the vector store works"
   ],
   "id": "e18b9febce88ad5f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Text Chunks ---\n",
      "Chunk 1:\n",
      "MetaData: \n",
      " {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: \n",
      " LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\n",
      "\n",
      "Planning\n",
      "\n",
      "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\n",
      "Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n",
      "\n",
      "\n",
      "Memory\n",
      "----------------------------------------\n",
      "Chunk 2:\n",
      "MetaData: \n",
      " {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: \n",
      " Memory\n",
      "\n",
      "Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\n",
      "Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\n",
      "\n",
      "\n",
      "Tool use\n",
      "\n",
      "The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Overview of a LLM-powered autonomous agent system.\n",
      "----------------------------------------\n",
      "Chunk 3:\n",
      "MetaData: \n",
      " {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: \n",
      " Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "----------------------------------------\n",
      "Chunk 4:\n",
      "MetaData: \n",
      " {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: \n",
      " Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "----------------------------------------\n",
      "Chunk 5:\n",
      "MetaData: \n",
      " {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: \n",
      " Self-Reflection#\n",
      "Self-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\n",
      "ReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\n",
      "The ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\n",
      "Thought: ...\n",
      "Action: ...\n",
      "Observation: ...\n",
      "... (Repeated many times)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## LangChain Retrievers\n",
    "Retrievers in LangChain are used to fetch relevant documents or information based on a user query. </br>\n",
    "They can be used to retrieve documents from various sources, such as databases, web pages, or vector stores. </br>\n",
    "\n",
    "### Types of Retrievers\n",
    "- **Vector Store Retriever**: Retrieves documents based on similarity search in a vector store.\n",
    "- **Document Loader Retriever**: Retrieves documents from a document loader.\n",
    "- **Multi-Query Retriever**: Retrieves documents based on multiple queries.\n",
    "- **EnsembleRetriever**: Combines results from multiple retrievers.\n",
    "\n",
    "### Reranking\n",
    "Reranking is a technique used to improve the quality of retrieved documents by reordering them based on relevance to the user query. </br>\n",
    "Reranking can be done using various methods, such as:\n",
    "- **Embedding-based Reranking**: Uses embeddings to calculate similarity between the query and retrieved documents.\n",
    "- **LLM-based Reranking**: Uses a language model to evaluate the relevance of retrieved documents based on the user query.\n",
    "- **Rule-based Reranking**: Uses predefined rules to reorder retrieved documents based on specific criteria.\n",
    "- **Metadata-based Reranking**: Uses metadata associated with documents to reorder them based on relevance to the user query.\n",
    "\n",
    "References:\n",
    "- https://python.langchain.com/docs/concepts/retrievers/\n"
   ],
   "id": "38f3d6c794950076"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T08:53:16.373044Z",
     "start_time": "2025-05-29T08:53:16.177622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def format_docs_for_context(docs: List[Document]) -> str:\n",
    "    \"\"\"\n",
    "    Formats the retrieved documents into a single string for the prompt context.\n",
    "    \"\"\"\n",
    "    return \"\\n\\n-----------\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# --- 6. Create Retriever and RAG Chain ---\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 4})  # Retrieve top 4 relevant chunks\n",
    "\n",
    "print(format_docs_for_context(retriever.invoke(\"What do you know about Generative Agents?\")))"
   ],
   "id": "71a8357c47cb4d6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\n",
      "Generative Agents Simulation#\n",
      "Generative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\n",
      "The design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\n",
      "\n",
      "-----------\n",
      "Prompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\n",
      "\n",
      "\n",
      "Planning & Reacting: translate the reflections and the environment information into actions\n",
      "\n",
      "Planning is essentially in order to optimize believability at the moment vs in time.\n",
      "Prompt template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\n",
      "Relationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\n",
      "Environment information is present in a tree structure.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The generative agent architecture. (Image source: Park et al. 2023)\n",
      "\n",
      "-----------\n",
      "LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\n",
      "\n",
      "Planning\n",
      "\n",
      "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\n",
      "Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n",
      "\n",
      "\n",
      "Memory\n",
      "\n",
      "-----------\n",
      "Case Studies#\n",
      "Scientific Discovery Agent#\n",
      "ChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\n",
      "\n",
      "The LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\n",
      "It is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## LangChain Chains\n",
    "\n",
    "Chains in LangChain allow you to create complex workflows by chaining together multiple operations, such as document retrieval, text processing, and LLM generation. </br>\n",
    "In the following steps, we will create a Retrieval-Augmented Generation (RAG) chain that retrieves relevant documents based on a user query and generates an answer using an LLM.\n",
    "\n",
    "### LangChain Expression Language (LCEL) \"Pipe\" Operator\n",
    "\n",
    "LangChain leverages Python's ability to overload operators (specifically the __or__ method) to implement a functional \"pipe\" or \"chaining\" mechanism. </br>\n",
    "**How it works in LCEL:**</br>\n",
    "When you write `component_a` | `component_b`, it means \"take the output of `component_a` and pass it as the input to `component_b`.\" </br>\n",
    "It creates a sequence of operations where the result of one step flows directly into the next.\n",
    "This makes building complex LLM applications (like RAG chains) much more readable and modular.\n",
    "Each \"component\" in an LCEL chain (like retriever, prompt, llm, StrOutputParser) is typically an instance of a Runnable object. LangChain's Runnable classes define the __or__ method, which is what allows this chaining syntax to work.\n"
   ],
   "id": "8f96f974be3be969"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T20:53:36.553273Z",
     "start_time": "2025-05-28T20:53:35.725599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    Helpful Answer:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "print(\"\\nCreating retriever and RAG chain...\")\n",
    "\n",
    "rag_chain = (\n",
    "        {\"context\": retriever | format_docs_for_context, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    ")\n",
    "print(\"RAG chain created.\")\n",
    "print(\"\\n--- Starting RAG Application (with AWS Bedrock Embeddings and LLM) ---\")\n",
    "\n",
    "# example_question = \"What are the main components of an LLM agent?\"\n",
    "example_question = \"What types  of memory does LLM application use?\"  # https://lilianweng.github.io/posts/2023-06-23-agent/#component-two-memory\n",
    "\n",
    "final_answer = rag_chain.invoke(example_question)\n",
    "print(\"Question:\", example_question)\n",
    "print(\"Answer:\", final_answer)\n",
    "\n"
   ],
   "id": "3d5061c761a07233",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating retriever and RAG chain...\n",
      "RAG chain created.\n",
      "\n",
      "--- Starting RAG Application (with AWS Bedrock Embeddings and LLM) ---\n",
      "Question: What types  of memory does LLM application use?\n",
      "Answer: LLM applications use both short-term memory (in-context learning) and long-term memory (external vector store for infinite recall).\n"
     ]
    }
   ],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
