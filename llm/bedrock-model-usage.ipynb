{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# AWS Bedrock\n",
    "\n",
    "\n",
    "AWS Bedrock is a fully managed service that provides access to foundation models (FMs) from leading AI companies. It allows you to build and scale generative AI applications using these models. In this notebook, we will explore how to use AWS Bedrock with the `boto3` library in Python."
   ],
   "id": "f12e12faf41d4e41"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## LLMs and Foundation Models\n",
    "Foundation models are large-scale machine learning models that are trained on vast amounts of data and can be fine-tuned for specific tasks. AWS Bedrock provides access to various foundation models, including text generation, image generation, and more. </br>\n",
    "\n",
    "* LLM is a stateless function. When \"memory\" or \"context\" is required, it is passed as part of the prompt.\n",
    "* The operation of fetching information from a database or other source is called \"retrieval augmentation generation\" (RAG).\n",
    "* The datasource used to fetch information is called a knowledge base.\n",
    "* LLMs [pricing](https://aws.amazon.com/bedrock/pricing/) depends on the model complexity and the number of tokens (~4 chars â‰… word) in the prompt and the response.\n",
    "* The longer the prompt, the more expensive and the slower it is.\n",
    "\n",
    "### Using AWS Bedrock models and regions\n",
    "\n",
    "AWS LLM models availability is region specific. `us-east-1` or `N. Virginia` is the region where all models are available. </br>\n",
    "In order to use a model of a specific provider, you need to request access to that model in the [Model Access](https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/modelaccess) section </br>\n",
    "In this tutorial we'll be using `us-east-1` region with several different models.\n",
    "\n",
    "### Cross-region inference\n",
    "Cross-region inference is a new AWS feature that enable LLM requests  to be processed in a different geographical region than where the request originated.\n",
    "Instead of being limited to the models and compute resources available in a single region, cross-region inference can automatically route your inference requests to other available regions.\n",
    "You can use the `Inference Profile ID` instead of the `Model ID` to specify the model you want to use. The Inference Profile ID is a unique identifier for a specific model and its associated compute resources in a specific region. </br>\n",
    "For list of available models and their Inference Profile IDs, please refer to the [AWS Cross-region inference Console](https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/inference-profiles).\n",
    "\n"
   ],
   "id": "b3154df63ade026c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Zero Shot\n",
    "The following example is called `zero-shot` prompting. </br>\n",
    "There are no examples or context provided to the model. The model is expected to understand the task and generate a response based on its own training. </br>\n",
    "\n",
    "> change the models in order to switch between different LLMs. </br>"
   ],
   "id": "e5b83cdab347501d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T16:01:58.102609Z",
     "start_time": "2025-05-07T16:01:55.919172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Create a Bedrock Runtime client in the AWS Region you want to use.\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "##> Show the differences between\n",
    "# model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\" # Model ID\n",
    "# model_id = \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\" # Inference Profile ID\n",
    "# model_id = \"amazon.titan-text-express-v1\"\n",
    "model_id = \"amazon.nova-lite-v1:0\"\n",
    "\n",
    "# Start a conversation with the user message.\n",
    "user_message = \"\"\"Meeting transcript:\n",
    "Miguel: Hi Brant, I want to discuss the workstream  for our new product launch\n",
    "Brant: Sure Miguel, is there anything in particular you want to discuss?\n",
    "Miguel: Yes, I want to talk about how users enter into the product.\n",
    "Brant: Ok, in that case let me add in Namita.\n",
    "Namita: Hey everyone\n",
    "Brant: Hi Namita, Miguel wants to discuss how users enter into the product.\n",
    "Miguel: its too complicated and we should remove friction. for example, why do I need to fill out additional forms?  I also find it difficult to find where to access the product when I first land on the landing page.\n",
    "Brant: I would also add that I think there are too many steps.\n",
    "Namita: Ok, I can work on the landing page to make the product more discoverable but brant can you work on the additional forms?\n",
    "Brant: Yes but I would need to work with James from another team as he needs to unblock the sign up workflow.  Miguel can you document any other concerns so that I can discuss with James only once?\n",
    "Miguel: Sure.\n",
    "\n",
    "From the meeting transcript above, Create a list of action items for each person.\n",
    "\"\"\"\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": user_message}],\n",
    "    }\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Send the message to the model, using a basic inference configuration.\n",
    "    #  https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/converse.html\n",
    "    response = client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=conversation,\n",
    "        inferenceConfig={\"maxTokens\": 4096, \"stopSequences\": [\"User:\"], \"temperature\": 0, \"topP\": 1},\n",
    "        additionalModelRequestFields={}\n",
    "    )\n",
    "\n",
    "    # Extract and print the response text.\n",
    "    response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    print(response_text)\n",
    "\n",
    "except (ClientError, Exception) as e:\n",
    "    print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "    exit(1)\n"
   ],
   "id": "8b88007436436904",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the meeting transcript, here is a list of action items for each person:\n",
      "\n",
      "**Miguel:**\n",
      "1. Document any other concerns related to the user entry process, specifically focusing on the sign-up workflow and landing page, to share with Brant for a single discussion with James.\n",
      "\n",
      "**Brant:**\n",
      "1. Collaborate with James from another team to address the issues with the sign-up workflow.\n",
      "2. Ensure that the discussion with James includes all concerns documented by Miguel.\n",
      "\n",
      "**Namita:**\n",
      "1. Work on improving the landing page to make the product more discoverable for users.\n",
      "\n",
      "**James (from another team):**\n",
      "1. Unblock the sign-up workflow in collaboration with Brant.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Understanding the response\n",
    "\n",
    "The LLM response is a JSON object that contains important information in addition to the generated text. </br>\n",
    "When analyzing and comparing LLM responses, look for the following fields:\n",
    "\n",
    "* `latency`: The time taken to process the request and generate a response.\n",
    "* `usage`: The number of tokens used in the prompt and the response. This is important for cost estimation.\n",
    "* `RequestId`: A unique identifier for the request. This can be useful for debugging and tracking purposes.\n"
   ],
   "id": "87fc54fae6e8428f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T14:33:43.072931Z",
     "start_time": "2025-04-20T14:33:43.059995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "    # print the response with json format and indentation\n",
    "    import json\n",
    "\n",
    "    print(json.dumps(response, indent=4, sort_keys=True))\n"
   ],
   "id": "2a5dd1faadc63f75",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"ResponseMetadata\": {\n",
      "        \"HTTPHeaders\": {\n",
      "            \"connection\": \"keep-alive\",\n",
      "            \"content-length\": \"859\",\n",
      "            \"content-type\": \"application/json\",\n",
      "            \"date\": \"Sun, 20 Apr 2025 14:33:26 GMT\",\n",
      "            \"x-amzn-requestid\": \"ca4f75e1-822b-4ef4-920f-0894744c3ec5\"\n",
      "        },\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"RequestId\": \"ca4f75e1-822b-4ef4-920f-0894744c3ec5\",\n",
      "        \"RetryAttempts\": 0\n",
      "    },\n",
      "    \"metrics\": {\n",
      "        \"latencyMs\": 869\n",
      "    },\n",
      "    \"output\": {\n",
      "        \"message\": {\n",
      "            \"content\": [\n",
      "                {\n",
      "                    \"text\": \"Based on the meeting transcript, here is a list of action items for each person:\\n\\n**Miguel:**\\n1. Document any other concerns related to the user entry process, specifically focusing on the sign-up workflow and landing page, to share with Brant for a single discussion with James.\\n\\n**Brant:**\\n1. Collaborate with James from another team to address the issues with the sign-up workflow.\\n2. Ensure that the discussion with James includes all concerns documented by Miguel.\\n\\n**Namita:**\\n1. Work on improving the landing page to make the product more discoverable for users.\\n\\n**James (from another team):**\\n1. Unblock the sign-up workflow in collaboration with Brant.\"\n",
      "                }\n",
      "            ],\n",
      "            \"role\": \"assistant\"\n",
      "        }\n",
      "    },\n",
      "    \"stopReason\": \"end_turn\",\n",
      "    \"usage\": {\n",
      "        \"inputTokens\": 249,\n",
      "        \"outputTokens\": 140,\n",
      "        \"totalTokens\": 389\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## One Shot\n",
    "One-shot prompting is a technique used to provide a single example to the model, helping it understand the task better. </br>\n",
    "\n",
    "In this example, we will use one-shot prompting to create a meeting summary and action items. </br>\n",
    "We will use the `assistant` role to provide the model with a system message that describes its role and the task it needs to perform. </br>\n",
    "In addition, we will provide a user message that contains the meeting transcript. </br>\n"
   ],
   "id": "951d9c212f656c79"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T08:05:00.830693Z",
     "start_time": "2025-04-21T08:04:50.583606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "model_id = \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\" # Inference Profile ID\n",
    "\n",
    "# Start a conversation with the user message.\n",
    "system_message = \"\"\"You are a meeting assistant that helps to summarize the meeting and create action items for each person.\n",
    "You are given a meeting transcript and you need to create a list of action items for each person in the meeting.\n",
    "The action items should be in the following format:\n",
    "\n",
    "=== Miguel ===\n",
    "    - action item 1\n",
    "    - action item 2\n",
    "=== Brant ===\n",
    "    - action item1\n",
    "    - action item 2\n",
    "=== Namita ===\n",
    "    - action item 1\n",
    "    - action item 2\n",
    "\n",
    "The action items should be clear and concise.\n",
    "The action items should be based on the meeting transcript and should not include any additional information.\n",
    "In the end of the response, mention the meeting participants and their roles in a JSON format.\n",
    "In addition rank their involvement in the meeting from 1 to 5, where 5 is the most involved and 1 is the least involved.\n",
    "{\n",
    "    {\"name\": \"Miguel\", \"role\": \"Product Manager\", \"involvement\": 5},\n",
    "    {\"name\": \"Brant\", \"role\": \"Software Engineer\", \"involvement\": 4},\n",
    "    {\"name\": \"Namita\", \"role\": \"UX Designer\", \"involvement\": 3}\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "user_message = \"\"\"Meeting transcript:\n",
    "Miguel: Hi Brant, I want to discuss the workstream  for our new product launch\n",
    "Brant: Sure Miguel, is there anything in particular you want to discuss?\n",
    "Miguel: Yes, I want to talk about how users enter into the product.\n",
    "Brant: Ok, in that case let me add in Namita.\n",
    "Namita: Hey everyone\n",
    "Brant: Hi Namita, Miguel wants to discuss how users enter into the product.\n",
    "Miguel: its too complicated and we should remove friction. for example, why do I need to fill out additional forms?  I also find it difficult to find where to access the product when I first land on the landing page.\n",
    "Brant: I would also add that I think there are too many steps.\n",
    "Namita: Ok, I can work on the landing page to make the product more discoverable but brant can you work on the additional forms?\n",
    "Brant: Yes but I would need to work with James from another team as he needs to unblock the sign up workflow.  Miguel can you document any other concerns so that I can discuss with James only once?\n",
    "Miguel: Sure.\n",
    "\"\"\"\n",
    "\n",
    "user_message2 = \"\"\"Meeting transcript:\n",
    "Attendees: Shimon (PM), Igor (CTO), Avi (Tech Lead)\n",
    "\n",
    "Shimon: Right, let's discuss integrating code coverage into the main pipeline. What are the main benefits and drawbacks?\n",
    "\n",
    "Igor: The major pro is improved code quality and maintainability. It gives us objective data on test effectiveness, reducing future bugs and technical debt. Itâ€™s a standard best practice.\n",
    "\n",
    "Avi: Agreed, Igor. The con is potential friction â€“ longer build times initially, and developers needing to potentially refactor or add more tests, impacting velocity slightly. We need to manage thresholds carefully.\n",
    "\n",
    "Shimon: So, increased confidence in quality versus a possible short-term slowdown?\n",
    "\n",
    "Igor: Exactly. A worthwhile investment for long-term stability.\n",
    "\n",
    "Avi: We can mitigate the impact by starting with warnings, not blockers.\n",
    "Barak: I dont code coverage is a good idea. It will slow down our development.\n",
    "Shimon: Barak, can you elaborate on that?\n",
    "Barak: The master has spoken.\n",
    "\"\"\"\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [{\"text\": system_message}],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": user_message2}],\n",
    "    }\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Send the message to the model, using a basic inference configuration.\n",
    "    #  https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/converse.html\n",
    "    response = client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=conversation,\n",
    "        inferenceConfig={\"maxTokens\": 4096, \"stopSequences\": [\"User:\"], \"temperature\": 0, \"topP\": 1},\n",
    "        additionalModelRequestFields={}\n",
    "    )\n",
    "\n",
    "    # Extract and print the response text.\n",
    "    response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    print(response_text)\n",
    "\n",
    "except (ClientError, Exception) as e:\n",
    "    print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "    exit(1)\n"
   ],
   "id": "82b0312e27d994a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the meeting transcript, here are the action items for each participant:\n",
      "\n",
      "=== Shimon ===\n",
      "    - Create a proposal document outlining the code coverage implementation plan\n",
      "    - Schedule a follow-up meeting to discuss specific threshold values\n",
      "    - Address Barak's concerns about development slowdown\n",
      "\n",
      "=== Igor ===\n",
      "    - Research and recommend code coverage tools suitable for the pipeline\n",
      "    - Prepare technical documentation for code coverage integration\n",
      "    - Define initial metrics for measuring the impact on build times\n",
      "\n",
      "=== Avi ===\n",
      "    - Design a gradual implementation plan starting with warnings\n",
      "    - Create guidelines for test coverage thresholds\n",
      "    - Prepare documentation for developers about new testing requirements\n",
      "\n",
      "=== Barak ===\n",
      "    - Provide detailed feedback about specific concerns regarding code coverage\n",
      "    - Document specific examples where code coverage might impact development speed\n",
      "\n",
      "Meeting Participants and Roles:\n",
      "{\n",
      "    {\"name\": \"Shimon\", \"role\": \"Product Manager\", \"involvement\": 4},\n",
      "    {\"name\": \"Igor\", \"role\": \"CTO\", \"involvement\": 5},\n",
      "    {\"name\": \"Avi\", \"role\": \"Tech Lead\", \"involvement\": 5},\n",
      "    {\"name\": \"Barak\", \"role\": \"Team Member\", \"involvement\": 2}\n",
      "}\n",
      "\n",
      "Note: Igor and Avi showed the highest involvement with detailed technical input and solutions. Shimon maintained good meeting flow and asked relevant questions. Barak had minimal constructive involvement with only brief negative feedback.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Few Shots\n",
    "Few-shot prompting is a technique used to provide multiple examples to the model, helping it understand the task better. </br>\n",
    "Each example is called a `shot` and it contains a prompt and a desired response. </br>\n",
    "The `shots` can be based on real-world examples or synthetic examples that exemplify the task. </br>\n"
   ],
   "id": "bf7235b12365041b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T15:48:40.062859Z",
     "start_time": "2025-04-21T15:48:29.091978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "model_id = \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\" # Inference Profile ID\n",
    "\n",
    "# Start a conversation with the user message.\n",
    "\n",
    "prompt = \"\"\"You are a product manager for a website builder platform (like Wix). For each new feature request, you need to outline the functional requirements, highlight important UI/UX considerations, and explain the business advantages. Respond in the following format for each feature request:\n",
    "\n",
    "**Functional Requirement:**\n",
    "[Clearly describe what the feature should do.]\n",
    "\n",
    "**UI/UX Important Points:**\n",
    "[List key considerations for the user interface and user experience of this feature.]\n",
    "\n",
    "**Business Advantage:**\n",
    "[Explain how this feature benefits the website builder platform.]\n",
    "\n",
    "----\n",
    "Examples:\n",
    "\n",
    "**Feature Request:** Add a built-in image editor with basic cropping and resizing tools.\n",
    "\n",
    "**Functional Requirement:**\n",
    "Users should be able to crop and resize images directly within the website builder without needing to upload pre-edited files. Supported formats should include JPG, PNG, and GIF. The editor should offer standard aspect ratio presets and freeform resizing.\n",
    "\n",
    "**UI/UX Important Points:**\n",
    "- The image editor should be easily accessible within the image settings panel.\n",
    "- Controls for cropping and resizing should be intuitive and visually clear.\n",
    "- Users should see a real-time preview of their edits.\n",
    "- An option to revert to the original image should be available.\n",
    "\n",
    "**Business Advantage:**\n",
    "- Improves user convenience and efficiency by eliminating the need for external image editing tools.\n",
    "- Can attract users who need quick image adjustments without complex software.\n",
    "- May reduce support requests related to image sizing issues.\n",
    "\n",
    "---\n",
    "\n",
    "**Feature Request:** Implement a library of pre-designed website sections (e.g., headers, footers, contact forms).\n",
    "\n",
    "**Functional Requirement:**\n",
    "Users should be able to browse and insert professionally designed website sections into their pages with a single click. The library should include various categories and styles, and users should be able to customize the content and styling of these sections.\n",
    "\n",
    "**UI/UX Important Points:**\n",
    "- The section library should be well-organized and easy to navigate, possibly with categories and search functionality.\n",
    "- Previews of the sections should be clear and representative of the final design.\n",
    "- The insertion process should be seamless and not disrupt the user's workflow.\n",
    "- Users should have clear visual cues on how to customize the content of the inserted sections.\n",
    "\n",
    "**Business Advantage:**\n",
    "- Speeds up the website creation process for users, making it more appealing to beginners.\n",
    "- Provides users with professionally designed elements, potentially leading to more visually appealing websites.\n",
    "- Can encourage users to build more comprehensive websites by offering readily available components.\n",
    "\n",
    "---\n",
    "\n",
    "**Feature Request:** Allow users to embed social media feeds (e.g., Instagram, Twitter) directly onto their websites.\n",
    "\n",
    "**Functional Requirement:**\n",
    "Users should be able to connect their social media accounts and display their latest posts on their website. They should have options to customize the number of posts displayed and the layout of the feed.\n",
    "\n",
    "**UI/UX Important Points:**\n",
    "- The connection process to social media accounts should be secure and straightforward.\n",
    "- Embedding options should be easily accessible within the website editor.\n",
    "- Users should have control over the visual presentation of the feed to match their website's design.\n",
    "- The embedded feed should be responsive and display correctly on different devices.\n",
    "\n",
    "**Business Advantage:**\n",
    "- Enhances user engagement by allowing them to showcase their social media presence.\n",
    "- Can drive traffic between the user's website and their social media profiles.\n",
    "- Adds dynamic content to websites, making them more lively and up-to-date.\n",
    "\n",
    "---\n",
    "\n",
    "Here is the actual Feature Request: {0}\n",
    "\"\"\"\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": prompt.format(\"Integrate with a third-party payment processor (e.g., Stripe, PayPal) to enable e-commerce functionality.\")}],\n",
    "        # \"content\": [{\"text\": prompt.format(\"Implement A built-in image editor with basic cropping and resizing tools.\")}],\n",
    "    }\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Send the message to the model, using a basic inference configuration.\n",
    "    #  https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/converse.html\n",
    "    response = client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=conversation,\n",
    "        inferenceConfig={\"maxTokens\": 4096, \"stopSequences\": [\"User:\"], \"temperature\": 0, \"topP\": 1},\n",
    "        additionalModelRequestFields={}\n",
    "    )\n",
    "\n",
    "    # Extract and print the response text.\n",
    "    response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    print(response_text)\n",
    "\n",
    "except (ClientError, Exception) as e:\n",
    "    print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "    exit(1)\n"
   ],
   "id": "f163013747606b0f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Functional Requirement:**\n",
      "Users should be able to set up and manage e-commerce functionality by connecting their Stripe/PayPal accounts to their websites. The integration should support:\n",
      "- Secure payment processing for multiple currencies\n",
      "- Product catalog management\n",
      "- Order tracking and management\n",
      "- Automatic email notifications for orders\n",
      "- Basic inventory tracking\n",
      "- Shopping cart functionality\n",
      "- Payment status monitoring\n",
      "- Refund processing capabilities\n",
      "\n",
      "**UI/UX Important Points:**\n",
      "- Simple step-by-step wizard for payment processor account connection\n",
      "- Clear security badges and certifications to build trust\n",
      "- Intuitive product upload and management interface\n",
      "- Easy-to-customize checkout page templates\n",
      "- Mobile-responsive shopping experience\n",
      "- Clear error messages and payment status indicators\n",
      "- Dashboard for order management and sales analytics\n",
      "- Simple refund process interface\n",
      "- Clear documentation and help resources\n",
      "\n",
      "**Business Advantage:**\n",
      "- Opens up a new revenue stream through e-commerce transaction fees\n",
      "- Attracts business-focused customers who need online selling capabilities\n",
      "- Increases platform stickiness as users become dependent on the e-commerce functionality\n",
      "- Potential for premium tier pricing for advanced e-commerce features\n",
      "- Competitive advantage over basic website builders without e-commerce capabilities\n",
      "- Creates opportunities for partnerships with payment processors\n",
      "- Increases average customer lifetime value through long-term usage\n",
      "- Positions the platform as a complete business solution rather than just a website builder\n",
      "\n",
      "Additional Considerations:\n",
      "- Regular security audits and compliance updates will be necessary\n",
      "- Need for dedicated customer support for e-commerce-related issues\n",
      "- Opportunity for future expansions (e.g., marketplace features, advanced inventory management)\n",
      "- Potential for integration with other business tools (accounting software, shipping services)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tool Use / Function Calling\n",
    "LLMs can be enhanced with tools and APIs to provide additional functionality. </br>\n",
    "For example, you can use APIs to fetch data from external sources, perform calculations, or interact with other services. </br>\n",
    "In this example, we will use the `python` tool to perform calculations and the `weather` API to fetch weather data. </br>\n",
    "\n",
    "**Tool Use Workflow:**\n",
    "\n",
    "1.  **Define Tools:** Specify tools with names, descriptions, and argument schemas. Include a user prompt (e.g., \"What's the weather like in New York today?\").\n",
    "2.  **LLM Decides:** The LLM determines if a tool is necessary and halts text generation if so.\n",
    "3.  **JSON Call:** The LLM outputs a JSON object containing the selected tool and its parameter values.\n",
    "4.  **Execute & Return:** The system extracts parameters, runs the tool, and returns the output to the LLM.\n",
    "5.  **Generate Answer:** The LLM uses the tool output to create a final response.\n",
    "\n",
    "![Tool Calling](../resources/images/tool-call-schema-removebg.webp)\n",
    "### References\n",
    "\n",
    "* [AWS Bedrock: Converse API tool use examples](https://docs.aws.amazon.com/bedrock/latest/userguide/tool-use-examples.html)\n",
    "* [Guide to Tool Calling](https://www.analyticsvidhya.com/blog/2024/08/tool-calling-in-llms/)\n",
    "\n",
    "\n"
   ],
   "id": "86eaa370c9df2983"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T16:57:59.136655Z",
     "start_time": "2025-04-27T16:57:51.913564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "model_id = \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\" # Inference Profile ID\n",
    "\n",
    "class LocationNotFoundException(Exception):\n",
    "    \"\"\"Raised when a location is not found.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Weather API: Fetch weather data from weatherapi.com, api key from user: mistriela@yopmail.com\n",
    "def fetch_weather(city):\n",
    "    print('Fetching weather data for city:', city)\n",
    "    base_url = \"https://api.weatherapi.com/v1/current.json\"\n",
    "    params = {\n",
    "        \"q\": city,\n",
    "        \"key\": \"6e7b99ab0f454283ab9125132252104\",\n",
    "        \"aqi\": \"no\"  # Get temperature in Celsius\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()  # Raise an error for bad HTTP responses\n",
    "        weather_data = response.json()\n",
    "        return {\n",
    "            \"city\": weather_data[\"location\"][\"name\"],\n",
    "            \"temperature\": weather_data[\"current\"][\"temp_c\"],\n",
    "            \"description\": weather_data[\"current\"][\"condition\"][\"text\"]\n",
    "        }\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise LocationNotFoundException(f\"Error fetching weather data: {e}\")\n",
    "\n",
    "\n",
    "def invoke_bedrock_llm_with_function_calling(prompt):\n",
    "    \"\"\"\n",
    "    Invokes an LLM on AWS Bedrock with function calling to get weather information.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The user's query (e.g., \"What's the weather in London?\").\n",
    "        model_id (str): The ID of the Bedrock LLM model to use.\n",
    "        region_name (str): The AWS region.\n",
    "\n",
    "    Returns:\n",
    "        str: The LLM's response, which may include the weather information\n",
    "             or an error message.\n",
    "    \"\"\"\n",
    "\n",
    "    tool_config = {\n",
    "        \"tools\": [\n",
    "            {\n",
    "                \"toolSpec\": {\n",
    "                    \"name\": \"fetch_weather\",\n",
    "                    \"description\": \"fetch weather information for a given city\",\n",
    "                    \"inputSchema\": {\n",
    "                        \"json\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"city\": {\n",
    "                                    \"type\": \"string\",\n",
    "                                    \"description\": \"The name of the city for which to fetch weather information.\"\n",
    "                                }\n",
    "                            },\n",
    "                            \"required\": [\n",
    "                                \"city\"\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "    # Note: there is no usage of `assistant` role as part of the prompt. It's the LLM responsibility to understand that a tool is required.\n",
    "    input_messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": prompt}]\n",
    "    }]\n",
    "\n",
    "    # Send the initial message to the model. if there is a weather request, the model will stop and return a tool use request.\n",
    "    # Several tools requests can be sent in a single response.\n",
    "    response = client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=input_messages,\n",
    "        toolConfig=tool_config\n",
    "    )\n",
    "    output_message = response['output']['message']\n",
    "    input_messages.append(output_message)\n",
    "    stop_reason = response['stopReason']\n",
    "\n",
    "    print(f\"LLM Message: {json.dumps(response, indent=4)}\")\n",
    "    # Print the LLM message and stop reason.\n",
    "    # See if any response starts with a json object root node: \"toolUse\"\n",
    "    if stop_reason == 'tool_use':\n",
    "        # Tool use requested. Call the tool and send the result to the model.\n",
    "        tool_requests = output_message['content']\n",
    "        for tool_request in tool_requests:\n",
    "            if 'toolUse' in tool_request:\n",
    "                tool = tool_request['toolUse']\n",
    "                print(f\"Requesting tool {tool['name']} Request: {tool['toolUseId']} \")\n",
    "\n",
    "                if tool['name'] == 'fetch_weather':\n",
    "                    tool_result = {}\n",
    "                    try:\n",
    "                        weather_data = fetch_weather(tool['input']['city'])\n",
    "                        tool_result = {\n",
    "                            \"toolUseId\": tool['toolUseId'],\n",
    "                            \"content\": [{\"json\": weather_data}]\n",
    "                        }\n",
    "                    except LocationNotFoundException as err:\n",
    "                        tool_result = {\n",
    "                            \"toolUseId\": tool['toolUseId'],\n",
    "                            \"content\": [{\"text\":  err.args[0]}],\n",
    "                            \"status\": 'error'\n",
    "                        }\n",
    "\n",
    "                    tool_result_message = {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [{\"toolResult\": tool_result}]\n",
    "                    }\n",
    "                    input_messages.append(tool_result_message)\n",
    "\n",
    "                    # Send the tool result to the model.\n",
    "                    response = client.converse(\n",
    "                        modelId=model_id,\n",
    "                        messages=input_messages,\n",
    "                        toolConfig=tool_config\n",
    "                    )\n",
    "                    output_message = response['output']['message']\n",
    "\n",
    "    # print the final response from the model.\n",
    "    # for content in output_message['content']:\n",
    "    #     print(json.dumps(content, indent=4))\n",
    "\n",
    "    return output_message\n",
    "\n",
    "user_query = \"What is the weather like in London?\"\n",
    "llm_response = invoke_bedrock_llm_with_function_calling(user_query)\n",
    "print(llm_response['content'][0]['text'])\n"
   ],
   "id": "fc75e70526d083eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Message: {\n",
      "    \"ResponseMetadata\": {\n",
      "        \"RequestId\": \"64d74cf3-4255-447d-8413-55d6ce7e8e1a\",\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"HTTPHeaders\": {\n",
      "            \"date\": \"Sun, 27 Apr 2025 16:57:55 GMT\",\n",
      "            \"content-type\": \"application/json\",\n",
      "            \"content-length\": \"485\",\n",
      "            \"connection\": \"keep-alive\",\n",
      "            \"x-amzn-requestid\": \"64d74cf3-4255-447d-8413-55d6ce7e8e1a\"\n",
      "        },\n",
      "        \"RetryAttempts\": 0\n",
      "    },\n",
      "    \"output\": {\n",
      "        \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": [\n",
      "                {\n",
      "                    \"text\": \"I'll help you check the current weather in London using the fetch_weather function.\"\n",
      "                },\n",
      "                {\n",
      "                    \"toolUse\": {\n",
      "                        \"toolUseId\": \"tooluse_4Ol_16JJQjGjvm4X1r4YWQ\",\n",
      "                        \"name\": \"fetch_weather\",\n",
      "                        \"input\": {\n",
      "                            \"city\": \"London\"\n",
      "                        }\n",
      "                    }\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"stopReason\": \"tool_use\",\n",
      "    \"usage\": {\n",
      "        \"inputTokens\": 399,\n",
      "        \"outputTokens\": 71,\n",
      "        \"totalTokens\": 470\n",
      "    },\n",
      "    \"metrics\": {\n",
      "        \"latencyMs\": 1976\n",
      "    }\n",
      "}\n",
      "Requesting tool fetch_weather Request: tooluse_4Ol_16JJQjGjvm4X1r4YWQ \n",
      "Fetching weather data for city: London\n",
      "Based on the weather data, in London it is currently 21.0Â°C (69.8Â°F) with patchy rain nearby. You might want to keep an umbrella handy if you're heading out!\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tool Chaining\n",
    "Tool chaining is a technique used to combine multiple tools or functions in a sequence to achieve a more complex task. </br>\n",
    "In this example, we will use tool chaining **recursively** to fetch the weather data and then determine the dress code based on the temperature. </br>\n",
    "\n",
    "### Execution Flow\n",
    "We will ask the model to provide a dress code for the hotter city. </br>\n",
    "The model will need to understand that it needs to call the `fetch_weather` tool to get the weather data for **both** cities. </br>\n",
    "Then, it will call the `get_dress_code` tool to determine the dress code based on the temperature. </br>\n",
    "\n",
    "### Important Lookouts\n",
    "* Experiment with the different models, check if `lite` models can be used instead of `pro` models.\n",
    "* Check the input/output of the llm - see how the conversation is built gradually."
   ],
   "id": "38104e3d96629d9b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T17:14:08.060569Z",
     "start_time": "2025-04-27T17:14:05.239654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "# model_id = \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
    "# model_id = \"amazon.nova-pro-v1:0\"\n",
    "model_id = \"amazon.nova-lite-v1:0\"\n",
    "\n",
    "class LocationNotFoundException(Exception):\n",
    "    \"\"\"Raised when a location is not found.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Weather API: Fetch weather data from weatherapi.com, api key from user: mistriela@yopmail.com\n",
    "def fetch_weather(city):\n",
    "    print('Fetching weather data for city:', city)\n",
    "    base_url = \"https://api.weatherapi.com/v1/current.json\"\n",
    "    params = {\n",
    "        \"q\": city,\n",
    "        \"key\": \"6e7b99ab0f454283ab9125132252104\",\n",
    "        \"aqi\": \"no\"  # Get temperature in Celsius\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()  # Raise an error for bad HTTP responses\n",
    "        weather_data = response.json()\n",
    "        return {\n",
    "            \"city\": weather_data[\"location\"][\"name\"],\n",
    "            \"temperature\": weather_data[\"current\"][\"temp_c\"],\n",
    "            \"description\": weather_data[\"current\"][\"condition\"][\"text\"]\n",
    "        }\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise LocationNotFoundException(f\"Error fetching weather data: {e}\")\n",
    "\n",
    "def get_dress_code(temperature: float) -> str:\n",
    "    \"\"\"\n",
    "    Determine the dress code based on the temperature.\n",
    "    Temperature ranges: -20 to 50 degrees Celsius.\n",
    "    Args:\n",
    "        temperature (float): The temperature in Celsius.\n",
    "\n",
    "    Returns:\n",
    "        str: The recommended dress code.\n",
    "    \"\"\"\n",
    "    print('Getting dress code for temperature:', temperature)\n",
    "    if temperature < -10:\n",
    "        return \"Wear a heavy winter coat, gloves, and a warm hat.\"\n",
    "    elif -10 <= temperature < 0:\n",
    "        return \"Wear a warm coat and gloves.\"\n",
    "    elif 0 <= temperature < 10:\n",
    "        return \"Wear a light jacket.\"\n",
    "    elif 10 <= temperature < 20:\n",
    "        return \"Wear a long-sleeve shirt.\"\n",
    "    elif 20 <= temperature < 30:\n",
    "        return \"Wear a short-sleeve shirt.\"\n",
    "    else:\n",
    "        return \"Wear summer clothes.\"\n",
    "\n",
    "tool_config = {\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"fetch_weather\",\n",
    "                \"description\": \"fetch weather information for a given city\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"city\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The name of the city for which to fetch weather information.\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"city\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"get_dress_code\",\n",
    "                \"description\": \"Get the dress code based on the temperature.\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"temperature\": {\n",
    "                                \"type\": \"number\",\n",
    "                                \"description\": \"The temperature in Celsius.\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"temperature\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "def invoke_bedrock_llm_with_multiple_function_calling(prompt, input_messages=None):\n",
    "    if input_messages is None:\n",
    "        input_messages = [{\"role\": \"user\", \"content\": [{\"text\": prompt}]}]\n",
    "\n",
    "    # print('input messages:', json.dumps(input_messages, indent=4))\n",
    "\n",
    "    response = client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=input_messages,\n",
    "        toolConfig=tool_config\n",
    "    )\n",
    "    output_message = response['output']['message']\n",
    "    # print('output_message:', json.dumps(response['output'], indent=4))\n",
    "    input_messages.append(output_message)\n",
    "    stop_reason = response['stopReason']\n",
    "\n",
    "    if stop_reason == 'tool_use':\n",
    "        response_contents = output_message['content']\n",
    "        tool_responses = []\n",
    "        for response_content in response_contents:\n",
    "            if 'toolUse' in response_content: ## if there is a tool use request in the response content\n",
    "                tool_request = response_content['toolUse']\n",
    "                print(f\"Requesting tool '{tool_request['name']}' ID[{tool_request['toolUseId']}] Input: {tool_request['input']}\")\n",
    "\n",
    "                tool_result = {}\n",
    "                if tool_request['name'] == 'fetch_weather':\n",
    "                    try:\n",
    "                        tool_result = fetch_weather(tool_request['input']['city'])\n",
    "\n",
    "                    except LocationNotFoundException as err:\n",
    "                        tool_result = {\"error\": err.message}\n",
    "\n",
    "                elif tool_request['name'] == 'get_dress_code':\n",
    "                    tool_result = {\"text\": get_dress_code(tool_request['input']['temperature'])}\n",
    "\n",
    "\n",
    "                tool_result_response = {\n",
    "                    \"toolResult\": {\n",
    "                        \"toolUseId\": tool_request['toolUseId'],\n",
    "                        \"content\": [{\"json\": tool_result}]\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                tool_responses.append(tool_result_response)\n",
    "\n",
    "        tool_result_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": tool_responses\n",
    "        }\n",
    "\n",
    "        input_messages.append(tool_result_message)\n",
    "\n",
    "        # Recursively call the function to process the next step\n",
    "        return invoke_bedrock_llm_with_multiple_function_calling(prompt, input_messages)\n",
    "\n",
    "    # If no more tools are required, return the final response\n",
    "    return output_message\n",
    "\n",
    "\n",
    "user_query = \"\"\"I want to travel to a cold location.\n",
    "I'm thinking Tel-Aviv or Moscow. What is the colder city? and what should I wear there?\n",
    "Provide concise textual answer.\n",
    "\"\"\"\n",
    "\n",
    "llm_response = invoke_bedrock_llm_with_multiple_function_calling(user_query)\n",
    "print(\"\\n\")\n",
    "print(llm_response['content'][0]['text'])\n"
   ],
   "id": "a55eb75aeb860b27",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting tool 'fetch_weather' ID[tooluse_8baoSFhITE2qxT0BwoIrfw] Input: {'city': 'Tel-Aviv'}\n",
      "Fetching weather data for city: Tel-Aviv\n",
      "Requesting tool 'fetch_weather' ID[tooluse_8QmzWgnlS7iOpR-PVqBLxA] Input: {'city': 'Moscow'}\n",
      "Fetching weather data for city: Moscow\n",
      "Requesting tool 'get_dress_code' ID[tooluse_cn7_wbkkT-mv6OQSjFE4jA] Input: {'temperature': 3.2}\n",
      "Getting dress code for temperature: 3.2\n",
      "\n",
      "\n",
      "<thinking> I have the dress code for Moscow based on its current temperature. I can now provide the User with the information they requested. </thinking>\n",
      "\n",
      "Hi there! Based on the current temperatures, Moscow is colder than Tel-Aviv. You should wear a light jacket if you travel to Moscow.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Structured Output\n",
    "Structured output is a technique used to format the output of the LLM in a specific way. </br>\n",
    "This can be useful for various applications, such as generating JSON or XML responses. </br>\n",
    "It also helps to ensure that the output is consistent and compliant with the expected format. </br>"
   ],
   "id": "d5976750f0a1ee06"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T15:22:46.490809Z",
     "start_time": "2025-05-07T15:22:44.934414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Create a Bedrock Runtime client in the AWS Region you want to use.\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "##> Show the differences between\n",
    "# model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\" # Model ID\n",
    "# model_id = \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\" # Inference Profile ID\n",
    "# model_id = \"amazon.titan-text-express-v1\"\n",
    "model_id = \"amazon.nova-lite-v1:0\"\n",
    "\n",
    "system_message = \"\"\"You are a car expert with ability to provide technical details about a specific car you are asked for.\n",
    "The response must be in a JSON format.\"\"\"\n",
    "\n",
    "# Start a conversation with the user message.\n",
    "# user_message = \"\"\"Hi i would like to know the technical details about the car Tesla Model Y.\"\"\"\n",
    "user_message = \"\"\"Hi i would like to know the technical details about the car Toyota Prius 2010.\"\"\"\n",
    "\n",
    "assistant_message = \"\"\"```json\n",
    "{\n",
    "    \"car\": {\n",
    "        \"name\": \"Car name\",\n",
    "        \"type\": \"Type of the car. E.g: Electric SUV\",\n",
    "        \"range\": \"The range of the car in miles\",\n",
    "        \"top_speed\": \"Top speed of the car in mph\",\n",
    "        \"acceleration\": \"0-60 mph time in seconds\",\n",
    "        \"battery_capacity\": \"For electric cars, mention the battery capacity in kWh\",\n",
    "        \"features\": Array of features of the car. E.g: \"Autopilot\",\"All-wheel drive\", \"Premium interior\", \"Panoramic glass roof\", \"Advanced safety features\",  \"Over-the-air software updates\", \"ABS brakes\"\n",
    "\n",
    "    }\n",
    "}\n",
    "```\"\"\"\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": system_message}],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [{\"text\": assistant_message}],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": user_message}],\n",
    "    },\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Send the message to the model, using a basic inference configuration.\n",
    "    #  https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/converse.html\n",
    "    response = client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=conversation,\n",
    "        inferenceConfig={\"maxTokens\": 4096, \"stopSequences\": [\"User:\"], \"temperature\": 0, \"topP\": 1},\n",
    "        additionalModelRequestFields={}\n",
    "    )\n",
    "\n",
    "    # Extract and print the response text.\n",
    "    response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    print(response_text)\n",
    "\n",
    "except (ClientError, Exception) as e:\n",
    "    print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "    exit(1)\n"
   ],
   "id": "29cff66c6426ddac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"car\": {\n",
      "        \"name\": \"Toyota Prius 2010\",\n",
      "        \"type\": \"Hybrid Sedan\",\n",
      "        \"range\": \"700 miles\",\n",
      "        \"top_speed\": \"110 mph\",\n",
      "        \"acceleration\": \"9.9 seconds\",\n",
      "        \"battery_capacity\": \"4.4 kWh\",\n",
      "        \"features\": [\n",
      "            \"Hybrid Synergy Drive\",\n",
      "            \"Regenerative braking\",\n",
      "            \"Eco, Normal, and Power driving modes\",\n",
      "            \"Smart key entry\",\n",
      "            \"Bluetooth connectivity\",\n",
      "            \"Six airbags\",\n",
      "            \"ABS with EBD and BA\",\n",
      "            \"Rearview camera\",\n",
      "            \"Power moonroof\",\n",
      "            \"12V DC power outlet\",\n",
      "            \"Six-speaker audio system\"\n",
      "        ]\n",
      "    }\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prompt Caching\n",
    "Prompt caching is a technique used to store and reuse previously generated prompts or responses. </br>\n",
    "This can help to reduce the costs and improve performance. </br>"
   ],
   "id": "89e2e84312e10f7e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
