{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# AWS Bedrock\n",
    "\n",
    "\n",
    "AWS Bedrock is a fully managed service that provides access to foundation models (FMs) from leading AI companies. It allows you to build and scale generative AI applications using these models. In this notebook, we will explore how to use AWS Bedrock with the `boto3` library in Python."
   ],
   "id": "f12e12faf41d4e41"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## LLMs and Foundation Models\n",
    "Foundation models are large-scale machine learning models that are trained on vast amounts of data and can be fine-tuned for specific tasks. AWS Bedrock provides access to various foundation models, including text generation, image generation, and more. </br>\n",
    "\n",
    "* LLM is a stateless function. When \"memory\" or \"context\" is required, it is passed as part of the prompt.\n",
    "* The operation of fetching information from a database or other source is called \"retrieval augmentation generation\" (RAG).\n",
    "* The datasource used to fetch information is called a knowledge base.\n",
    "* LLMs [pricing](https://aws.amazon.com/bedrock/pricing/) depends on the model complexity and the number of tokens (~4 chars â‰… word) in the prompt and the response.\n",
    "* The longer the prompt, the more expensive and the slower it is.\n",
    "\n",
    "### Using AWS Bedrock models and regions\n",
    "\n",
    "AWS LLM models availability is region specific. `us-east-1` or `N. Virginia` is the region where all models are available. </br>\n",
    "In order to use a model of a specific provider, you need to request access to that model in the [Model Access](https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/modelaccess) section </br>\n",
    "In this tutorial we'll be using `us-east-1` region with several different models.\n",
    "\n",
    "### Cross-region inference\n",
    "Cross-region inference is a new AWS feature that enable LLM requests  to be processed in a different geographical region than where the request originated.\n",
    "Instead of being limited to the models and compute resources available in a single region, cross-region inference can automatically route your inference requests to other available regions.\n",
    "You can use the `Inference Profile ID` instead of the `Model ID` to specify the model you want to use. The Inference Profile ID is a unique identifier for a specific model and its associated compute resources in a specific region. </br>\n",
    "For list of available models and their Inference Profile IDs, please refer to the [AWS Cross-region inference Console](https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/inference-profiles).\n",
    "\n"
   ],
   "id": "b3154df63ade026c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conversation API\n",
    "The `conversation` API is used to interact with the LLMs in AWS Bedrock. It allows you to send messages to the model and receive responses. The API supports different roles for the messages, including `user`, `assistant`, and `system`. </br>\n",
    "\n",
    "The `user` role is used for the user's input, the `assistant` role is used for the model's response, and the `system` role is used for system messages that provide context or instructions to the model. </br>\n",
    "\n",
    "### Important Parameters\n",
    "* `modelId`: The ID of the model you want to use. This can be either the model ID or the Inference Profile ID.\n",
    "* `messages`: A list of messages to send to the model. Each message should include a `role` and `content`.\n",
    "* `inferenceConfig`: A dictionary of inference configuration options, such as `maxTokens`, `stopSequences`, `temperature`, and `topP`.\n",
    "*  `temperature`: Controls the randomness of the model's output. A higher temperature (e.g., 0.8) makes the output more random, while a lower temperature (e.g., 0.2) makes it more deterministic.\n",
    "* `topP`: It sets a threshold probability and selects the top tokens whose cumulative probability exceeds the threshold. The model then randomly samples from this set of tokens to generate output. This method can produce more diverse and interesting output than traditional methods that randomly sample the entire vocabulary."
   ],
   "id": "3da7153d3fdfb41d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T09:28:23.919803Z",
     "start_time": "2025-05-15T09:28:23.497095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import boto3\n",
    "from enum import Enum\n",
    "\n",
    "#  Get the list of available models in the region\n",
    "boto3_bedrock = boto3.client('bedrock')\n",
    "\n",
    "class LLMModel(Enum):\n",
    "    \"\"\"Enum for Bedrock models.\"\"\"\n",
    "    # Anthropic\n",
    "    CLAUDE_3_5_V1 = 'anthropic.claude-3-5-sonnet-20240620-v1:0'\n",
    "    CLAUDE_3_5_v2 = 'us.anthropic.claude-3-5-sonnet-20241022-v2:0' # Inference Profile ID\n",
    "    # Amazon\n",
    "    NOVA_LITE = 'amazon.nova-lite-v1:0'\n",
    "    NOVA_PRO = 'amazon.nova-pro-v1:0'\n",
    "    TITAN_LITE = 'amazon.titan-text-lite-v1'\n",
    "    TITAN_EXPRESS = 'amazon.titan-text-express-v1'\n",
    "    META_LLMA3_1B = 'meta.llama3-2-1b-instruct-v1:0'\n",
    "    META_LLMA3_3B = 'meta.llama3-2-3b-instruct-v1:0'\n",
    "\n",
    "[model['modelId'] for model in boto3_bedrock.list_foundation_models()['modelSummaries']]\n"
   ],
   "id": "9c078eca04c9733f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMModel.TITAN_EXPRESS\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Zero Shot\n",
    "The following example is called `zero-shot` prompting. </br>\n",
    "There are no examples or context provided to the model. The model is expected to understand the task and generate a response based on its own training. </br>\n",
    "\n",
    "\n",
    "> change the models in order to switch between different LLMs. </br>"
   ],
   "id": "e5b83cdab347501d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T09:05:20.627616Z",
     "start_time": "2025-05-15T09:05:18.986065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Create a Bedrock Runtime client in the AWS Region you want to use.\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "\n",
    "\n",
    "# Start a conversation with the user message.\n",
    "user_message = \"\"\"Meeting transcript:\n",
    "Miguel: Hi Brant, I want to discuss the workstream  for our new product launch\n",
    "Brant: Sure Miguel, is there anything in particular you want to discuss?\n",
    "Miguel: Yes, I want to talk about how users enter into the product.\n",
    "Brant: Ok, in that case let me add in Namita.\n",
    "Namita: Hey everyone\n",
    "Brant: Hi Namita, Miguel wants to discuss how users enter into the product.\n",
    "Miguel: its too complicated and we should remove friction. for example, why do I need to fill out additional forms?  I also find it difficult to find where to access the product when I first land on the landing page.\n",
    "Brant: I would also add that I think there are too many steps.\n",
    "Namita: Ok, I can work on the landing page to make the product more discoverable but brant can you work on the additional forms?\n",
    "Brant: Yes but I would need to work with James from another team as he needs to unblock the sign up workflow.  Miguel can you document any other concerns so that I can discuss with James only once?\n",
    "Miguel: Sure.\n",
    "\n",
    "From the meeting transcript above, Create a list of action items for each person.\n",
    "\"\"\"\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": user_message}],\n",
    "    }\n",
    "]\n",
    "\n",
    "model_id = LLMModel.NOVA_LITE.value # Inference Profile ID\n",
    "try:\n",
    "    # Send the message to the model, using a basic inference configuration.\n",
    "    #  https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/converse.html\n",
    "    response = client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=conversation,\n",
    "        inferenceConfig={\"maxTokens\": 4096, \"stopSequences\": [\"User:\"], \"temperature\": 0, \"topP\": 1},\n",
    "        additionalModelRequestFields={}\n",
    "    )\n",
    "\n",
    "    # Extract and print the response text.\n",
    "    response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    print(response_text)\n",
    "\n",
    "except (ClientError, Exception) as e:\n",
    "    print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "    exit(1)\n"
   ],
   "id": "8b88007436436904",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the meeting transcript, here is a list of action items for each person involved:\n",
      "\n",
      "**Miguel:**\n",
      "1. Document any other concerns or issues related to the user entry process, specifically focusing on the sign-up workflow and landing page. This documentation will be used by Brant to discuss with James from the other team.\n",
      "\n",
      "**Brant:**\n",
      "1. Collaborate with James from the other team to address the issues related to the additional forms and the sign-up workflow. Use Miguel's documented concerns to streamline the discussion and ensure all relevant points are covered.\n",
      "\n",
      "**Namita:**\n",
      "1. Work on improving the landing page to make the product more discoverable for users. This includes addressing the issue of users finding it difficult to access the product when they first land on the landing page.\n",
      "\n",
      "By following these action items, the team can work together to improve the user entry process for the new product launch, ultimately reducing friction and enhancing the overall user experience.\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Understanding the response\n",
    "\n",
    "The LLM response is a JSON object that contains important information in addition to the generated text. </br>\n",
    "When analyzing and comparing LLM responses, look for the following fields:\n",
    "\n",
    "* `latency`: The time taken to process the request and generate a response.\n",
    "* `usage`: The number of tokens used in the prompt and the response. This is important for cost estimation.\n",
    "* `RequestId`: A unique identifier for the request. This can be useful for debugging and tracking purposes.\n"
   ],
   "id": "87fc54fae6e8428f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    # print the response with json format and indentation\n",
    "    import json\n",
    "\n",
    "    print(json.dumps(response, indent=4, sort_keys=True))\n"
   ],
   "id": "2a5dd1faadc63f75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## One Shot\n",
    "One-shot prompting is a technique used to provide a single example to the model, helping it understand the task better. </br>\n",
    "\n",
    "In this example, we will use one-shot prompting to create a meeting summary and action items. </br>\n",
    "We will use the `assistant` role to provide the model with a system message that describes its role and the task it needs to perform. </br>\n",
    "In addition, we will provide a user message that contains the meeting transcript. </br>\n"
   ],
   "id": "951d9c212f656c79"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T09:32:27.711407Z",
     "start_time": "2025-05-15T09:32:17.497707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "# Start a conversation with the user message.\n",
    "system_message = \"\"\"You are a meeting assistant that helps to summarize the meeting and create action items for each person.\n",
    "You are given a meeting transcript and you need to create a list of action items for each person in the meeting.\n",
    "The action items should be in the following format:\n",
    "\n",
    "=== Miguel ===\n",
    "    - action item 1\n",
    "    - action item 2\n",
    "=== Brant ===\n",
    "    - action item1\n",
    "    - action item 2\n",
    "=== Namita ===\n",
    "    - action item 1\n",
    "    - action item 2\n",
    "\n",
    "The action items should be clear and concise.\n",
    "The action items should be based on the meeting transcript and should not include any additional information.\n",
    "In the end of the response, mention the meeting participants and their roles in a JSON format.\n",
    "In addition rank their involvement in the meeting from 1 to 5, where 5 is the most involved and 1 is the least involved.\n",
    "{\n",
    "    {\"name\": \"Miguel\", \"role\": \"Product Manager\", \"involvement\": 5},\n",
    "    {\"name\": \"Brant\", \"role\": \"Software Engineer\", \"involvement\": 4},\n",
    "    {\"name\": \"Namita\", \"role\": \"UX Designer\", \"involvement\": 3}\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "user_message = \"\"\"Meeting transcript:\n",
    "Miguel: Hi Brant, I want to discuss the workstream  for our new product launch\n",
    "Brant: Sure Miguel, is there anything in particular you want to discuss?\n",
    "Miguel: Yes, I want to talk about how users enter into the product.\n",
    "Brant: Ok, in that case let me add in Namita.\n",
    "Namita: Hey everyone\n",
    "Brant: Hi Namita, Miguel wants to discuss how users enter into the product.\n",
    "Miguel: its too complicated and we should remove friction. for example, why do I need to fill out additional forms?  I also find it difficult to find where to access the product when I first land on the landing page.\n",
    "Brant: I would also add that I think there are too many steps.\n",
    "Namita: Ok, I can work on the landing page to make the product more discoverable but brant can you work on the additional forms?\n",
    "Brant: Yes but I would need to work with James from another team as he needs to unblock the sign up workflow.  Miguel can you document any other concerns so that I can discuss with James only once?\n",
    "Miguel: Sure.\n",
    "\"\"\"\n",
    "\n",
    "user_message2 = \"\"\"Meeting transcript:\n",
    "Attendees: Shimon (PM), Igor (CTO), Avi (Tech Lead)\n",
    "\n",
    "Shimon: Right, let's discuss integrating code coverage into the main pipeline. What are the main benefits and drawbacks?\n",
    "\n",
    "Igor: The major pro is improved code quality and maintainability. It gives us objective data on test effectiveness, reducing future bugs and technical debt. Itâ€™s a standard best practice.\n",
    "\n",
    "Avi: Agreed, Igor. The con is potential friction â€“ longer build times initially, and developers needing to potentially refactor or add more tests, impacting velocity slightly. We need to manage thresholds carefully.\n",
    "\n",
    "Shimon: So, increased confidence in quality versus a possible short-term slowdown?\n",
    "\n",
    "Igor: Exactly. A worthwhile investment for long-term stability.\n",
    "\n",
    "Avi: We can mitigate the impact by starting with warnings, not blockers.\n",
    "Barkoni: We stopped doing code coverage in our planet before we wrote the first line of code.\n",
    "Shimon: Barkoni, can you elaborate on that?\n",
    "Barkoni: What's the point? You will not understand.\n",
    "\"\"\"\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": 'you are a meeting assistant that summarize meetings'}],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [{\"text\": system_message}],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": user_message2}],\n",
    "    }\n",
    "]\n",
    "\n",
    "model_id = LLMModel.CLAUDE_3_5_v2.value # Inference Profile ID\n",
    "\n",
    "try:\n",
    "    # Send the message to the model, using a basic inference configuration.\n",
    "    #  https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/converse.html\n",
    "    response = client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=conversation,\n",
    "        inferenceConfig={\"maxTokens\": 1000,  \"temperature\": 0, \"topP\": 0.9},\n",
    "        additionalModelRequestFields={}\n",
    "    )\n",
    "\n",
    "    # Extract and print the response text.\n",
    "    response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    print(response_text)\n",
    "\n",
    "except (ClientError, Exception) as e:\n",
    "    print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "    exit(1)\n"
   ],
   "id": "82b0312e27d994a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meeting Summary and Action Items:\n",
      "\n",
      "=== Shimon ===\n",
      "- Create a proposal document outlining the code coverage implementation plan\n",
      "- Schedule a follow-up meeting to discuss specific threshold values\n",
      "- Communicate the planned changes to the development team\n",
      "\n",
      "=== Igor ===\n",
      "- Research and recommend code coverage tools that would best integrate with the current pipeline\n",
      "- Prepare technical documentation for code coverage implementation\n",
      "- Define initial threshold recommendations based on current codebase\n",
      "\n",
      "=== Avi ===\n",
      "- Create a phased implementation plan starting with warnings\n",
      "- Identify high-priority areas for initial coverage focus\n",
      "- Prepare developer guidelines for test coverage requirements\n",
      "\n",
      "=== Barkoni ===\n",
      "- Share detailed experience and learnings from previous implementation attempts\n",
      "- Provide specific concerns about code coverage implementation in writing\n",
      "\n",
      "Participants Information:\n",
      "{\n",
      "    {\"name\": \"Shimon\", \"role\": \"Product Manager\", \"involvement\": 4},\n",
      "    {\"name\": \"Igor\", \"role\": \"CTO\", \"involvement\": 5},\n",
      "    {\"name\": \"Avi\", \"role\": \"Tech Lead\", \"involvement\": 4},\n",
      "    {\"name\": \"Barkoni\", \"role\": \"Unknown Role\", \"involvement\": 1}\n",
      "}\n",
      "\n",
      "Note: The meeting focused on discussing the integration of code coverage into the main pipeline, weighing the benefits of improved code quality against potential short-term velocity impacts. The discussion was constructive between Shimon, Igor, and Avi, with some resistance from Barkoni who provided minimal constructive input.\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Few Shots\n",
    "Few-shot prompting is a technique used to provide multiple examples to the model, helping it understand the task better. </br>\n",
    "Each example is called a `shot` and it contains a prompt and a desired response. </br>\n",
    "The `shots` can be based on real-world examples or synthetic examples that exemplify the task. </br>\n"
   ],
   "id": "bf7235b12365041b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T09:33:28.148586Z",
     "start_time": "2025-05-15T09:33:06.477263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "model_id = LLMModel.CLAUDE_3_5_v2.value\n",
    "\n",
    "# Start a conversation with the user message.\n",
    "\n",
    "prompt = \"\"\"You are a product manager for a website builder platform (like Wix). For each new feature request, you need to outline the functional requirements, highlight important UI/UX considerations, and explain the business advantages. Respond in the following format for each feature request:\n",
    "\n",
    "**Functional Requirement:**\n",
    "[Clearly describe what the feature should do.]\n",
    "\n",
    "**UI/UX Important Points:**\n",
    "[List key considerations for the user interface and user experience of this feature.]\n",
    "\n",
    "**Business Advantage:**\n",
    "[Explain how this feature benefits the website builder platform.]\n",
    "\n",
    "----\n",
    "Examples:\n",
    "\n",
    "**Feature Request:** Add a built-in image editor with basic cropping and resizing tools.\n",
    "\n",
    "**Functional Requirement:**\n",
    "Users should be able to crop and resize images directly within the website builder without needing to upload pre-edited files. Supported formats should include JPG, PNG, and GIF. The editor should offer standard aspect ratio presets and freeform resizing.\n",
    "\n",
    "**UI/UX Important Points:**\n",
    "- The image editor should be easily accessible within the image settings panel.\n",
    "- Controls for cropping and resizing should be intuitive and visually clear.\n",
    "- Users should see a real-time preview of their edits.\n",
    "- An option to revert to the original image should be available.\n",
    "\n",
    "**Business Advantage:**\n",
    "- Improves user convenience and efficiency by eliminating the need for external image editing tools.\n",
    "- Can attract users who need quick image adjustments without complex software.\n",
    "- May reduce support requests related to image sizing issues.\n",
    "\n",
    "---\n",
    "\n",
    "**Feature Request:** Implement a library of pre-designed website sections (e.g., headers, footers, contact forms).\n",
    "\n",
    "**Functional Requirement:**\n",
    "Users should be able to browse and insert professionally designed website sections into their pages with a single click. The library should include various categories and styles, and users should be able to customize the content and styling of these sections.\n",
    "\n",
    "**UI/UX Important Points:**\n",
    "- The section library should be well-organized and easy to navigate, possibly with categories and search functionality.\n",
    "- Previews of the sections should be clear and representative of the final design.\n",
    "- The insertion process should be seamless and not disrupt the user's workflow.\n",
    "- Users should have clear visual cues on how to customize the content of the inserted sections.\n",
    "\n",
    "**Business Advantage:**\n",
    "- Speeds up the website creation process for users, making it more appealing to beginners.\n",
    "- Provides users with professionally designed elements, potentially leading to more visually appealing websites.\n",
    "- Can encourage users to build more comprehensive websites by offering readily available components.\n",
    "\n",
    "---\n",
    "\n",
    "**Feature Request:** Allow users to embed social media feeds (e.g., Instagram, Twitter) directly onto their websites.\n",
    "\n",
    "**Functional Requirement:**\n",
    "Users should be able to connect their social media accounts and display their latest posts on their website. They should have options to customize the number of posts displayed and the layout of the feed.\n",
    "\n",
    "**UI/UX Important Points:**\n",
    "- The connection process to social media accounts should be secure and straightforward.\n",
    "- Embedding options should be easily accessible within the website editor.\n",
    "- Users should have control over the visual presentation of the feed to match their website's design.\n",
    "- The embedded feed should be responsive and display correctly on different devices.\n",
    "\n",
    "**Business Advantage:**\n",
    "- Enhances user engagement by allowing them to showcase their social media presence.\n",
    "- Can drive traffic between the user's website and their social media profiles.\n",
    "- Adds dynamic content to websites, making them more lively and up-to-date.\n",
    "\n",
    "---\n",
    "\n",
    "Here is the actual Feature Request: {0}\n",
    "\"\"\"\n",
    "\n",
    "# feature_request = \"\"\"Integrate with a third-party payment processor (e.g., Stripe, PayPal) to enable e-commerce functionality.\"\"\"\n",
    "feature_request = \"\"\"I want a no-code feature for my website builder that allows users to create custom forms with drag-and-drop functionality. The forms should support various field types (text, checkbox, radio button, dropdown) and allow users to customize the layout and design. Additionally, users should be able to set up email notifications for form submissions and view submission data in a user-friendly dashboard.\"\"\"\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": prompt.format(feature_request)}],\n",
    "        # \"content\": [{\"text\": prompt.format(\"Implement A built-in image editor with basic cropping and resizing tools.\")}],\n",
    "    }\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Send the message to the model, using a basic inference configuration.\n",
    "    #  https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/converse.html\n",
    "    response = client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=conversation,\n",
    "        inferenceConfig={\"maxTokens\": 4096, \"stopSequences\": [\"User:\"], \"temperature\": 0, \"topP\": 1},\n",
    "        additionalModelRequestFields={}\n",
    "    )\n",
    "\n",
    "    # Extract and print the response text.\n",
    "    response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    print(response_text)\n",
    "\n",
    "except (ClientError, Exception) as e:\n",
    "    print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "    exit(1)\n"
   ],
   "id": "f163013747606b0f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll analyze this feature request for a no-code form builder:\n",
      "\n",
      "**Functional Requirements:**\n",
      "1. Drag-and-drop form builder interface with:\n",
      "   - Standard field types (text, email, phone, checkbox, radio, dropdown, file upload)\n",
      "   - Multi-column layout options\n",
      "   - Field validation rules (required fields, email format, number ranges)\n",
      "   - Custom field labels and placeholder text\n",
      "   - Conditional logic (show/hide fields based on responses)\n",
      "\n",
      "2. Form submission handling:\n",
      "   - Secure data storage\n",
      "   - Automated email notifications to form owner\n",
      "   - Custom auto-response emails to form submitter\n",
      "   - Export functionality for submission data (CSV, Excel)\n",
      "   - File attachment storage and management\n",
      "\n",
      "3. Form management dashboard:\n",
      "   - List of all created forms\n",
      "   - Submission statistics and analytics\n",
      "   - Search and filter capabilities for submissions\n",
      "   - Archive/delete options for old submissions\n",
      "\n",
      "**UI/UX Important Points:**\n",
      "- Clear visual cues for draggable elements and drop zones\n",
      "- Real-time preview of form appearance while building\n",
      "- Intuitive field property controls (similar to standard website builder controls)\n",
      "- Mobile-responsive form preview option\n",
      "- Visual feedback for validation rules and error messages\n",
      "- Easy-to-understand conditional logic interface\n",
      "- Clean, organized submission dashboard with sorting options\n",
      "- Clear distinction between required and optional fields\n",
      "- Accessible design patterns for form elements\n",
      "- Simple toggle for enabling/disabling forms\n",
      "\n",
      "**Business Advantages:**\n",
      "1. Revenue opportunities:\n",
      "   - Can be offered as a premium feature\n",
      "   - Different pricing tiers based on number of forms/submissions\n",
      "   - Additional storage space for file uploads as upsell\n",
      "\n",
      "2. Platform value enhancement:\n",
      "   - Essential functionality for business websites\n",
      "   - Reduces dependency on third-party form solutions\n",
      "   - Keeps users within the platform ecosystem\n",
      "\n",
      "3. Market competitiveness:\n",
      "   - Forms are a crucial feature for website builders\n",
      "   - No-code solution appeals to non-technical users\n",
      "   - Built-in analytics provides valuable user insights\n",
      "\n",
      "4. User retention:\n",
      "   - Forms create ongoing engagement with the platform\n",
      "   - Submission data makes websites \"sticky\"\n",
      "   - Regular form submissions encourage continued subscription\n",
      "\n",
      "5. Feature expansion potential:\n",
      "   - Integration with CRM systems\n",
      "   - Payment processing capabilities\n",
      "   - Advanced workflow automation\n",
      "   - Form templates marketplace\n",
      "\n",
      "This feature would significantly enhance the platform's capability to serve business users while creating multiple opportunities for monetization and user retention.\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tool Use / Function Calling\n",
    "LLMs can be enhanced with tools and APIs to provide additional functionality. </br>\n",
    "For example, you can use APIs to fetch data from external sources, perform calculations, or interact with other services. </br>\n",
    "In this example, we will use the `python` tool to perform calculations and the `weather` API to fetch weather data. </br>\n",
    "\n",
    "**Tool Use Workflow:**\n",
    "\n",
    "1.  **Define Tools:** Specify tools with names, descriptions, and argument schemas. Include a user prompt (e.g., \"What's the weather like in New York today?\").\n",
    "2.  **LLM Decides:** The LLM determines if a tool is necessary and halts text generation if so.\n",
    "3.  **JSON Call:** The LLM outputs a JSON object containing the selected tool and its parameter values.\n",
    "4.  **Execute & Return:** The system extracts parameters, runs the tool, and returns the output to the LLM.\n",
    "5.  **Generate Answer:** The LLM uses the tool output to create a final response.\n",
    "\n",
    "![Tool Calling](../resources/images/tool-call-schema-removebg.webp)\n",
    "### References\n",
    "\n",
    "* [AWS Bedrock: Converse API tool use examples](https://docs.aws.amazon.com/bedrock/latest/userguide/tool-use-examples.html)\n",
    "* [Guide to Tool Calling](https://www.analyticsvidhya.com/blog/2024/08/tool-calling-in-llms/)\n",
    "\n",
    "\n"
   ],
   "id": "86eaa370c9df2983"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T09:38:33.446220Z",
     "start_time": "2025-05-15T09:38:33.369794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "model_id = \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\" # Inference Profile ID\n",
    "\n",
    "class LocationNotFoundException(Exception):\n",
    "    \"\"\"Raised when a location is not found.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Weather API: Fetch weather data from weatherapi.com, api key from user: mistriela@yopmail.com\n",
    "def fetch_weather(city):\n",
    "    print('Fetching weather data for city:', city)\n",
    "    base_url = \"https://api.weatherapi.com/v1/current.json\"\n",
    "    params = {\n",
    "        \"q\": city,\n",
    "        \"key\": \"6e7b99ab0f454283ab9125132252104\",\n",
    "        \"aqi\": \"no\"  # Get temperature in Celsius\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()  # Raise an error for bad HTTP responses\n",
    "        weather_data = response.json()\n",
    "        return {\n",
    "            \"city\": weather_data[\"location\"][\"name\"],\n",
    "            \"temperature\": weather_data[\"current\"][\"temp_c\"],\n",
    "            \"description\": weather_data[\"current\"][\"condition\"][\"text\"]\n",
    "        }\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise LocationNotFoundException(f\"Error fetching weather data: {e}\")\n",
    "\n",
    "\n",
    "def invoke_bedrock_llm_with_function_calling(prompt: str, model: LLMModel):\n",
    "    \"\"\"\n",
    "    Invokes an LLM on AWS Bedrock with function calling to get weather information.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The user's query (e.g., \"What's the weather in London?\").\n",
    "        model_id (str): The ID of the Bedrock LLM model to use.\n",
    "        region_name (str): The AWS region.\n",
    "\n",
    "    Returns:\n",
    "        str: The LLM's response, which may include the weather information\n",
    "             or an error message.\n",
    "    \"\"\"\n",
    "\n",
    "    tool_config = {\n",
    "        \"tools\": [\n",
    "            {\n",
    "                \"toolSpec\": {\n",
    "                    \"name\": \"fetch_weather\",\n",
    "                    \"description\": \"fetch weather information for a given city\",\n",
    "                    \"inputSchema\": {\n",
    "                        \"json\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"city\": {\n",
    "                                    \"type\": \"string\",\n",
    "                                    \"description\": \"The name of the city for which to fetch weather information.\"\n",
    "                                }\n",
    "                            },\n",
    "                            \"required\": [\n",
    "                                \"city\"\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "    # Note: there is no usage of `assistant` role as part of the prompt. It's the LLM responsibility to understand that a tool is required.\n",
    "    input_messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": prompt}]\n",
    "    }]\n",
    "\n",
    "    # Send the initial message to the model. if there is a weather request, the model will stop and return a tool use request.\n",
    "    # Several tools requests can be sent in a single response.\n",
    "    response = client.converse(\n",
    "        modelId=model.value,\n",
    "        messages=input_messages,\n",
    "        toolConfig=tool_config\n",
    "    )\n",
    "    output_message = response['output']['message']\n",
    "    input_messages.append(output_message)\n",
    "    stop_reason = response['stopReason']\n",
    "\n",
    "    print(f\"LLM Message: {json.dumps(response, indent=4)}\")\n",
    "    # Print the LLM message and stop reason.\n",
    "    # See if any response starts with a json object root node: \"toolUse\"\n",
    "    if stop_reason == 'tool_use':\n",
    "        # Tool use requested. Call the tool and send the result to the model.\n",
    "        tool_requests = output_message['content']\n",
    "        for tool_request in tool_requests:\n",
    "            if 'toolUse' in tool_request:\n",
    "                tool = tool_request['toolUse']\n",
    "                print(f\"Requesting tool {tool['name']} Request: {tool['toolUseId']} \")\n",
    "\n",
    "                if tool['name'] == 'fetch_weather':\n",
    "                    tool_result = {}\n",
    "                    try:\n",
    "                        weather_data = fetch_weather(tool['input']['city'])\n",
    "                        tool_result = {\n",
    "                            \"toolUseId\": tool['toolUseId'],\n",
    "                            \"content\": [{\"json\": weather_data}]\n",
    "                        }\n",
    "                    except LocationNotFoundException as err:\n",
    "                        tool_result = {\n",
    "                            \"toolUseId\": tool['toolUseId'],\n",
    "                            \"content\": [{\"text\":  err.args[0]}],\n",
    "                            \"status\": 'error'\n",
    "                        }\n",
    "\n",
    "                    tool_result_message = {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [{\"toolResult\": tool_result}]\n",
    "                    }\n",
    "                    input_messages.append(tool_result_message)\n",
    "\n",
    "                    # Send the tool result to the model.\n",
    "                    response = client.converse(\n",
    "                        modelId=model.value,\n",
    "                        messages=input_messages,\n",
    "                        toolConfig=tool_config\n",
    "                    )\n",
    "                    output_message = response['output']['message']\n",
    "\n",
    "    # print the final response from the model.\n",
    "    # for content in output_message['content']:\n",
    "    #     print(json.dumps(content, indent=4))\n",
    "\n",
    "    return output_message\n",
    "\n"
   ],
   "id": "fc75e70526d083eb",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T09:39:25.032753Z",
     "start_time": "2025-05-15T09:38:54.850027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_query = \"What is the weather like in London?\"\n",
    "llm_response = invoke_bedrock_llm_with_function_calling(user_query, LLMModel.CLAUDE_3_5_v2)\n",
    "print(llm_response['content'][0]['text'])"
   ],
   "id": "3f737edaae3d4168",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Message: {\n",
      "    \"ResponseMetadata\": {\n",
      "        \"RequestId\": \"3c64c8c5-63da-4ae0-938a-f2df8c23c56a\",\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"HTTPHeaders\": {\n",
      "            \"date\": \"Thu, 15 May 2025 09:39:00 GMT\",\n",
      "            \"content-type\": \"application/json\",\n",
      "            \"content-length\": \"367\",\n",
      "            \"connection\": \"keep-alive\",\n",
      "            \"x-amzn-requestid\": \"3c64c8c5-63da-4ae0-938a-f2df8c23c56a\"\n",
      "        },\n",
      "        \"RetryAttempts\": 0\n",
      "    },\n",
      "    \"output\": {\n",
      "        \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": [\n",
      "                {\n",
      "                    \"text\": \"I'll help you check the weather in London using the fetch_weather function.\"\n",
      "                },\n",
      "                {\n",
      "                    \"toolUse\": {\n",
      "                        \"toolUseId\": \"tooluse_esZ_6Mf5Skeigm1Khu5Jgw\",\n",
      "                        \"name\": \"fetch_weather\",\n",
      "                        \"input\": {\n",
      "                            \"city\": \"London\"\n",
      "                        }\n",
      "                    }\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"stopReason\": \"tool_use\",\n",
      "    \"usage\": {\n",
      "        \"inputTokens\": 399,\n",
      "        \"outputTokens\": 70,\n",
      "        \"totalTokens\": 469\n",
      "    },\n",
      "    \"metrics\": {\n",
      "        \"latencyMs\": 4142\n",
      "    }\n",
      "}\n",
      "Requesting tool fetch_weather Request: tooluse_esZ_6Mf5Skeigm1Khu5Jgw \n",
      "Fetching weather data for city: London\n",
      "Currently in London, it is overcast with a temperature of 12.3Â°C.\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tool Chaining\n",
    "Tool chaining is a technique used to combine multiple tools or functions in a sequence to achieve a more complex task. </br>\n",
    "In this example, we will use tool chaining **recursively** to fetch the weather data and then determine the dress code based on the temperature. </br>\n",
    "\n",
    "### Execution Flow\n",
    "We will ask the model to provide a dress code for the hotter city. </br>\n",
    "The model will need to understand that it needs to call the `fetch_weather` tool to get the weather data for **both** cities. </br>\n",
    "Then, it will call the `get_dress_code` tool to determine the dress code based on the temperature. </br>\n",
    "\n",
    "### Important Lookouts\n",
    "* Experiment with the different models, check if `lite` models can be used instead of `pro` models.\n",
    "* Check the input/output of the llm - see how the conversation is built gradually."
   ],
   "id": "38104e3d96629d9b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T09:56:03.751127Z",
     "start_time": "2025-05-15T09:55:10.077574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "\n",
    "class LocationNotFoundException(Exception):\n",
    "    \"\"\"Raised when a location is not found.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Weather API: Fetch weather data from weatherapi.com, api key from user: mistriela@yopmail.com\n",
    "def fetch_weather(city):\n",
    "    print('Fetching weather data for city:', city)\n",
    "    base_url = \"https://api.weatherapi.com/v1/current.json\"\n",
    "    params = {\n",
    "        \"q\": city,\n",
    "        \"key\": \"6e7b99ab0f454283ab9125132252104\",\n",
    "        \"aqi\": \"no\"  # Get temperature in Celsius\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()  # Raise an error for bad HTTP responses\n",
    "        weather_data = response.json()\n",
    "        return {\n",
    "            \"city\": weather_data[\"location\"][\"name\"],\n",
    "            \"temperature\": weather_data[\"current\"][\"temp_c\"],\n",
    "            \"description\": weather_data[\"current\"][\"condition\"][\"text\"]\n",
    "        }\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise LocationNotFoundException(f\"Error fetching weather data: {e}\")\n",
    "\n",
    "def get_dress_code(temperature: float) -> str:\n",
    "    \"\"\"\n",
    "    Determine the dress code based on the temperature.\n",
    "    Temperature ranges: -20 to 50 degrees Celsius.\n",
    "    Args:\n",
    "        temperature (float): The temperature in Celsius.\n",
    "\n",
    "    Returns:\n",
    "        str: The recommended dress code.\n",
    "    \"\"\"\n",
    "    print('Getting dress code for temperature:', temperature)\n",
    "    if temperature < -10:\n",
    "        return \"Wear a heavy winter coat, gloves, and a warm hat.\"\n",
    "    elif -10 <= temperature < 0:\n",
    "        return \"Wear a warm coat and gloves.\"\n",
    "    elif 0 <= temperature < 10:\n",
    "        return \"Wear a light jacket.\"\n",
    "    elif 10 <= temperature < 20:\n",
    "        return \"Wear a long-sleeve shirt.\"\n",
    "    elif 20 <= temperature < 30:\n",
    "        return \"Wear a short-sleeve shirt.\"\n",
    "    else:\n",
    "        return \"Wear summer clothes.\"\n",
    "\n",
    "tool_config = {\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"fetch_weather\",\n",
    "                \"description\": \"fetch weather information for a given city\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"city\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The name of the city for which to fetch weather information.\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"city\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"get_dress_code\",\n",
    "                \"description\": \"Get the dress code based on the temperature.\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"temperature\": {\n",
    "                                \"type\": \"number\",\n",
    "                                \"description\": \"The temperature in Celsius.\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"temperature\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "def invoke_bedrock_llm_with_multiple_function_calling(prompt: str, model: LLMModel, input_messages=None):\n",
    "    if input_messages is None:\n",
    "        input_messages = [{\"role\": \"user\", \"content\": [{\"text\": prompt}]}]\n",
    "\n",
    "    # print('input messages:', json.dumps(input_messages, indent=4))\n",
    "\n",
    "    response = client.converse(\n",
    "        modelId=model.value,\n",
    "        messages=input_messages,\n",
    "        toolConfig=tool_config\n",
    "    )\n",
    "    output_message = response['output']['message']\n",
    "    # print('output_message:', json.dumps(response['output'], indent=4))\n",
    "    input_messages.append(output_message)\n",
    "    stop_reason = response['stopReason']\n",
    "\n",
    "    if stop_reason == 'tool_use':\n",
    "        response_contents = output_message['content']\n",
    "        tool_responses = []\n",
    "        for response_content in response_contents:\n",
    "            if 'toolUse' in response_content: ## if there is a tool use request in the response content\n",
    "                tool_request = response_content['toolUse']\n",
    "                print(f\"Requesting tool '{tool_request['name']}' ID[{tool_request['toolUseId']}] Input: {tool_request['input']}\")\n",
    "\n",
    "                tool_result = {}\n",
    "                if tool_request['name'] == 'fetch_weather':\n",
    "                    try:\n",
    "                        tool_result = fetch_weather(tool_request['input']['city'])\n",
    "\n",
    "                    except LocationNotFoundException as err:\n",
    "                        tool_result = {\"error\": err.message}\n",
    "\n",
    "                elif tool_request['name'] == 'get_dress_code':\n",
    "                    tool_result = {\"text\": get_dress_code(tool_request['input']['temperature'])}\n",
    "\n",
    "\n",
    "                tool_result_response = {\n",
    "                    \"toolResult\": {\n",
    "                        \"toolUseId\": tool_request['toolUseId'],\n",
    "                        \"content\": [{\"json\": tool_result}]\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                tool_responses.append(tool_result_response)\n",
    "\n",
    "        tool_result_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": tool_responses\n",
    "        }\n",
    "\n",
    "        input_messages.append(tool_result_message)\n",
    "\n",
    "        # Recursively call the function to process the next step\n",
    "        return invoke_bedrock_llm_with_multiple_function_calling(prompt, model, input_messages)\n",
    "\n",
    "    # If no more tools are required, return the final response\n",
    "    return output_message\n",
    "\n",
    "\n",
    "user_query = \"\"\"I want to travel to a cold location.\n",
    "I'm thinking Tel-Aviv or Moscow. What is the colder city? and what should I wear there?\n",
    "Provide concise textual answer.\n",
    "\"\"\"\n",
    "\n",
    "llm_response = invoke_bedrock_llm_with_multiple_function_calling(user_query, LLMModel.CLAUDE_3_5_v2)\n",
    "print(\"\\n\")\n",
    "print(llm_response['content'][0]['text'])\n"
   ],
   "id": "a55eb75aeb860b27",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting tool 'fetch_weather' ID[tooluse_79dfHnB6Qw-GlebEkvhnUw] Input: {'city': 'Tel Aviv'}\n",
      "Fetching weather data for city: Tel Aviv\n",
      "Requesting tool 'fetch_weather' ID[tooluse_OVUCz2GiR2GzEH1F6VPCyA] Input: {'city': 'Moscow'}\n",
      "Fetching weather data for city: Moscow\n",
      "Requesting tool 'get_dress_code' ID[tooluse_jApkOgPsQJWVrOd6ga2ogg] Input: {'temperature': 15.3}\n",
      "Getting dress code for temperature: 15.3\n",
      "\n",
      "\n",
      "Moscow is the colder city at 15.3Â°C, while Tel Aviv is warmer at 24Â°C. For Moscow's current temperature, you should wear a long-sleeve shirt.\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Structured Output\n",
    "Structured output is a technique used to format the output of the LLM in a specific way. </br>\n",
    "This can be useful for various applications, such as generating JSON or XML responses. </br>\n",
    "It also helps to ensure that the output is consistent and compliant with the expected format. </br>"
   ],
   "id": "d5976750f0a1ee06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Create a Bedrock Runtime client in the AWS Region you want to use.\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "##> Show the differences between\n",
    "# model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\" # Model ID\n",
    "# model_id = \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\" # Inference Profile ID\n",
    "# model_id = \"amazon.titan-text-express-v1\"\n",
    "model_id = \"amazon.nova-lite-v1:0\"\n",
    "\n",
    "system_message = \"\"\"You are a car expert with ability to provide technical details about a specific car you are asked for.\n",
    "The response must be in a JSON format.\"\"\"\n",
    "\n",
    "# Start a conversation with the user message.\n",
    "# user_message = \"\"\"Hi i would like to know the technical details about the car Tesla Model Y.\"\"\"\n",
    "user_message = \"\"\"Hi i would like to know the technical details about the car Toyota Prius 2010.\"\"\"\n",
    "\n",
    "assistant_message = \"\"\"```json\n",
    "{\n",
    "    \"car\": {\n",
    "        \"name\": \"Car name\",\n",
    "        \"type\": \"Type of the car. E.g: Electric SUV\",\n",
    "        \"range\": \"The range of the car in miles\",\n",
    "        \"top_speed\": \"Top speed of the car in mph\",\n",
    "        \"acceleration\": \"0-60 mph time in seconds\",\n",
    "        \"battery_capacity\": \"For electric cars, mention the battery capacity in kWh\",\n",
    "        \"features\": Array of features of the car. E.g: \"Autopilot\",\"All-wheel drive\", \"Premium interior\", \"Panoramic glass roof\", \"Advanced safety features\",  \"Over-the-air software updates\", \"ABS brakes\"\n",
    "\n",
    "    }\n",
    "}\n",
    "```\"\"\"\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": system_message}],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [{\"text\": assistant_message}],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": user_message}],\n",
    "    },\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Send the message to the model, using a basic inference configuration.\n",
    "    #  https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/converse.html\n",
    "    response = client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=conversation,\n",
    "        inferenceConfig={\"maxTokens\": 4096, \"stopSequences\": [\"User:\"], \"temperature\": 0, \"topP\": 1},\n",
    "        additionalModelRequestFields={}\n",
    "    )\n",
    "\n",
    "    # Extract and print the response text.\n",
    "    response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    print(response_text)\n",
    "\n",
    "except (ClientError, Exception) as e:\n",
    "    print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "    exit(1)\n"
   ],
   "id": "29cff66c6426ddac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prompt Caching\n",
    "\n",
    "Prompt caching is a powerful feature in Amazon Bedrock that significantly reduces response latency for workloads with repetitive contexts.\n",
    "\n",
    "### What is Prompt Caching?\n",
    "\n",
    "Prompt caching allows you to store portions of your conversation context, enabling models to:\n",
    "- Reuse cached context instead of reprocessing inputs\n",
    "- Reduce response Time-To-First-Token (TTFT) for subsequent queries\n",
    "\n",
    "### When to Use Prompt Caching\n",
    "\n",
    "Prompt caching delivers maximum benefits for:\n",
    "- **Chat with Document**: By caching the document as input context on the first request, each user query becomes more efficient, perhaps enabling simpler architectures that avoid heavier solutions like vector databases.\n",
    "- **Coding assistants**: Reusing long code files in prompts enables near real-time inline suggestions, eliminating much of the time spent reprocessing code files.\n",
    "- **Agentic workflows**: Longer system prompts can be used to refine agent behavior without degrading the end-user experience. By caching the system prompts and complex tool definitions, the time to process each step in the agentic flow can be reduced.\n",
    "- **Few-Shot Learning**: Including numerous high-quality examples and complex instructions, such as for customer service or technical troubleshooting, can benefit from prompt caching.\n",
    "\n",
    "### Benefits of Prompt Caching\n",
    "\n",
    "- **Faster Response Times**: Avoid reprocessing the same context repeatedly\n",
    "- **Improved User Experience**: Reduced TTFT to create more natural conversations\n",
    "- **Cost Efficiency**: Potentially lower token usage by avoiding redundant processing\n",
    "\n",
    "### Model Support\n",
    "Be advised that the prompt caching feature is model-specific. You should review the [supported models](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-caching.html#prompt-caching-models) and details on the minimum number of tokens per cache checkpoint and maximum number of cache checkpoints per request\n",
    "\n",
    "### How it Works\n",
    "\n",
    "Prompt caching works by storing the context of the conversation in a cache. When a new request is made, the model checks if the context is already in the cache. If it is, the model uses the cached context instead of reprocessing the input.\n",
    "\n",
    "As we saw earlier, a prompt is an array of messages. A message can be marked as a cache checkpoint. Once a message is marked as a cache checkpoint,\n",
    "The **entire section of the prompt preceding the checkpoint then becomes the cached prompt prefix**.\n",
    "\n",
    "Now when the model processes a new request, that includes the all the prompt, it will not process the messages that are part of the cached prompt prefix, it will only need to process the messages that follow the checkpoint.\n",
    "\n",
    "\n",
    "![Prompt Caching](../resources/images/prompt-caching-aws-bedrock.png)\n",
    "\n",
    "The following diagram illustrates how cache hits work. A, B, C, D represent distinct portions of the prompt. A, B and C are marked as the prompt prefix. Cache hits occur when subsequent requests contain the same A, B, C prompt prefix.\n",
    "\n",
    "![Prompt Caching](../resources/images/cache-prompt-prefix.png)\n",
    "### References\n",
    "* [Amazon Bedrock: Prompt Caching](https://aws.amazon.com/blogs/machine-learning/effectively-use-prompt-caching-on-amazon-bedrock/)\n",
    "\n"
   ],
   "id": "89e2e84312e10f7e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Important:</b> Execute the following cell to make sure you have the latest version of the SDK (boto3 min version: boto3-1.37.24.)\n",
    "</div>"
   ],
   "id": "11f02c665c7f8b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Before we start lets make sure we have the latest version of the SDK (boto3 min version: boto3-1.37.24.)\n",
    "import boto3\n",
    "boto3_version = boto3.__version__\n",
    "print(f\"boto3 version: {boto3_version}\")\n",
    "if boto3_version < \"1.37.24\":\n",
    "    print(\"Updating your boto3 version to latest version.\")\n",
    "    !pip install --upgrade boto3\n",
    "    print(\"boto3 was updated to the latest version.\")\n",
    "    print(\"Please restart the kernel and re-run the notebook.\")\n",
    "    exit(1)\n",
    "else:\n",
    "    print(\"boto3 version is up to date.\")\n"
   ],
   "id": "c3c6a476592e6447",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Prompt Caching Example\n",
    "In the following example we will create a **Terms of Use Analyzer**  that can answer questions about the terms of use of a specific website. </br>\n",
    "Since the terms of use are long, we will use the prompt caching feature to cache the terms of use and only process the user question. </br>\n"
   ],
   "id": "ce241a518e0238e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T17:19:05.666022Z",
     "start_time": "2025-05-14T17:19:05.651939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "def chat_with_document(document, user_query, model_id):\n",
    "\n",
    "    instructions_terms_of_use_analyzer = \"\"\"\n",
    "    I will provide you with the Terms of Use document of a specific website, followed by a question about its content. Your task is to analyze the Terms of Use, extract relevant information, and provide a comprehensive answer to the question. Please follow these detailed instructions:\n",
    "\n",
    "    1. Identifying Relevant Clauses (Quotes):\n",
    "       - Carefully read through the entire Terms of Use document.\n",
    "       - Identify sections or clauses of the text that are directly relevant to answering the question.\n",
    "       - Pay special attention to definitions, user obligations, disclaimers of warranties, limitations of liability, governing law, dispute resolution, and any clauses related to the specific query.\n",
    "       - Select quotes that provide key information, context, or support for the answer. These are typically specific clauses or parts of clauses.\n",
    "       - Quotes should be concise and to the point, ideally no more than 2-4 sentences each, capturing the core of the relevant legal statement.\n",
    "       - Choose a diverse range of quotes if multiple clauses address different aspects of the question.\n",
    "       - Aim to select between 2 to 5 quotes, depending on the complexity of the question and the structure of the Terms of Use.\n",
    "\n",
    "    2. Presenting the Relevant Clauses:\n",
    "       - List the selected quotes under the heading 'Relevant clauses:'\n",
    "       - Number each quote sequentially, starting from [1].\n",
    "       - Present each quote exactly as it appears in the original text, enclosed in quotation marks.\n",
    "       - If no relevant clauses can be found to directly answer the question, write 'No directly relevant clauses found' instead.\n",
    "       - Example format:\n",
    "         Relevant clauses:\n",
    "         [1] \"Users agree not to use the service for any illegal or unauthorized purpose.\"\n",
    "         [2] \"The Company reserves the right to terminate your access to the Service at any time, without notice, for any reason whatsoever.\"\n",
    "\n",
    "    3. Formulating the Answer:\n",
    "       - Begin your answer with the heading 'Answer:' on a new line after the quotes.\n",
    "       - Provide a clear, concise, and accurate answer to the question based on the information in the Terms of Use.\n",
    "       - Ensure your answer is comprehensive and addresses all aspects of the question.\n",
    "       - Use information from the quoted clauses to support your answer, but rephrase and explain the implications rather than repeating them verbatim.\n",
    "       - Where possible, explain any legal jargon or complex phrasing in simpler terms, while remaining faithful to the original meaning of the clause.\n",
    "       - Maintain a logical flow and structure in your response.\n",
    "\n",
    "    4. Referencing Clauses in the Answer:\n",
    "       - Do not explicitly mention or introduce quotes in your answer (e.g., avoid phrases like 'According to clause [1]').\n",
    "       - Instead, add the bracketed number of the relevant quote at the end of each sentence or point that uses information from that clause.\n",
    "       - If a sentence or point is supported by multiple clauses, include all relevant quote numbers.\n",
    "       - Example: 'The website prohibits users from engaging in unlawful activities. [1] Furthermore, the platform can suspend user accounts without prior notification if terms are violated. [2]'\n",
    "\n",
    "    5. Handling Ambiguity or Lack of Specific Information:\n",
    "       - If the Terms of Use do not contain enough specific information to fully answer the question, or if a clause is ambiguous, clearly state this in your answer.\n",
    "       - Provide any partial information that is available, and explain what aspects are not explicitly covered or remain unclear.\n",
    "       - If there are multiple possible interpretations of a clause relevant to the question, explain this and provide answers based on plausible interpretations if possible, noting the ambiguity.\n",
    "       - State clearly that the analysis is based *only* on the provided text and cannot infer unstated terms.\n",
    "\n",
    "    6. Maintaining Objectivity and Disclaimer:\n",
    "       - Stick to the facts and statements presented in the Terms of Use document. Do not include personal opinions, interpretations beyond the text, or external information not found in the document.\n",
    "       - Your analysis is for informational purposes only and should NOT be considered legal advice. Always recommend consulting with a legal professional for specific advice regarding Terms of Use.\n",
    "       - If the document presents one-sided or particularly restrictive terms, you can note this objectively in your answer without endorsing or refuting the legal validity or fairness of such terms.\n",
    "\n",
    "    7. Formatting and Style:\n",
    "       - Use clear paragraph breaks to separate different points or aspects of your answer.\n",
    "       - Employ bullet points or numbered lists if it helps to organize information about specific rights, obligations, or restrictions more clearly.\n",
    "       - Ensure proper grammar, punctuation, and spelling throughout your response.\n",
    "       - Maintain a professional, neutral, and informative tone throughout your answer.\n",
    "\n",
    "    8. Length and Depth:\n",
    "       - Provide an answer that is sufficiently detailed to address the question comprehensively based on the Terms of Use.\n",
    "       - However, avoid unnecessary verbosity. Aim for clarity and conciseness, focusing on the aspects most relevant to the user's query.\n",
    "       - The length of your answer should be proportional to the complexity of the question and the amount of relevant information within the Terms of Use.\n",
    "\n",
    "    9. Dealing with Complex or Multi-part Questions about Terms:\n",
    "       - For questions with multiple parts (e.g., 'What are the user's rights regarding data privacy and what is the process for account termination?'), address each part separately and clearly.\n",
    "       - Use subheadings or numbered points to break down your answer if necessary, ensuring each component of the query is addressed.\n",
    "\n",
    "    10. Concluding the Answer:\n",
    "        - If appropriate, provide a brief summary of the key findings from the Terms of Use related to the question.\n",
    "        - Reiterate that the information is based solely on the provided document and is not legal advice. If the question implies seeking guidance (e.g., 'Should I be concerned about X clause?'), frame the answer by explaining what the clause means according to the text, rather than advising on concern.\n",
    "\n",
    "    Remember, your goal is to provide a clear, accurate, and well-supported analysis based solely on the content of the given Terms of Use document. Adhere to these instructions carefully to ensure a high-quality response that effectively addresses the user's query about the website's terms.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    document_content =  f\"Here is the document:  <document> {document} </document>\"\n",
    "\n",
    "    messages_body = [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': [\n",
    "                {\n",
    "                    'text': instructions_terms_of_use_analyzer\n",
    "                },\n",
    "                {\n",
    "                    'text': document_content\n",
    "                },\n",
    "                {\n",
    "                    \"cachePoint\": {\n",
    "                        \"type\": \"default\"\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    'text': user_query\n",
    "                },\n",
    "            ]\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    inference_config={\n",
    "        'maxTokens': 10000,\n",
    "        'temperature': 0,\n",
    "        'topP': 1\n",
    "    }\n",
    "    client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "\n",
    "    response = client.converse(\n",
    "        messages=messages_body,\n",
    "        modelId=model_id,\n",
    "        inferenceConfig=inference_config\n",
    "    )\n",
    "\n",
    "    output_message = response[\"output\"][\"message\"]\n",
    "    response_text = output_message[\"content\"][0][\"text\"]\n",
    "\n",
    "    print(\"Response text:\")\n",
    "    print(response_text)\n",
    "\n",
    "    print(\"Usage:\")\n",
    "    print(json.dumps(response[\"usage\"], indent=2))\n"
   ],
   "id": "f8bc4b8700b7e145",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now lets chat with the document and pay attention to the:\n",
    "* `cacheWriteInputTokens` - the number of tokens used to write the cache\n",
    "* `cacheReadInputTokens` - the number of tokens used to read the cache\n",
    "\n",
    "In the first request, the model will process the entire document and write it to the cache. </br>\n",
    "In the second request, the model will only process the user question and read the rest from the cache. </br>"
   ],
   "id": "4c6415319f8123f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T17:22:55.270078Z",
     "start_time": "2025-05-14T17:22:55.220542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "terms_of_use = requests.get('https://www.pexels.com/terms-of-service/').text\n",
    "model_id=\"amazon.nova-lite-v1:0\"\n",
    "\n",
    "\n",
    "questions = [\n",
    "    'Is my information is used by 3rd parties?',\n",
    "    'Is the service is GDPR compliant?',\n",
    "]\n"
   ],
   "id": "f011f09190fdea79",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T17:24:29.035783Z",
     "start_time": "2025-05-14T17:24:27.295361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chat_with_document(\n",
    "    document=terms_of_use,\n",
    "    user_query=questions[0],\n",
    "    model_id=model_id\n",
    ")"
   ],
   "id": "e1034cf621c4cfaa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response text:\n",
      "Relevant clauses:\n",
      "[1] \"No, your information is not shared with third parties except as described in this document.\"\n",
      "\n",
      "Answer:\n",
      "Based on the provided Terms of Use document, your information is not used by third parties unless explicitly stated in the document. [1] The document clearly mentions that user information is not shared with third parties except as described within the terms themselves. If you have concerns about how your data might be used or shared, it would be best to review the specific sections of the Terms of Use that address data sharing and privacy practices. Remember, this analysis is based solely on the provided text and does not constitute legal advice. For specific concerns or detailed understanding, consulting with a legal professional is recommended.\n",
      "Usage:\n",
      "{\n",
      "  \"inputTokens\": 16,\n",
      "  \"outputTokens\": 144,\n",
      "  \"totalTokens\": 5753,\n",
      "  \"cacheReadInputTokens\": 0,\n",
      "  \"cacheWriteInputTokens\": 5593\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T17:25:03.353435Z",
     "start_time": "2025-05-14T17:25:01.065668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chat_with_document(\n",
    "    document=terms_of_use,\n",
    "    user_query=questions[1],\n",
    "    model_id=model_id\n",
    ")"
   ],
   "id": "5870c4d0cc1d8a71",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response text:\n",
      "Relevant clauses:\n",
      "[1] \"We collect and process personal data in accordance with applicable data protection laws, including the General Data Protection Regulation (GDPR).\"\n",
      "[2] \"Users have the right to access, rectify, and delete their personal data, as well as the right to data portability and to object to processing.\"\n",
      "[3] \"We implement appropriate technical and organizational measures to protect personal data against unauthorized or unlawful processing and against accidental loss, destruction, or damage.\"\n",
      "\n",
      "Answer:\n",
      "Based on the provided Terms of Use, the service claims to collect and process personal data in compliance with applicable data protection laws, including the GDPR. [1] Users are informed that they have specific rights regarding their personal data, such as the right to access, rectify, and delete their data, as well as the right to data portability and to object to processing. [2] Additionally, the service states that it implements appropriate technical and organizational measures to protect personal data against unauthorized or unlawful processing and against accidental loss, destruction, or damage. [3] \n",
      "\n",
      "However, it is important to note that the analysis is based solely on the provided text and cannot confirm the actual compliance with GDPR or the effectiveness of the measures implemented. For specific advice regarding GDPR compliance, it is recommended to consult with a legal professional.\n",
      "Usage:\n",
      "{\n",
      "  \"inputTokens\": 11,\n",
      "  \"outputTokens\": 261,\n",
      "  \"totalTokens\": 5866,\n",
      "  \"cacheReadInputTokens\": 5594,\n",
      "  \"cacheWriteInputTokens\": 0\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
