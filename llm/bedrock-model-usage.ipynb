{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# AWS Bedrock\n",
    "\n",
    "\n",
    "AWS Bedrock is a fully managed service that provides access to foundation models (FMs) from leading AI companies. It allows you to build and scale generative AI applications using these models. In this notebook, we will explore how to use AWS Bedrock with the `boto3` library in Python."
   ],
   "id": "f12e12faf41d4e41"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## LLMs and Foundation Models\n",
    "Foundation models are large-scale machine learning models that are trained on vast amounts of data and can be fine-tuned for specific tasks. AWS Bedrock provides access to various foundation models, including text generation, image generation, and more. </br>\n",
    "\n",
    "* LLM is a stateless function. When \"memory\" or \"context\" is required, it is passed as part of the prompt.\n",
    "* The operation of fetching information from a database or other source is called \"retrieval augmentation generation\" (RAG).\n",
    "* The datasource used to fetch information is called a knowledge base.\n",
    "* LLMs [pricing](https://aws.amazon.com/bedrock/pricing/) depends on the model complexity and the number of tokens (~4 chars â‰… word) in the prompt and the response.\n",
    "* The longer the prompt, the more expensive and the slower it is.\n",
    "\n",
    "### Using AWS Bedrock models and regions\n",
    "\n",
    "AWS LLM models availability is region specific. `us-east-1` or `N. Virginia` is the region where all models are available. </br>\n",
    "In order to use a model of a specific provider, you need to request access to that model in the [Model Access](https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/modelaccess) section </br>\n",
    "In this tutorial we'll be using `us-east-1` region with several different models.\n",
    "\n",
    "### Cross-region inference\n",
    "Cross-region inference is a new AWS feature that enable LLM requests  to be processed in a different geographical region than where the request originated.\n",
    "Instead of being limited to the models and compute resources available in a single region, cross-region inference can automatically route your inference requests to other available regions.\n",
    "You can use the `Inference Profile ID` instead of the `Model ID` to specify the model you want to use. The Inference Profile ID is a unique identifier for a specific model and its associated compute resources in a specific region. </br>\n",
    "For list of available models and their Inference Profile IDs, please refer to the [AWS Cross-region inference Console](https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/inference-profiles).\n",
    "\n"
   ],
   "id": "b3154df63ade026c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conversation API\n",
    "The `conversation` API is used to interact with the LLMs in AWS Bedrock. It allows you to send messages to the model and receive responses. The API supports different roles for the messages, including `user`, `assistant`, and `system`. </br>\n",
    "\n",
    "The `user` role is used for the user's input, the `assistant` role is used for the model's response, and the `system` role is used for system messages that provide context or instructions to the model. </br>\n",
    "\n",
    "### Important Parameters\n",
    "* `modelId`: The ID of the model you want to use. This can be either the model ID or the Inference Profile ID.\n",
    "* `messages`: A list of messages to send to the model. Each message should include a `role` and `content`.\n",
    "* `inferenceConfig`: A dictionary of inference configuration options, such as `maxTokens`, `stopSequences`, `temperature`, and `topP`.\n",
    "*  `temperature`: Controls the randomness of the model's output. A higher temperature (e.g., 0.8) makes the output more random, while a lower temperature (e.g., 0.2) makes it more deterministic.\n",
    "* `topP`: It sets a threshold probability and selects the top tokens whose cumulative probability exceeds the threshold. The model then randomly samples from this set of tokens to generate output. This method can produce more diverse and interesting output than traditional methods that randomly sample the entire vocabulary."
   ],
   "id": "3da7153d3fdfb41d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T09:59:29.556568Z",
     "start_time": "2025-05-15T09:59:25.416366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import boto3\n",
    "from enum import Enum\n",
    "\n",
    "#  Get the list of available models in the region\n",
    "boto3_bedrock = boto3.client('bedrock')\n",
    "\n",
    "class LLMModel(Enum):\n",
    "    \"\"\"Enum for Bedrock models.\"\"\"\n",
    "    # Anthropic\n",
    "    CLAUDE_3_5_V1 = 'anthropic.claude-3-5-sonnet-20240620-v1:0'\n",
    "    CLAUDE_3_5_v2 = 'us.anthropic.claude-3-5-sonnet-20241022-v2:0' # Inference Profile ID\n",
    "    # Amazon\n",
    "    NOVA_LITE = 'amazon.nova-lite-v1:0'\n",
    "    NOVA_PRO = 'amazon.nova-pro-v1:0'\n",
    "    TITAN_LITE = 'amazon.titan-text-lite-v1'\n",
    "    TITAN_EXPRESS = 'amazon.titan-text-express-v1'\n",
    "    META_LLMA3_1B = 'meta.llama3-2-1b-instruct-v1:0'\n",
    "    META_LLMA3_3B = 'meta.llama3-2-3b-instruct-v1:0'\n",
    "\n",
    "[model['modelId'] for model in boto3_bedrock.list_foundation_models()['modelSummaries']]\n"
   ],
   "id": "9c078eca04c9733f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amazon.titan-text-lite-v1:0:4k',\n",
       " 'amazon.titan-text-lite-v1',\n",
       " 'amazon.titan-text-express-v1:0:8k',\n",
       " 'amazon.titan-text-express-v1',\n",
       " 'amazon.titan-embed-image-v1:0',\n",
       " 'amazon.titan-embed-image-v1',\n",
       " 'amazon.titan-embed-text-v2:0',\n",
       " 'amazon.titan-image-generator-v1',\n",
       " 'amazon.nova-pro-v1:0',\n",
       " 'amazon.nova-lite-v1:0',\n",
       " 'amazon.nova-micro-v1:0',\n",
       " 'amazon.nova-canvas-v1:0',\n",
       " 'amazon.nova-reel-v1:0',\n",
       " 'anthropic.claude-3-sonnet-20240229-v1:0:28k',\n",
       " 'anthropic.claude-3-sonnet-20240229-v1:0:200k',\n",
       " 'anthropic.claude-3-sonnet-20240229-v1:0',\n",
       " 'anthropic.claude-3-haiku-20240307-v1:0:48k',\n",
       " 'anthropic.claude-3-haiku-20240307-v1:0',\n",
       " 'anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
       " 'anthropic.claude-3-7-sonnet-20250219-v1:0',\n",
       " 'cohere.embed-english-v3',\n",
       " 'cohere.embed-multilingual-v3',\n",
       " 'mistral.mistral-7b-instruct-v0:2',\n",
       " 'mistral.mixtral-8x7b-instruct-v0:1',\n",
       " 'mistral.mistral-large-2402-v1:0',\n",
       " 'mistral.pixtral-large-2502-v1:0',\n",
       " 'meta.llama3-2-1b-instruct-v1:0',\n",
       " 'meta.llama3-2-3b-instruct-v1:0']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Zero Shot\n",
    "The following example is called `zero-shot` prompting. </br>\n",
    "There are no examples or context provided to the model. The model is expected to understand the task and generate a response based on its own training. </br>\n",
    "\n",
    "\n",
    "> change the models in order to switch between different LLMs. </br>"
   ],
   "id": "e5b83cdab347501d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T10:31:15.210999Z",
     "start_time": "2025-05-15T10:31:12.089829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Create a Bedrock Runtime client in the AWS Region you want to use.\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "\n",
    "\n",
    "# Start a conversation with the user message.\n",
    "user_message = \"\"\"Meeting transcript:\n",
    "Miguel: Hi Brant, I want to discuss the workstream  for our new product launch\n",
    "Brant: Sure Miguel, is there anything in particular you want to discuss?\n",
    "Miguel: Yes, I want to talk about how users enter into the product.\n",
    "Brant: Ok, in that case let me add in Namita.\n",
    "Namita: Hey everyone\n",
    "Brant: Hi Namita, Miguel wants to discuss how users enter into the product.\n",
    "Miguel: its too complicated and we should remove friction. for example, why do I need to fill out additional forms?  I also find it difficult to find where to access the product when I first land on the landing page.\n",
    "Brant: I would also add that I think there are too many steps.\n",
    "Namita: Ok, I can work on the landing page to make the product more discoverable but brant can you work on the additional forms?\n",
    "Brant: Yes but I would need to work with James from another team as he needs to unblock the sign up workflow.  Miguel can you document any other concerns so that I can discuss with James only once?\n",
    "Miguel: Sure.\n",
    "\n",
    "From the meeting transcript above, Create a list of action items for each person.\n",
    "\"\"\"\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": user_message}],\n",
    "    }\n",
    "]\n",
    "\n",
    "model_id = LLMModel.NOVA_PRO.value # Inference Profile ID\n",
    "try:\n",
    "    # Send the message to the model, using a basic inference configuration.\n",
    "    #  https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/converse.html\n",
    "    response = client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=conversation,\n",
    "        inferenceConfig={\"maxTokens\": 4096, \"stopSequences\": [\"User:\"], \"temperature\": 0, \"topP\": 1},\n",
    "        additionalModelRequestFields={}\n",
    "    )\n",
    "\n",
    "    # Extract and print the response text.\n",
    "    response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    print(response_text)\n",
    "\n",
    "except (ClientError, Exception) as e:\n",
    "    print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "    exit(1)\n"
   ],
   "id": "8b88007436436904",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here's a list of action items for each person based on the meeting transcript:\n",
      "\n",
      "### Miguel's Action Items:\n",
      "1. **Document Concerns:**\n",
      "   - Prepare a detailed document outlining any additional concerns or friction points related to user entry into the product.\n",
      "   - Ensure the document is comprehensive to facilitate a single discussion with James.\n",
      "\n",
      "### Brant's Action Items:\n",
      "1. **Coordinate with James:**\n",
      "   - Reach out to James from another team to unblock the sign-up workflow.\n",
      "   - Discuss the removal of additional forms with James using the documented concerns provided by Miguel.\n",
      "\n",
      "### Namita's Action Items:\n",
      "1. **Improve Landing Page:**\n",
      "   - Work on making the product more discoverable on the landing page.\n",
      "   - Ensure that users can easily find where to access the product upon landing.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Understanding the response\n",
    "\n",
    "The LLM response is a JSON object that contains important information in addition to the generated text. </br>\n",
    "When analyzing and comparing LLM responses, look for the following fields:\n",
    "\n",
    "* `latency`: The time taken to process the request and generate a response.\n",
    "* `usage`: The number of tokens used in the prompt and the response. This is important for cost estimation.\n",
    "* `RequestId`: A unique identifier for the request. This can be useful for debugging and tracking purposes.\n"
   ],
   "id": "87fc54fae6e8428f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T10:32:29.452692Z",
     "start_time": "2025-05-15T10:32:29.449308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "    # print the response with json format and indentation\n",
    "    import json\n",
    "\n",
    "    print(json.dumps(response, indent=4, sort_keys=True))\n"
   ],
   "id": "2a5dd1faadc63f75",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"ResponseMetadata\": {\n",
      "        \"HTTPHeaders\": {\n",
      "            \"connection\": \"keep-alive\",\n",
      "            \"content-length\": \"1004\",\n",
      "            \"content-type\": \"application/json\",\n",
      "            \"date\": \"Thu, 15 May 2025 10:31:15 GMT\",\n",
      "            \"x-amzn-requestid\": \"1a81d4f8-04d6-4ee3-aaee-89d9d17af2dc\"\n",
      "        },\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"RequestId\": \"1a81d4f8-04d6-4ee3-aaee-89d9d17af2dc\",\n",
      "        \"RetryAttempts\": 0\n",
      "    },\n",
      "    \"metrics\": {\n",
      "        \"latencyMs\": 1211\n",
      "    },\n",
      "    \"output\": {\n",
      "        \"message\": {\n",
      "            \"content\": [\n",
      "                {\n",
      "                    \"text\": \"Certainly! Here's a list of action items for each person based on the meeting transcript:\\n\\n### Miguel's Action Items:\\n1. **Document Concerns:**\\n   - Prepare a detailed document outlining any additional concerns or friction points related to user entry into the product.\\n   - Ensure the document is comprehensive to facilitate a single discussion with James.\\n\\n### Brant's Action Items:\\n1. **Coordinate with James:**\\n   - Reach out to James from another team to unblock the sign-up workflow.\\n   - Discuss the removal of additional forms with James using the documented concerns provided by Miguel.\\n\\n### Namita's Action Items:\\n1. **Improve Landing Page:**\\n   - Work on making the product more discoverable on the landing page.\\n   - Ensure that users can easily find where to access the product upon landing.\"\n",
      "                }\n",
      "            ],\n",
      "            \"role\": \"assistant\"\n",
      "        }\n",
      "    },\n",
      "    \"stopReason\": \"end_turn\",\n",
      "    \"usage\": {\n",
      "        \"inputTokens\": 249,\n",
      "        \"outputTokens\": 167,\n",
      "        \"totalTokens\": 416\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## One Shot\n",
    "One-shot prompting is a technique used to provide a single example to the model, helping it understand the task better. </br>\n",
    "\n",
    "In this example, we will use one-shot prompting to create a meeting summary and action items. </br>\n",
    "We will use the `assistant` role to provide the model with a system message that describes its role and the task it needs to perform. </br>\n",
    "In addition, we will provide a user message that contains the meeting transcript. </br>\n"
   ],
   "id": "951d9c212f656c79"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T10:45:04.820390Z",
     "start_time": "2025-05-15T10:45:02.182227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "# Start a conversation with the user message.\n",
    "system_message = \"\"\"You are a meeting assistant that helps to summarize the meeting and create action items for each person.\n",
    "You are given a meeting transcript and you need to create a list of action items for each person in the meeting.\n",
    "The action items should be in the following format:\n",
    "\n",
    "=== Miguel ===\n",
    "    - action item 1\n",
    "    - action item 2\n",
    "=== Brant ===\n",
    "    - action item1\n",
    "    - action item 2\n",
    "=== Namita ===\n",
    "    - action item 1\n",
    "    - action item 2\n",
    "\n",
    "The action items should be clear and concise.\n",
    "The action items should be based on the meeting transcript and should not include any additional information.\n",
    "In the end of the response, mention the meeting participants and their roles in a JSON format.\n",
    "In addition rank their involvement in the meeting from 1 to 5, where 5 is the most involved and 1 is the least involved.\n",
    "{\n",
    "    {\"name\": \"Miguel\", \"role\": \"Product Manager\", \"involvement\": 5},\n",
    "    {\"name\": \"Brant\", \"role\": \"Software Engineer\", \"involvement\": 4},\n",
    "    {\"name\": \"Namita\", \"role\": \"UX Designer\", \"involvement\": 3}\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "user_message = \"\"\"Meeting transcript:\n",
    "Miguel: Hi Brant, I want to discuss the workstream  for our new product launch\n",
    "Brant: Sure Miguel, is there anything in particular you want to discuss?\n",
    "Miguel: Yes, I want to talk about how users enter into the product.\n",
    "Brant: Ok, in that case let me add in Namita.\n",
    "Namita: Hey everyone\n",
    "Brant: Hi Namita, Miguel wants to discuss how users enter into the product.\n",
    "Miguel: its too complicated and we should remove friction. for example, why do I need to fill out additional forms?  I also find it difficult to find where to access the product when I first land on the landing page.\n",
    "Brant: I would also add that I think there are too many steps.\n",
    "Namita: Ok, I can work on the landing page to make the product more discoverable but brant can you work on the additional forms?\n",
    "Brant: Yes but I would need to work with James from another team as he needs to unblock the sign up workflow.  Miguel can you document any other concerns so that I can discuss with James only once?\n",
    "Miguel: Sure.\n",
    "\"\"\"\n",
    "\n",
    "user_message2 = \"\"\"Meeting transcript:\n",
    "Attendees: Shimon (PM), Igor (CTO), Avi (Tech Lead), Barkoni(Alien)\n",
    "\n",
    "Shimon: Right, let's discuss integrating code coverage into the main pipeline. What are the main benefits and drawbacks?\n",
    "\n",
    "Igor: The major pro is improved code quality and maintainability. It gives us objective data on test effectiveness, reducing future bugs and technical debt. Itâ€™s a standard best practice.\n",
    "\n",
    "Avi: Agreed, Igor. The con is potential friction â€“ longer build times initially, and developers needing to potentially refactor or add more tests, impacting velocity slightly. We need to manage thresholds carefully.\n",
    "\n",
    "Shimon: So, increased confidence in quality versus a possible short-term slowdown?\n",
    "\n",
    "Igor: Exactly. A worthwhile investment for long-term stability.\n",
    "\n",
    "Avi: We can mitigate the impact by starting with warnings, not blockers.\n",
    "Barkoni: We stopped doing code coverage in our planet before we wrote the first line of code.\n",
    "Shimon: Barkoni, can you elaborate on that?\n",
    "Barkoni: What's the point? You will not understand.\n",
    "\"\"\"\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": 'you are a meeting assistant that summarize meetings'}],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [{\"text\": system_message}],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": user_message2}],\n",
    "    }\n",
    "]\n",
    "\n",
    "model_id = LLMModel.NOVA_LITE.value # Inference Profile ID\n",
    "\n",
    "try:\n",
    "    # Send the message to the model, using a basic inference configuration.\n",
    "    #  https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/converse.html\n",
    "    response = client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=conversation,\n",
    "        inferenceConfig={\"maxTokens\": 1000,  \"temperature\": 0, \"topP\": 0.9},\n",
    "        additionalModelRequestFields={}\n",
    "    )\n",
    "\n",
    "    # Extract and print the response text.\n",
    "    response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    print(response_text)\n",
    "\n",
    "except (ClientError, Exception) as e:\n",
    "    print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "    exit(1)\n"
   ],
   "id": "82b0312e27d994a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Shimon ===\n",
      "    - Discuss with the team the pros and cons of integrating code coverage into the main pipeline.\n",
      "    - Evaluate the potential impact on build times and development velocity.\n",
      "    - Consider starting with warnings instead of blockers to mitigate the impact.\n",
      "    - Research best practices for integrating code coverage into the pipeline.\n",
      "=== Igor ===\n",
      "    - Provide guidance on managing thresholds for code coverage.\n",
      "    - Evaluate the long-term benefits of improved code quality and maintainability.\n",
      "    - Consider the potential impact on build times and development velocity.\n",
      "    - Research tools and best practices for code coverage.\n",
      "=== Avi ===\n",
      "    - Evaluate the potential impact on build times and development velocity.\n",
      "    - Consider strategies for mitigating the impact on development velocity.\n",
      "    - Research best practices for code coverage and test effectiveness.\n",
      "    - Provide guidance on refactoring or adding more tests as needed.\n",
      "=== Barkoni ===\n",
      "    - Elaborate on the decision to stop doing code coverage in their planet.\n",
      "    - Provide insights on the potential benefits and drawbacks of code coverage.\n",
      "    - Research best practices for code coverage and test effectiveness.\n",
      "\n",
      "Meeting participants and their roles:\n",
      "\n",
      "{\n",
      "    {\"name\": \"Shimon\", \"role\": \"Product Manager\", \"involvement\": 5},\n",
      "    {\"name\": \"Igor\", \"role\": \"CTO\", \"involvement\": 4},\n",
      "    {\"name\": \"Avi\", \"role\": \"Tech Lead\", \"involvement\": 3},\n",
      "    {\"name\": \"Barkoni\", \"role\": \"Alien\", \"involvement\": 2}\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Few Shots\n",
    "Few-shot prompting is a technique used to provide multiple examples to the model, helping it understand the task better. </br>\n",
    "Each example is called a `shot` and it contains a prompt and a desired response. </br>\n",
    "The `shots` can be based on real-world examples or synthetic examples that exemplify the task. </br>\n"
   ],
   "id": "bf7235b12365041b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T10:55:02.150698Z",
     "start_time": "2025-05-15T10:54:56.965279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "model_id = LLMModel.NOVA_PRO.value\n",
    "\n",
    "# Start a conversation with the user message.\n",
    "\n",
    "prompt = \"\"\"You are a product manager for a website builder platform (like Wix). For each new feature request, you need to outline the functional requirements, highlight important UI/UX considerations, and explain the business advantages. Respond in the following format for each feature request:\n",
    "\n",
    "**Functional Requirement:**\n",
    "[Clearly describe what the feature should do.]\n",
    "\n",
    "**UI/UX Important Points:**\n",
    "[List key considerations for the user interface and user experience of this feature.]\n",
    "\n",
    "**Business Advantage:**\n",
    "[Explain how this feature benefits the website builder platform.]\n",
    "\n",
    "----\n",
    "Examples:\n",
    "\n",
    "**Feature Request:** Add a built-in image editor with basic cropping and resizing tools.\n",
    "\n",
    "**Functional Requirement:**\n",
    "Users should be able to crop and resize images directly within the website builder without needing to upload pre-edited files. Supported formats should include JPG, PNG, and GIF. The editor should offer standard aspect ratio presets and freeform resizing.\n",
    "\n",
    "**UI/UX Important Points:**\n",
    "- The image editor should be easily accessible within the image settings panel.\n",
    "- Controls for cropping and resizing should be intuitive and visually clear.\n",
    "- Users should see a real-time preview of their edits.\n",
    "- An option to revert to the original image should be available.\n",
    "\n",
    "**Business Advantage:**\n",
    "- Improves user convenience and efficiency by eliminating the need for external image editing tools.\n",
    "- Can attract users who need quick image adjustments without complex software.\n",
    "- May reduce support requests related to image sizing issues.\n",
    "\n",
    "---\n",
    "\n",
    "**Feature Request:** Implement a library of pre-designed website sections (e.g., headers, footers, contact forms).\n",
    "\n",
    "**Functional Requirement:**\n",
    "Users should be able to browse and insert professionally designed website sections into their pages with a single click. The library should include various categories and styles, and users should be able to customize the content and styling of these sections.\n",
    "\n",
    "**UI/UX Important Points:**\n",
    "- The section library should be well-organized and easy to navigate, possibly with categories and search functionality.\n",
    "- Previews of the sections should be clear and representative of the final design.\n",
    "- The insertion process should be seamless and not disrupt the user's workflow.\n",
    "- Users should have clear visual cues on how to customize the content of the inserted sections.\n",
    "\n",
    "**Business Advantage:**\n",
    "- Speeds up the website creation process for users, making it more appealing to beginners.\n",
    "- Provides users with professionally designed elements, potentially leading to more visually appealing websites.\n",
    "- Can encourage users to build more comprehensive websites by offering readily available components.\n",
    "\n",
    "---\n",
    "\n",
    "**Feature Request:** Allow users to embed social media feeds (e.g., Instagram, Twitter) directly onto their websites.\n",
    "\n",
    "**Functional Requirement:**\n",
    "Users should be able to connect their social media accounts and display their latest posts on their website. They should have options to customize the number of posts displayed and the layout of the feed.\n",
    "\n",
    "**UI/UX Important Points:**\n",
    "- The connection process to social media accounts should be secure and straightforward.\n",
    "- Embedding options should be easily accessible within the website editor.\n",
    "- Users should have control over the visual presentation of the feed to match their website's design.\n",
    "- The embedded feed should be responsive and display correctly on different devices.\n",
    "\n",
    "**Business Advantage:**\n",
    "- Enhances user engagement by allowing them to showcase their social media presence.\n",
    "- Can drive traffic between the user's website and their social media profiles.\n",
    "- Adds dynamic content to websites, making them more lively and up-to-date.\n",
    "\n",
    "---\n",
    "\n",
    "Here is the actual Feature Request: {0}\n",
    "\"\"\"\n",
    "\n",
    "feature_request = \"\"\"Integrate with a third-party payment processor (e.g., Stripe, PayPal) to enable e-commerce functionality.\"\"\"\n",
    "feature_request2 = \"\"\"I want a no-code feature for my website builder that allows users to create custom forms with drag-and-drop functionality. The forms should support various field types (text, checkbox, radio button, dropdown) and allow users to customize the layout and design. Additionally, users should be able to set up email notifications for form submissions and view submission data in a user-friendly dashboard.\"\"\"\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": prompt.format(feature_request)}],\n",
    "        # \"content\": [{\"text\": prompt.format(\"Implement A built-in image editor with basic cropping and resizing tools.\")}],\n",
    "    }\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Send the message to the model, using a basic inference configuration.\n",
    "    #  https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/converse.html\n",
    "    response = client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=conversation,\n",
    "        inferenceConfig={\"maxTokens\": 4096, \"stopSequences\": [\"User:\"], \"temperature\": 0, \"topP\": 1},\n",
    "        additionalModelRequestFields={}\n",
    "    )\n",
    "\n",
    "    # Extract and print the response text.\n",
    "    response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    print(response_text)\n",
    "\n",
    "except (ClientError, Exception) as e:\n",
    "    print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "    exit(1)\n"
   ],
   "id": "f163013747606b0f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Feature Request:** Integrate with a third-party payment processor (e.g., Stripe, PayPal) to enable e-commerce functionality.\n",
      "\n",
      "**Functional Requirement:**\n",
      "Users should be able to set up an online store within their website, including the ability to add products, manage inventory, and process payments through integrated third-party payment processors like Stripe or PayPal. The platform should support various payment methods, including credit/debit cards and digital wallets. Users should also have access to sales reports and the ability to manage orders directly within the website builder.\n",
      "\n",
      "**UI/UX Important Points:**\n",
      "- The integration setup process should be straightforward, with clear instructions and support for connecting to Stripe, PayPal, or other supported payment processors.\n",
      "- The product management interface should be intuitive, allowing users to easily add, edit, and manage products, including uploading images, setting prices, and managing inventory.\n",
      "- The checkout process should be seamless and secure, with a clear call-to-action and progress indicators.\n",
      "- Users should have the option to customize the appearance of their online store to match their websiteâ€™s design.\n",
      "- The order management system should provide a clear overview of sales, including order statuses, customer information, and the ability to fulfill or cancel orders.\n",
      "- Sales reports should be easy to access and understand, with options to filter and export data.\n",
      "\n",
      "**Business Advantage:**\n",
      "- Opens up new revenue streams for users by allowing them to sell products or services directly from their websites.\n",
      "- Attracts a broader audience, including small business owners and artisans, looking for an all-in-one solution to build and manage their online stores.\n",
      "- Enhances user retention by providing comprehensive e-commerce capabilities within the platform, reducing the need for users to seek out additional tools.\n",
      "- Can increase user satisfaction and loyalty by offering a complete set of features for building and running an online business.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tool Use / Function Calling\n",
    "LLMs can be enhanced with tools and APIs to provide additional functionality. </br>\n",
    "For example, you can use APIs to fetch data from external sources, perform calculations, or interact with other services. </br>\n",
    "In this example, we will use the `python` tool to perform calculations and the `weather` API to fetch weather data. </br>\n",
    "\n",
    "**Tool Use Workflow:**\n",
    "\n",
    "1.  **Define Tools:** Specify tools with names, descriptions, and argument schemas. Include a user prompt (e.g., \"What's the weather like in New York today?\").\n",
    "2.  **LLM Decides:** The LLM determines if a tool is necessary and halts text generation if so.\n",
    "3.  **JSON Call:** The LLM outputs a JSON object containing the selected tool and its parameter values.\n",
    "4.  **Execute & Return:** The system extracts parameters, runs the tool, and returns the output to the LLM.\n",
    "5.  **Generate Answer:** The LLM uses the tool output to create a final response.\n",
    "\n",
    "![Tool Calling](../resources/images/tool-call-schema-removebg.webp)\n",
    "### References\n",
    "\n",
    "* [AWS Bedrock: Converse API tool use examples](https://docs.aws.amazon.com/bedrock/latest/userguide/tool-use-examples.html)\n",
    "* [Guide to Tool Calling](https://www.analyticsvidhya.com/blog/2024/08/tool-calling-in-llms/)\n",
    "\n",
    "\n"
   ],
   "id": "86eaa370c9df2983"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T11:01:24.514386Z",
     "start_time": "2025-05-15T11:01:24.450167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "model_id = \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\" # Inference Profile ID\n",
    "\n",
    "class LocationNotFoundException(Exception):\n",
    "    \"\"\"Raised when a location is not found.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Weather API: Fetch weather data from weatherapi.com, api key from user: mistriela@yopmail.com\n",
    "def fetch_weather(city):\n",
    "    print('Fetching weather data for city:', city)\n",
    "    base_url = \"https://api.weatherapi.com/v1/current.json\"\n",
    "    params = {\n",
    "        \"q\": city,\n",
    "        \"key\": \"6e7b99ab0f454283ab9125132252104\",\n",
    "        \"aqi\": \"no\"  # Get temperature in Celsius\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()  # Raise an error for bad HTTP responses\n",
    "        weather_data = response.json()\n",
    "        return {\n",
    "            \"city\": weather_data[\"location\"][\"name\"],\n",
    "            \"temperature\": weather_data[\"current\"][\"temp_c\"],\n",
    "            \"description\": weather_data[\"current\"][\"condition\"][\"text\"]\n",
    "        }\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise LocationNotFoundException(f\"Error fetching weather data: {e}\")\n",
    "\n",
    "\n",
    "def invoke_bedrock_llm_with_function_calling(prompt: str, model: LLMModel):\n",
    "    \"\"\"\n",
    "    Invokes an LLM on AWS Bedrock with function calling to get weather information.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The user's query (e.g., \"What's the weather in London?\").\n",
    "        model_id (str): The ID of the Bedrock LLM model to use.\n",
    "        region_name (str): The AWS region.\n",
    "\n",
    "    Returns:\n",
    "        str: The LLM's response, which may include the weather information\n",
    "             or an error message.\n",
    "    \"\"\"\n",
    "\n",
    "    tool_config = {\n",
    "        \"tools\": [\n",
    "            {\n",
    "                \"toolSpec\": {\n",
    "                    \"name\": \"fetch_weather\",\n",
    "                    \"description\": \"fetch weather information for a given city\",\n",
    "                    \"inputSchema\": {\n",
    "                        \"json\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"city\": {\n",
    "                                    \"type\": \"string\",\n",
    "                                    \"description\": \"The name of the city for which to fetch weather information.\"\n",
    "                                }\n",
    "                            },\n",
    "                            \"required\": [\n",
    "                                \"city\"\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "    # Note: there is no usage of `assistant` role as part of the prompt. It's the LLM responsibility to understand that a tool is required.\n",
    "    input_messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": prompt}]\n",
    "    }]\n",
    "\n",
    "    # Send the initial message to the model. if there is a weather request, the model will stop and return a tool use request.\n",
    "    # Several tools requests can be sent in a single response.\n",
    "    response = client.converse(\n",
    "        modelId=model.value,\n",
    "        messages=input_messages,\n",
    "        toolConfig=tool_config\n",
    "    )\n",
    "    output_message = response['output']['message']\n",
    "    input_messages.append(output_message)\n",
    "    stop_reason = response['stopReason']\n",
    "\n",
    "    print(f\"LLM Message: {json.dumps(response, indent=4)}\")\n",
    "    # Print the LLM message and stop reason.\n",
    "    # See if any response starts with a json object root node: \"toolUse\"\n",
    "    if stop_reason == 'tool_use':\n",
    "        # Tool use requested. Call the tool and send the result to the model.\n",
    "        tool_requests = output_message['content']\n",
    "        for tool_request in tool_requests:\n",
    "            if 'toolUse' in tool_request:\n",
    "                tool = tool_request['toolUse']\n",
    "                print(f\"Requesting tool {tool['name']} Request: {tool['toolUseId']} \")\n",
    "\n",
    "                if tool['name'] == 'fetch_weather':\n",
    "                    tool_result = {}\n",
    "                    try:\n",
    "                        weather_data = fetch_weather(tool['input']['city'])\n",
    "                        tool_result = {\n",
    "                            \"toolUseId\": tool['toolUseId'],\n",
    "                            \"content\": [{\"json\": weather_data}]\n",
    "                        }\n",
    "                    except LocationNotFoundException as err:\n",
    "                        tool_result = {\n",
    "                            \"toolUseId\": tool['toolUseId'],\n",
    "                            \"content\": [{\"text\":  err.args[0]}],\n",
    "                            \"status\": 'error'\n",
    "                        }\n",
    "\n",
    "                    tool_result_message = {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [{\"toolResult\": tool_result}]\n",
    "                    }\n",
    "                    input_messages.append(tool_result_message)\n",
    "\n",
    "                    # Send the tool result to the model.\n",
    "                    response = client.converse(\n",
    "                        modelId=model.value,\n",
    "                        messages=input_messages,\n",
    "                        toolConfig=tool_config\n",
    "                    )\n",
    "                    output_message = response['output']['message']\n",
    "\n",
    "    # print the final response from the model.\n",
    "    # for content in output_message['content']:\n",
    "    #     print(json.dumps(content, indent=4))\n",
    "\n",
    "    return output_message\n",
    "\n"
   ],
   "id": "fc75e70526d083eb",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T11:17:32.702213Z",
     "start_time": "2025-05-15T11:17:06.543102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_query = \"What is the weather like in Yeruham?\"\n",
    "llm_response = invoke_bedrock_llm_with_function_calling(user_query, LLMModel.CLAUDE_3_5_v2)\n",
    "print(llm_response['content'][0]['text'])"
   ],
   "id": "3f737edaae3d4168",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Message: {\n",
      "    \"ResponseMetadata\": {\n",
      "        \"RequestId\": \"e6f37ed7-4dfd-4bef-a036-1ec24c8aba1e\",\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"HTTPHeaders\": {\n",
      "            \"date\": \"Thu, 15 May 2025 11:17:08 GMT\",\n",
      "            \"content-type\": \"application/json\",\n",
      "            \"content-length\": \"336\",\n",
      "            \"connection\": \"keep-alive\",\n",
      "            \"x-amzn-requestid\": \"e6f37ed7-4dfd-4bef-a036-1ec24c8aba1e\"\n",
      "        },\n",
      "        \"RetryAttempts\": 0\n",
      "    },\n",
      "    \"output\": {\n",
      "        \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": [\n",
      "                {\n",
      "                    \"text\": \"I'll help you check the weather in Yeruham.\"\n",
      "                },\n",
      "                {\n",
      "                    \"toolUse\": {\n",
      "                        \"toolUseId\": \"tooluse_iv-yKHvnRSKl_RmwehLFFQ\",\n",
      "                        \"name\": \"fetch_weather\",\n",
      "                        \"input\": {\n",
      "                            \"city\": \"Yeruham\"\n",
      "                        }\n",
      "                    }\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"stopReason\": \"tool_use\",\n",
      "    \"usage\": {\n",
      "        \"inputTokens\": 402,\n",
      "        \"outputTokens\": 70,\n",
      "        \"totalTokens\": 472\n",
      "    },\n",
      "    \"metrics\": {\n",
      "        \"latencyMs\": 2130\n",
      "    }\n",
      "}\n",
      "Requesting tool fetch_weather Request: tooluse_iv-yKHvnRSKl_RmwehLFFQ \n",
      "Fetching weather data for city: Yeruham\n",
      "Currently in Yeruham, it is sunny with a temperature of 27.2Â°C.\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tool Chaining\n",
    "Tool chaining is a technique used to combine multiple tools or functions in a sequence to achieve a more complex task. </br>\n",
    "In this example, we will use tool chaining **recursively** to fetch the weather data and then determine the dress code based on the temperature. </br>\n",
    "\n",
    "### Execution Flow\n",
    "We will ask the model to provide a dress code for the hotter city. </br>\n",
    "The model will need to understand that it needs to call the `fetch_weather` tool to get the weather data for **both** cities. </br>\n",
    "Then, it will call the `get_dress_code` tool to determine the dress code based on the temperature. </br>\n",
    "\n",
    "### Important Lookouts\n",
    "* Experiment with the different models, check if `lite` models can be used instead of `pro` models.\n",
    "* Check the input/output of the llm - see how the conversation is built gradually."
   ],
   "id": "38104e3d96629d9b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T11:27:52.403404Z",
     "start_time": "2025-05-15T11:27:18.223051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "\n",
    "class LocationNotFoundException(Exception):\n",
    "    \"\"\"Raised when a location is not found.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Weather API: Fetch weather data from weatherapi.com, api key from user: mistriela@yopmail.com\n",
    "def fetch_weather(city):\n",
    "    print('Fetching weather data for city:', city)\n",
    "    base_url = \"https://api.weatherapi.com/v1/current.json\"\n",
    "    params = {\n",
    "        \"q\": city,\n",
    "        \"key\": \"6e7b99ab0f454283ab9125132252104\",\n",
    "        \"aqi\": \"no\"  # Get temperature in Celsius\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()  # Raise an error for bad HTTP responses\n",
    "        weather_data = response.json()\n",
    "        return {\n",
    "            \"city\": weather_data[\"location\"][\"name\"],\n",
    "            \"temperature\": weather_data[\"current\"][\"temp_c\"],\n",
    "            \"description\": weather_data[\"current\"][\"condition\"][\"text\"]\n",
    "        }\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise LocationNotFoundException(f\"Error fetching weather data: {e}\")\n",
    "\n",
    "def get_dress_code(temperature: float) -> str:\n",
    "    \"\"\"\n",
    "    Determine the dress code based on the temperature.\n",
    "    Temperature ranges: -20 to 50 degrees Celsius.\n",
    "    Args:\n",
    "        temperature (float): The temperature in Celsius.\n",
    "\n",
    "    Returns:\n",
    "        str: The recommended dress code.\n",
    "    \"\"\"\n",
    "    print('Getting dress code for temperature:', temperature)\n",
    "    if temperature < -10:\n",
    "        return \"Wear a heavy winter coat, gloves, and a warm hat.\"\n",
    "    elif -10 <= temperature < 0:\n",
    "        return \"Wear a warm coat and gloves.\"\n",
    "    elif 0 <= temperature < 10:\n",
    "        return \"Wear a light jacket.\"\n",
    "    elif 10 <= temperature < 20:\n",
    "        return \"Wear a long-sleeve shirt.\"\n",
    "    elif 20 <= temperature < 30:\n",
    "        return \"Wear a short-sleeve shirt.\"\n",
    "    else:\n",
    "        return \"Wear summer clothes.\"\n",
    "\n",
    "tool_config = {\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"fetch_weather\",\n",
    "                \"description\": \"fetch weather information for a given city\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"city\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The name of the city for which to fetch weather information.\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"city\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"get_dress_code\",\n",
    "                \"description\": \"Get the dress code based on the temperature.\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"temperature\": {\n",
    "                                \"type\": \"number\",\n",
    "                                \"description\": \"The temperature in Celsius.\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"temperature\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "def invoke_bedrock_llm_with_multiple_function_calling(prompt: str, model: LLMModel, input_messages=None):\n",
    "    if input_messages is None:\n",
    "        input_messages = [{\"role\": \"user\", \"content\": [{\"text\": prompt}]}]\n",
    "\n",
    "    # print('input messages:', json.dumps(input_messages, indent=4))\n",
    "\n",
    "    response = client.converse(\n",
    "        modelId=model.value,\n",
    "        messages=input_messages,\n",
    "        toolConfig=tool_config\n",
    "    )\n",
    "    output_message = response['output']['message']\n",
    "    # print('output_message:', json.dumps(response['output'], indent=4))\n",
    "    input_messages.append(output_message)\n",
    "    stop_reason = response['stopReason']\n",
    "\n",
    "    if stop_reason == 'tool_use':\n",
    "        response_contents = output_message['content']\n",
    "        tool_responses = []\n",
    "        for response_content in response_contents:\n",
    "            if 'toolUse' in response_content: ## if there is a tool use request in the response content\n",
    "                tool_request = response_content['toolUse']\n",
    "                print(f\"Requesting tool '{tool_request['name']}' ID[{tool_request['toolUseId']}] Input: {tool_request['input']}\")\n",
    "\n",
    "                tool_result = {}\n",
    "                if tool_request['name'] == 'fetch_weather':\n",
    "                    try:\n",
    "                        tool_result = fetch_weather(tool_request['input']['city'])\n",
    "\n",
    "                    except LocationNotFoundException as err:\n",
    "                        tool_result = {\"error\": err.message}\n",
    "\n",
    "                elif tool_request['name'] == 'get_dress_code':\n",
    "                    tool_result = {\"text\": get_dress_code(tool_request['input']['temperature'])}\n",
    "\n",
    "\n",
    "                tool_result_response = {\n",
    "                    \"toolResult\": {\n",
    "                        \"toolUseId\": tool_request['toolUseId'],\n",
    "                        \"content\": [{\"json\": tool_result}]\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                tool_responses.append(tool_result_response)\n",
    "\n",
    "        tool_result_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": tool_responses\n",
    "        }\n",
    "\n",
    "        input_messages.append(tool_result_message)\n",
    "\n",
    "        # Recursively call the function to process the next step\n",
    "        return invoke_bedrock_llm_with_multiple_function_calling(prompt, model, input_messages)\n",
    "\n",
    "    # If no more tools are required, return the final response\n",
    "    return output_message\n",
    "\n",
    "\n",
    "user_query = \"\"\"I want to travel to a hot location.\n",
    "I'm thinking Tel-Aviv or Moscow. What is the hotter city? and what should I wear there depends on the current weather?\n",
    "Provide a concise textual answer.\n",
    "\"\"\"\n",
    "\n",
    "llm_response = invoke_bedrock_llm_with_multiple_function_calling(user_query, LLMModel.CLAUDE_3_5_v2)\n",
    "print(\"\\n\")\n",
    "print(llm_response['content'][0]['text'])\n"
   ],
   "id": "a55eb75aeb860b27",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting tool 'fetch_weather' ID[tooluse_zavJIH5ZTzeNsEsEjUP4jQ] Input: {'city': 'Tel Aviv'}\n",
      "Fetching weather data for city: Tel Aviv\n",
      "Requesting tool 'fetch_weather' ID[tooluse_ZIShhkR9SCCl9gQ5INUT4w] Input: {'city': 'Moscow'}\n",
      "Fetching weather data for city: Moscow\n",
      "Requesting tool 'get_dress_code' ID[tooluse_oUK-ia40QPWyI4nhf67Sbg] Input: {'temperature': 25.1}\n",
      "Getting dress code for temperature: 25.1\n",
      "\n",
      "\n",
      "Tel Aviv is currently hotter at 25.1Â°C, while Moscow is at 17Â°C. For Tel Aviv's weather, it's recommended to wear a short-sleeve shirt as it's sunny and warm.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Structured Output\n",
    "Structured output is a technique used to format the output of the LLM in a specific way. </br>\n",
    "This can be useful for various applications, such as generating JSON or XML responses. </br>\n",
    "It also helps to ensure that the output is consistent and compliant with the expected format. </br>"
   ],
   "id": "d5976750f0a1ee06"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T11:30:52.120378Z",
     "start_time": "2025-05-15T11:30:41.630634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Create a Bedrock Runtime client in the AWS Region you want to use.\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "##> Show the differences between\n",
    "# model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\" # Model ID\n",
    "# model_id = \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\" # Inference Profile ID\n",
    "# model_id = \"amazon.titan-text-express-v1\"\n",
    "model_id = LLMModel.CLAUDE_3_5_v2.value\n",
    "\n",
    "system_message = \"\"\"You are a car expert with ability to provide technical details about a specific car you are asked for.\n",
    "The response must be in a JSON format.\"\"\"\n",
    "\n",
    "# Start a conversation with the user message.\n",
    "# user_message = \"\"\"Hi i would like to know the technical details about the car Tesla Model Y.\"\"\"\n",
    "user_message = \"\"\"Hi i would like to know the technical details about the car Toyota Prius 2010.\"\"\"\n",
    "\n",
    "assistant_message = \"\"\"```json\n",
    "{\n",
    "    \"car\": {\n",
    "        \"name\": \"Car name\",\n",
    "        \"type\": \"Type of the car. E.g: Electric SUV\",\n",
    "        \"range\": \"The range of the car in miles\",\n",
    "        \"top_speed\": \"Top speed of the car in mph\",\n",
    "        \"acceleration\": \"0-60 mph time in seconds\",\n",
    "        \"battery_capacity\": \"For electric cars, mention the battery capacity in kWh\",\n",
    "        \"features\": Array of features of the car. E.g: \"Autopilot\",\"All-wheel drive\", \"Premium interior\", \"Panoramic glass roof\", \"Advanced safety features\",  \"Over-the-air software updates\", \"ABS brakes\"\n",
    "\n",
    "    }\n",
    "}\n",
    "```\"\"\"\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": system_message}],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [{\"text\": assistant_message}],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": user_message}],\n",
    "    },\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Send the message to the model, using a basic inference configuration.\n",
    "    #  https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/converse.html\n",
    "    response = client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=conversation,\n",
    "        inferenceConfig={\"maxTokens\": 4096, \"stopSequences\": [\"User:\"], \"temperature\": 0, \"topP\": 1},\n",
    "        additionalModelRequestFields={}\n",
    "    )\n",
    "\n",
    "    # Extract and print the response text.\n",
    "    response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    print(response_text)\n",
    "\n",
    "except (ClientError, Exception) as e:\n",
    "    print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "    exit(1)\n"
   ],
   "id": "29cff66c6426ddac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"car\": {\n",
      "        \"name\": \"Toyota Prius 2010\",\n",
      "        \"type\": \"Hybrid Hatchback\",\n",
      "        \"engine\": {\n",
      "            \"type\": \"1.8L 4-cylinder with Electric Motor\",\n",
      "            \"combined_power\": \"134 hp\",\n",
      "            \"gas_engine\": \"98 hp\",\n",
      "            \"electric_motor\": \"80 hp\"\n",
      "        },\n",
      "        \"fuel_economy\": {\n",
      "            \"city\": \"51 mpg\",\n",
      "            \"highway\": \"48 mpg\",\n",
      "            \"combined\": \"50 mpg\"\n",
      "        },\n",
      "        \"top_speed\": \"112 mph\",\n",
      "        \"acceleration\": \"9.8 seconds\",\n",
      "        \"battery\": {\n",
      "            \"type\": \"Nickel-Metal Hydride (NiMH)\",\n",
      "            \"capacity\": \"1.3 kWh\"\n",
      "        },\n",
      "        \"transmission\": \"Continuously Variable Transmission (CVT)\",\n",
      "        \"features\": [\n",
      "            \"Smart Key System\",\n",
      "            \"Push Button Start\",\n",
      "            \"Multi-Information Display\",\n",
      "            \"EV/ECO/Power driving modes\",\n",
      "            \"ABS with Electronic Brake-force Distribution\",\n",
      "            \"Vehicle Stability Control\",\n",
      "            \"Traction Control\",\n",
      "            \"Touch Tracer Display\",\n",
      "            \"Bluetooth connectivity\",\n",
      "            \"LED taillights\"\n",
      "        ],\n",
      "        \"dimensions\": {\n",
      "            \"length\": \"175.6 inches\",\n",
      "            \"width\": \"68.7 inches\",\n",
      "            \"height\": \"58.7 inches\",\n",
      "            \"wheelbase\": \"106.3 inches\"\n",
      "        },\n",
      "        \"weight\": \"3,042 lbs\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prompt Caching\n",
    "\n",
    "Prompt caching is a powerful feature in Amazon Bedrock that significantly reduces response latency for workloads with repetitive contexts.\n",
    "\n",
    "### What is Prompt Caching?\n",
    "\n",
    "Prompt caching allows you to store portions of your conversation context, enabling models to:\n",
    "- Reuse cached context instead of reprocessing inputs\n",
    "- Reduce response Time-To-First-Token (TTFT) for subsequent queries\n",
    "\n",
    "### When to Use Prompt Caching\n",
    "\n",
    "Prompt caching delivers maximum benefits for:\n",
    "- **Chat with Document**: By caching the document as input context on the first request, each user query becomes more efficient, perhaps enabling simpler architectures that avoid heavier solutions like vector databases.\n",
    "- **Coding assistants**: Reusing long code files in prompts enables near real-time inline suggestions, eliminating much of the time spent reprocessing code files.\n",
    "- **Agentic workflows**: Longer system prompts can be used to refine agent behavior without degrading the end-user experience. By caching the system prompts and complex tool definitions, the time to process each step in the agentic flow can be reduced.\n",
    "- **Few-Shot Learning**: Including numerous high-quality examples and complex instructions, such as for customer service or technical troubleshooting, can benefit from prompt caching.\n",
    "\n",
    "### Benefits of Prompt Caching\n",
    "\n",
    "- **Faster Response Times**: Avoid reprocessing the same context repeatedly\n",
    "- **Improved User Experience**: Reduced TTFT to create more natural conversations\n",
    "- **Cost Efficiency**: Potentially lower token usage by avoiding redundant processing\n",
    "\n",
    "### Model Support\n",
    "Be advised that the prompt caching feature is model-specific. You should review the [supported models](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-caching.html#prompt-caching-models) and details on the minimum number of tokens per cache checkpoint and maximum number of cache checkpoints per request\n",
    "\n",
    "### How it Works\n",
    "\n",
    "Prompt caching works by storing the context of the conversation in a cache. When a new request is made, the model checks if the context is already in the cache. If it is, the model uses the cached context instead of reprocessing the input.\n",
    "\n",
    "As we saw earlier, a prompt is an array of messages. A message can be marked as a cache checkpoint. Once a message is marked as a cache checkpoint,\n",
    "The **entire section of the prompt preceding the checkpoint then becomes the cached prompt prefix**.\n",
    "\n",
    "Now when the model processes a new request, that includes the all the prompt, it will not process the messages that are part of the cached prompt prefix, it will only need to process the messages that follow the checkpoint.\n",
    "\n",
    "\n",
    "![Prompt Caching](../resources/images/prompt-caching-aws-bedrock.png)\n",
    "\n",
    "The following diagram illustrates how cache hits work. A, B, C, D represent distinct portions of the prompt. A, B and C are marked as the prompt prefix. Cache hits occur when subsequent requests contain the same A, B, C prompt prefix.\n",
    "\n",
    "![Prompt Caching](../resources/images/cache-prompt-prefix.png)\n",
    "### References\n",
    "* [Amazon Bedrock: Prompt Caching](https://aws.amazon.com/blogs/machine-learning/effectively-use-prompt-caching-on-amazon-bedrock/)\n",
    "\n"
   ],
   "id": "89e2e84312e10f7e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Important:</b> Execute the following cell to make sure you have the latest version of the SDK (boto3 min version: boto3-1.37.24.)\n",
    "</div>"
   ],
   "id": "11f02c665c7f8b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T11:40:40.920133Z",
     "start_time": "2025-05-15T11:40:40.915349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Before we start lets make sure we have the latest version of the SDK (boto3 min version: boto3-1.37.24.)\n",
    "import boto3\n",
    "boto3_version = boto3.__version__\n",
    "print(f\"boto3 version: {boto3_version}\")\n",
    "if boto3_version < \"1.37.24\":\n",
    "    print(\"Updating your boto3 version to latest version.\")\n",
    "    !pip install --upgrade boto3\n",
    "    print(\"boto3 was updated to the latest version.\")\n",
    "    print(\"Please restart the kernel and re-run the notebook.\")\n",
    "    exit(1)\n",
    "else:\n",
    "    print(\"boto3 version is up to date.\")\n"
   ],
   "id": "c3c6a476592e6447",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boto3 version: 1.38.15\n",
      "boto3 version is up to date.\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Prompt Caching Example\n",
    "In the following example we will create a **Terms of Use Analyzer**  that can answer questions about the terms of use of a specific website. </br>\n",
    "Since the terms of use are long, we will use the prompt caching feature to cache the terms of use and only process the user question. </br>\n"
   ],
   "id": "ce241a518e0238e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T11:43:17.819062Z",
     "start_time": "2025-05-15T11:43:17.813151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "def chat_with_document(document, user_query, model_id):\n",
    "\n",
    "    instructions_terms_of_use_analyzer = \"\"\"\n",
    "    I will provide you with the Terms of Use document of a specific website, followed by a question about its content. Your task is to analyze the Terms of Use, extract relevant information, and provide a comprehensive answer to the question. Please follow these detailed instructions:\n",
    "\n",
    "    1. Identifying Relevant Clauses (Quotes):\n",
    "       - Carefully read through the entire Terms of Use document.\n",
    "       - Identify sections or clauses of the text that are directly relevant to answering the question.\n",
    "       - Pay special attention to definitions, user obligations, disclaimers of warranties, limitations of liability, governing law, dispute resolution, and any clauses related to the specific query.\n",
    "       - Select quotes that provide key information, context, or support for the answer. These are typically specific clauses or parts of clauses.\n",
    "       - Quotes should be concise and to the point, ideally no more than 2-4 sentences each, capturing the core of the relevant legal statement.\n",
    "       - Choose a diverse range of quotes if multiple clauses address different aspects of the question.\n",
    "       - Aim to select between 2 to 5 quotes, depending on the complexity of the question and the structure of the Terms of Use.\n",
    "\n",
    "    2. Presenting the Relevant Clauses:\n",
    "       - List the selected quotes under the heading 'Relevant clauses:'\n",
    "       - Number each quote sequentially, starting from [1].\n",
    "       - Present each quote exactly as it appears in the original text, enclosed in quotation marks.\n",
    "       - If no relevant clauses can be found to directly answer the question, write 'No directly relevant clauses found' instead.\n",
    "       - Example format:\n",
    "         Relevant clauses:\n",
    "         [1] \"Users agree not to use the service for any illegal or unauthorized purpose.\"\n",
    "         [2] \"The Company reserves the right to terminate your access to the Service at any time, without notice, for any reason whatsoever.\"\n",
    "\n",
    "    3. Formulating the Answer:\n",
    "       - Begin your answer with the heading 'Answer:' on a new line after the quotes.\n",
    "       - Provide a clear, concise, and accurate answer to the question based on the information in the Terms of Use.\n",
    "       - Ensure your answer is comprehensive and addresses all aspects of the question.\n",
    "       - Use information from the quoted clauses to support your answer, but rephrase and explain the implications rather than repeating them verbatim.\n",
    "       - Where possible, explain any legal jargon or complex phrasing in simpler terms, while remaining faithful to the original meaning of the clause.\n",
    "       - Maintain a logical flow and structure in your response.\n",
    "\n",
    "    4. Referencing Clauses in the Answer:\n",
    "       - Do not explicitly mention or introduce quotes in your answer (e.g., avoid phrases like 'According to clause [1]').\n",
    "       - Instead, add the bracketed number of the relevant quote at the end of each sentence or point that uses information from that clause.\n",
    "       - If a sentence or point is supported by multiple clauses, include all relevant quote numbers.\n",
    "       - Example: 'The website prohibits users from engaging in unlawful activities. [1] Furthermore, the platform can suspend user accounts without prior notification if terms are violated. [2]'\n",
    "\n",
    "    5. Handling Ambiguity or Lack of Specific Information:\n",
    "       - If the Terms of Use do not contain enough specific information to fully answer the question, or if a clause is ambiguous, clearly state this in your answer.\n",
    "       - Provide any partial information that is available, and explain what aspects are not explicitly covered or remain unclear.\n",
    "       - If there are multiple possible interpretations of a clause relevant to the question, explain this and provide answers based on plausible interpretations if possible, noting the ambiguity.\n",
    "       - State clearly that the analysis is based *only* on the provided text and cannot infer unstated terms.\n",
    "\n",
    "    6. Maintaining Objectivity and Disclaimer:\n",
    "       - Stick to the facts and statements presented in the Terms of Use document. Do not include personal opinions, interpretations beyond the text, or external information not found in the document.\n",
    "       - Your analysis is for informational purposes only and should NOT be considered legal advice. Always recommend consulting with a legal professional for specific advice regarding Terms of Use.\n",
    "       - If the document presents one-sided or particularly restrictive terms, you can note this objectively in your answer without endorsing or refuting the legal validity or fairness of such terms.\n",
    "\n",
    "    7. Formatting and Style:\n",
    "       - Use clear paragraph breaks to separate different points or aspects of your answer.\n",
    "       - Employ bullet points or numbered lists if it helps to organize information about specific rights, obligations, or restrictions more clearly.\n",
    "       - Ensure proper grammar, punctuation, and spelling throughout your response.\n",
    "       - Maintain a professional, neutral, and informative tone throughout your answer.\n",
    "\n",
    "    8. Length and Depth:\n",
    "       - Provide an answer that is sufficiently detailed to address the question comprehensively based on the Terms of Use.\n",
    "       - However, avoid unnecessary verbosity. Aim for clarity and conciseness, focusing on the aspects most relevant to the user's query.\n",
    "       - The length of your answer should be proportional to the complexity of the question and the amount of relevant information within the Terms of Use.\n",
    "\n",
    "    9. Dealing with Complex or Multi-part Questions about Terms:\n",
    "       - For questions with multiple parts (e.g., 'What are the user's rights regarding data privacy and what is the process for account termination?'), address each part separately and clearly.\n",
    "       - Use subheadings or numbered points to break down your answer if necessary, ensuring each component of the query is addressed.\n",
    "\n",
    "    10. Concluding the Answer:\n",
    "        - If appropriate, provide a brief summary of the key findings from the Terms of Use related to the question.\n",
    "        - Reiterate that the information is based solely on the provided document and is not legal advice. If the question implies seeking guidance (e.g., 'Should I be concerned about X clause?'), frame the answer by explaining what the clause means according to the text, rather than advising on concern.\n",
    "\n",
    "    Remember, your goal is to provide a clear, accurate, and well-supported analysis based solely on the content of the given Terms of Use document. Adhere to these instructions carefully to ensure a high-quality response that effectively addresses the user's query about the website's terms.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    document_content =  f\"Here is the document:  <document> {document} </document>\"\n",
    "\n",
    "    messages_body = [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': [\n",
    "                {\n",
    "                    'text': instructions_terms_of_use_analyzer\n",
    "                },\n",
    "                {\n",
    "                    'text': document_content\n",
    "                },\n",
    "                {\n",
    "                    \"cachePoint\": {\n",
    "                        \"type\": \"default\"\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    'text': user_query\n",
    "                },\n",
    "            ]\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    inference_config={\n",
    "        'maxTokens': 10000,\n",
    "        'temperature': 0,\n",
    "        'topP': 1\n",
    "    }\n",
    "    client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "\n",
    "    response = client.converse(\n",
    "        messages=messages_body,\n",
    "        modelId=model_id,\n",
    "        inferenceConfig=inference_config\n",
    "    )\n",
    "\n",
    "    output_message = response[\"output\"][\"message\"]\n",
    "    response_text = output_message[\"content\"][0][\"text\"]\n",
    "\n",
    "    print(\"Response text:\")\n",
    "    print(response_text)\n",
    "\n",
    "    print(\"Usage:\")\n",
    "    print(json.dumps(response[\"usage\"], indent=2))\n"
   ],
   "id": "f8bc4b8700b7e145",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now lets chat with the document and pay attention to the:\n",
    "* `cacheWriteInputTokens` - the number of tokens used to write the cache\n",
    "* `cacheReadInputTokens` - the number of tokens used to read the cache\n",
    "\n",
    "In the first request, the model will process the entire document and write it to the cache. </br>\n",
    "In the second request, the model will only process the user question and read the rest from the cache. </br>"
   ],
   "id": "4c6415319f8123f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T11:52:33.181228Z",
     "start_time": "2025-05-15T11:52:31.620831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "terms_of_use = requests.get('https://www.fiverr.com/legal-portal/legal-terms/terms-of-service').text\n",
    "model_id=\"amazon.nova-lite-v1:0\"\n",
    "\n",
    "print(terms_of_use)\n",
    "\n",
    "questions = [\n",
    "    'Is my information is used by 3rd parties?',\n",
    "    'Is the service is GDPR compliant?',\n",
    "]\n"
   ],
   "id": "f011f09190fdea79",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html><html lang=\"en-US\"><head><meta charset=\"utf-8\"><meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge, chrome=1\"><meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"><meta http-equiv=\"Cache-Control\" content=\"no-cache\"><link rel=\"icon shortcut\" href=\"data:image/x-icon;base64,AAABAAEAEBAAAAEAIABoBAAAFgAAACgAAAAQAAAAIAAAAAEAIAAAAAAAAAQAABMLAAATCwAAAAAAAAAAAAAAAAAAAAAAAHO/HQBzvx0Hc78dSXO/Hahzvx3kc78d+3O/Hftzvx3kc78dqHO/HUhzvx0Hc78dAAAAAAAAAAAAc78fAHO/HQBzvx0Yc78dknO/He9zvx3/c78d/3O/Hf9zvx3/c78d/3O/Hf9zvx3uc78dknO/HRhzvx0Ac78cAHO/HQBzvx0Yc78dsXO/Hf9zvx3/cr4b/3G+Gf9yvxv/c78d/3K+G/9xvhn/cr4b/3O/Hf9zvx2xc78dGHO/HQBzvx0Gc78dknO/Hf9zvx3/c78e/43LR/+c0l//hMc5/3K/G/+JyUH/nNJf/4jJP/9zvxz/c78d/3O/HZJzvx0Gc78dSXO/He1zvx3/c78d/3TAH//O6K///////67afP9wvRf/v+KY//////+74JL/cb4a/3O/Hf9zvx3tc78dSXO/Hahzvx3/c78d/3O/Hf90wB//0Omz//////+v237/b70X/8Hjm///////veGV/3G+Gv9zvx3/c78d/3O/Hadzvx3jc78d/3O/Hf9zvx3/dMAf/9Dps///////r9t+/2+9F//B45v//////73hlf9xvhr/c78d/3O/Hf9zvx3jc78d+3O/Hf9zvx3/c78d/3S/Hv/Q6bP//////6/afv9vvRb/weOb//////+94ZX/cb4a/3O/Hf9zvx3/c78d+3O/Hftzvx3/c78c/3nCJv+23on/7Pbh///////e8Mn/w+Se/+Xz1f//////veGV/3G+Gv9zvx3/c78d/3O/Hftzvx3jc78d/3K/HP98wyz/4PHN/////v//////+/35//n89f/4/PP/+fz1/7nfjv9xvhv/c78d/3O/Hf9zvx3jc78dp3O/Hf9zvx3/dcAg/4vKQ//Z7sL//////8fmpf+SzVD/kc1N/5DMS/+CxjX/c78c/3O/Hf9zvx3/c78dqHO/HUpzvx3tc78d/3O/Hf9xvhn/uN+N///////x+en/2+/F/5/TZP9vvRf/cr8b/3O/Hf9zvx3/c78d7XO/HUlzvx0Gc78dknO/Hf9zvx3/cr8c/4DFMv/E5J//5/TZ/+334v+o13P/cb4a/3O/Hf9zvx3/c78d/3O/HZJzvx0Gc78dAHO/HRhzvx2xc78d/3O/Hf9yvxz/dMAf/37ELv+AxTL/ecIm/3O/Hf9zvx3/c78d/3O/HbFzvx0Yc78dAHS/GgBzvx0Ac78dGHO/HZJzvx3uc78d/3O/Hf9yvxz/cr8c/3O/HP9zvx3/c78d73O/HZJzvx0Yc78dAHS/GgAAAAAAAAAAAHO/HQBzvx0Hc78dSXO/Hahzvx3kc78d+3O/Hftzvx3kc78dqHO/HUlzvx0Hc78dAAAAAAAAAAAA4AcAAMADAACAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIABAADAAwAA4AcAAA==\"><meta name=\"googlebot\" content=\"noindex\"><meta name=\"robots\" content=\"noindex\"><title>It needs a human touch</title><style>html{-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;height:100%}*{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:0 0;box-sizing:border-box;border-color:#fff;text-align:center}article,aside,figure,footer,header,hgroup,nav,section{display:block}nav ol,nav ul,ol,ul{list-style:none;list-style-image:none}body,main{display:flex;flex-direction:column}body{background:#fff;font:400 16px/24px Macan,Helvetica Neue,Helvetica,Arial,sans-serif;color:#74767e;vertical-align:top;min-height:100%;position:relative;justify-content:flex-start;align-items:stretch;align-content:stretch}body>header{border-bottom:1px solid #ddd;width:100%;height:80px;background:#fff url(data:image/svg+xml;base64,PHN2ZyBjbGFzcz0iZml2ZXJyLWxvZ28tc3RhdGljIiB3aWR0aD0iODkiIGhlaWdodD0iMjciIGZpbGw9Im5vbmUiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyI+PHBhdGggZD0iTTgxLjYgMTMuMWgtMy4xYy0yIDAtMy4xIDEuNS0zLjEgNC4xdjkuM2gtNlYxMy4xaC0yLjVjLTIgMC0zLjEgMS41LTMuMSA0LjF2OS4zaC02VjguMWg2djIuOGMxLTIuMiAyLjMtMi44IDQuMy0yLjhoNy4zdjIuOGMxLTIuMiAyLjMtMi44IDQuMy0yLjhoMnptLTI1LjIgNS42SDQ0Yy4zIDIuMSAxLjYgMy4yIDMuNyAzLjIgMS42IDAgMi43LS43IDMuMS0xLjhsNS4zIDEuNWMtMS4zIDMuMi00LjUgNS4xLTguNCA1LjEtNi41IDAtOS41LTUuMS05LjUtOS41IDAtNC4zIDIuNi05LjQgOS4xLTkuNCA2LjkgMCA5LjIgNS4yIDkuMiA5LjEgMCAuOSAwIDEuNC0uMSAxLjh6bS01LjctMy41Yy0uMS0xLjYtMS4zLTMtMy4zLTMtMS45IDAtMyAuOC0zLjQgM3pNMjcuOCAyNi41SDMzbDYuNi0xOC4zaC02bC0zLjIgMTAuNy0zLjItMTAuOGgtNnptLTI0LjQgMGg1LjlWMTMuMUgxNXYxMy40aDUuOVY4LjFIOS4zVjdjMC0xLjIuOS0yIDIuMi0ySDE1VjBoLTQuNEM2LjMgMCAzLjQgMi43IDMuNCA2LjZ2MS41SDB2NWgzLjR6IiBmaWxsPSIjMDAzOTEyIi8+PHBhdGggZD0iTTg1LjMgMjdjMiAwIDMuNy0xLjcgMy43LTMuN3MtMS43LTMuNy0zLjctMy43LTMuNyAxLjctMy43IDMuNyAxLjcgMy43IDMuNyAzLjd6IiBmaWxsPSIjMWRiZjczIi8+PC9zdmc+) 40px no-repeat;background-size:auto 27px}@media (max-width:1000px){body>header{background-position:50%}}main{justify-content:center;align-items:center;align-content:center;flex-grow:1;padding:1em 1em 0}main>article{margin:0 0 4em}figure,figure img{margin:1em}figure>*{max-width:100%}h1{font-size:24px;line-height:32px;color:#404145;max-width:600px}@media (max-width:1000px){h1{font-size:20px;line-height:28px}}a{text-decoration:none;color:#188652}a:hover{text-decoration:underline}code,pre{font:400 .9em/1.1em monospace}main>aside{width:600px;padding:2em 0}footer,main>aside{border-top:1px solid #dadbdd;margin-top:2em}footer img,main>aside img{height:1em;margin:0 0 0 .5em;vertical-align:middle}footer aside,main>aside aside{display:flex;justify-content:space-between;width:100%;max-width:1400px;margin:0 auto;padding:2em}@media (max-width:1000px){footer aside,main>aside aside{padding:1em 0;flex-direction:column-reverse}footer aside div,main>aside aside div{padding:1em}}footer aside *,main>aside aside *{text-align:left;vertical-align:auto}@media (max-width:1000px){footer aside *,main>aside aside *{text-align:center}}footer p,main>aside p{font:16px/150% Macan,Helvetica Neue,Helvetica,Arial,sans-serif;color:#95979d}footer a[href*=\"app.appsflyer.com\"],main>aside a[href*=\"app.appsflyer.com\"]{display:inline-block;margin:.3em 0 0 .5em}footer a[href*=\"app.appsflyer.com\"]:first-child,main>aside a[href*=\"app.appsflyer.com\"]:first-child{margin:.3em 0 0}.cf-wrapper{color:#fff;max-width:600px}.cf-wrapper h1{color:#404145;margin:1em 0 0}.cf-wrapper *{font-size:1em;color:#74767e;text-align:left}body>header{border:0}main{flex-direction:row-reverse}@media (max-height:800px),(max-width:1000px){main{flex-direction:column-reverse}}@media (max-height:800px),(min-width:1000px){*{text-align:left}}.quickfix{font-size:.85em;border:1px solid #1dbf73;border-radius:3px;position:relative;max-width:600px}.quickfix h2{position:absolute;top:-.5em;left:1em;background:#fff;padding:0 .5em;font-size:.75em;line-height:1em;font-style:italic;font-weight:700}.quickfix li{margin:.5em .5em .5em 2em;list-style-type:disc}small{font-size:.8em;padding:.5em}.details *,.quickfix li{text-align:left}@keyframes px-challenge-appear{0%,60%{max-height:0}to{max-height:200px}}.g-recaptcha{display:flex;justify-content:center}#recaptcha_widget>div{margin:0 auto}#px-captcha>iframe{margin:0 auto;animation:px-challenge-appear .6s ease-in}.px-loader-wrapper{justify-content:space-around}</style><meta name=\"description\" content=\"Complete the task and we'll get you right back into Fiverr.\"><meta http-equiv=\"status\" content=\"403 Forbidden\"><meta property=\"og:url\" content=\"https://www.fiverr.com\"><meta property=\"og:title\" content=\"Fiverr - Freelance Services Marketplace for Businesses\"><meta property=\"og:description\" content=\"Fiverr's mission is to change how the world works together. Fiverr connects businesses with freelancers offering digital services in 500+ categories.\"><meta property=\"og:image\" content=\"https://npm-assets.fiverrcdn.com/assets/uploads/fiverr-og-logo.png\"><meta name=\"twitter:card\" content=\"summary_large_image\"><meta property=\"twitter:domain\" content=\"www.fiverr.com\"><meta property=\"twitter:url\" content=\"http://www.fiverr.com\"><meta name=\"twitter:title\" content=\"Fiverr - Freelance Services Marketplace for Businesses\"><meta name=\"twitter:description\" content=\"Fiverr's mission is to change how the world works together. Fiverr connects businesses with freelancers offering digital services in 500+ categories.\"><meta name=\"twitter:image\" content=\"https://npm-assets.fiverrcdn.com/assets/uploads/fiverr-og-logo.png\"></head><meta name=\"pxAppId\" content=\"PXK3bezZfO\"><body><header></header><main><figure><img src=\"data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMzQ4IiBoZWlnaHQ9IjI3OSIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxwYXRoIGQ9Ik0xMjguMTE3IDExMS4yODloNjUuMzQ4VjE4MWgtNjUuMzQ4di02OS43MTFaIiBzdHJva2U9IiMyMzI0MjYiIHN0cm9rZS13aWR0aD0iMS41IiBzdHJva2UtbWl0ZXJsaW1pdD0iMTAiLz4KICAgIDxwYXRoIGQ9Ik0xODAuODk1IDExMS4yODljLjIwOCAxMi4xMDgtMS4wNDkgMjIuNTU2IDAgMzQuODUyIDEuMDQ5IDEyLjI5Ni0xLjIwOSAyOC40NTMgMCAzNC44NTJNMTY2LjUyNCAxMTEuNDYzYzAgMTIuNjUyIDEuNjc1IDEzLjcyNSAxLjA0MyAyNi41OTktLjYzMyAxMi44NzUtLjEzMiAxMC43MjguMTQ2IDIwLjM4Mi4yNzggOS42NTQuMTM5IDEyLjUxMSAwIDE1LjM3NnMuMTU5IDcuMTguMTU5IDcuMThNMTUyLjc1MiAxMTEuNDYzYy40MTcgNC43ODIgMS4yNTguNTg2IDEuNDY2IDE2LjI3Ni4yMDkgMTUuNjkuMjA5IDIzLjYzIDAgMjYuNjI3LS4yMDggMi45OTctLjQ3MiA1Ljg2Mi0uNjUzIDEzLjAxNC0uMTgxIDcuMTUxLjU0OSAxMy42Mi44MTMgMTMuNjJNMTM4LjY2NiAxMTEuNDYzYy40MjQgNi45MjkuNjMyIDEzLjM2MyAxLjI1OCAyMC45MTEuNjI1IDcuNTQ5IDAgMTMuMjQ0IDAgMTkuMzA4IDAgNi4wNjUgMS4yNzggMTUuMDIyIDEuMjcxIDE5LjY3MS0uMDA3IDQuNjQ5IDAgOS42ODIgMCA5LjY4MiIgc3Ryb2tlPSIjMjMyNDI2IiBzdHJva2Utd2lkdGg9IjEuNSIgc3Ryb2tlLW1pdGVybGltaXQ9IjEwIi8+CiAgICA8cGF0aCBkPSJNMzEyLjQ0NiAyNThoLTMwMSIgc3Ryb2tlPSIjMjMyNDI2IiBzdHJva2Utd2lkdGg9IjEuNSIgc3Ryb2tlLWxpbmVqb2luPSJiZXZlbCIvPgogICAgPHBhdGggZD0iTTc4LjM2MyAyNTcuODQxYy0xMy4wMTQgMC0yMy41NjQtMTAuNTgzLTIzLjU2NC0yMy42MzdzMTAuNTUtMjMuNjM2IDIzLjU2NC0yMy42MzYgMjMuNTY0IDEwLjU4MiAyMy41NjQgMjMuNjM2YzAgMTMuMDU0LTEwLjU1IDIzLjYzNy0yMy41NjQgMjMuNjM3WiIgZmlsbD0iIzFEQkY3MyIvPgogICAgPHBhdGggZD0iTTEzNC42NTYgMTkwLjIxNWMtNS40MzQgMTYuMjQ4LTI1LjUwMiAyNS41ODEtNDYuNTU4IDIxLjA1Ny0yMS4wNTUtNC41MjQtMzUuNTY0LTIxLjI4Ny0zMy44OS0zOC4zMzdsODAuNDQ4IDE3LjI4WiIgc3Ryb2tlPSIjMjMyNDI2IiBzdHJva2Utd2lkdGg9IjEuNSIgc3Ryb2tlLW1pdGVybGltaXQ9IjEwIi8+CiAgICA8cGF0aCBkPSJtMTk0Ljk1MiAxOTkuMDA0IDQuNTI0LTIzLjMzLTIxLjk2NSAyMC4yNTYtNC41ODctMTQuOTMtMTguNTQ2IDE2LjkyNEwxMzEuMzA3IDE4MWwxMi41NDMgMjkuNTY4TDExNS43IDIyMy40bDI5Ljk4NSAyLjc2OC03LjAxOSAzMS43NzEgMjMuNTQzLTIzLjczNSAxNi4yMTkgMjMuNzM1IDUuODE2LTI1LjMzMSAyOS4zNzMgMTguNzMtMTYuODMtMzEuODY5IDIyLjc3OC0xNy4zLTI0LjYxMy0zLjE2NVoiIHN0cm9rZT0iIzIzMjQyNiIgc3Ryb2tlLXdpZHRoPSIxLjUiIHN0cm9rZS1saW5lam9pbj0iYmV2ZWwiLz4KICAgIDxwYXRoIGQ9Ik0yMjkuMjA0IDE4NS40MjZoNDEuNzlsMjAuODg5IDM2LjMwMS0yMC44ODkgMzYuMjk1aC00MS43OWwtMjAuODg5LTM2LjI5NSAyMC44ODktMzYuMzAxWiIgZmlsbD0iIzIzMjQyNiIgc3Ryb2tlPSIjMjMyNDI2IiBzdHJva2Utd2lkdGg9IjEuNSIgc3Ryb2tlLWxpbmVqb2luPSJiZXZlbCIvPgogICAgPHBhdGggZD0iTTI0MS4zMTUgMTU3LjMyOHMtMTguODI1IDE3LjAzNi0zLjIxNyAyNi40ODhjMTUuNjA3IDkuNDUyIDQwLjM5NC0yMS40MDYgMzYuMDM3LTM2LjE2Mi00LjM1Ny0xNC43NTctMjYuNDA2IDAtMjYuNDA2IDBzMTMuMzA3LTE3LjAzNiA0LjU4Ni0zMS43NjRjLTguNzIxLTE0LjcyOS0yMi4wMzUgMy42OC0yMi4wMzUgMy42OHM5LjE4Ny0yOS4wMDQtMTUuMTQyLTI3Ljg4MWMtMjQuMzI4IDEuMTIyLTI1LjQ3NCAzMi45MjEtMTUuNjA3IDQwLjA1OCA5Ljg2OCA3LjEzOCAyMC4wMiAyLjMgMjAuMDIgMi4zcy0xNC4wNTggMTcuNzI2LTIuMTE5IDI3LjYyNGMxMS45MzggOS44OTggMjMuODgzLTQuMzQzIDIzLjg4My00LjM0M1pNMTQxLjQ4NiA1NS41MDJhMzEuNTczIDMxLjU3MyAwIDAgMC0yLjc0MSA0Mi4wMiAzMS41NzQgMzEuNTc0IDAgMSAwIDE0LjU1OC00OS4zMjgiIHN0cm9rZT0iIzIzMjQyNiIgc3Ryb2tlLXdpZHRoPSIxLjUiIHN0cm9rZS1saW5lam9pbj0iYmV2ZWwiLz4KICAgIDxwYXRoIGQ9Ik0xNDkuMDU1IDYwLjk3N2MtNC4xMTQgMC03LjQ1LTMuMzQ1LTcuNDUtNy40NzIgMC00LjEyNyAzLjMzNi03LjQ3MyA3LjQ1LTcuNDczIDQuMTE0IDAgNy40NDkgMy4zNDYgNy40NDkgNy40NzNzLTMuMzM1IDcuNDcyLTcuNDQ5IDcuNDcyWiIgc3Ryb2tlPSIjMjMyNDI2IiBzdHJva2Utd2lkdGg9IjEuNSIgc3Ryb2tlLW1pdGVybGltaXQ9IjEwIi8+CiAgICA8cGF0aCBkPSJNMTU2LjQwNiA1NC42NTVzLTguMjQ4IDAtMTIuNjE5LTYuNDM0YzEuMjIxLTEuMjM5IDQuNzM1LTMuMjg5IDkuMDI1LTEuNTczIiBzdHJva2U9IiMyMzI0MjYiIHN0cm9rZS13aWR0aD0iMS41IiBzdHJva2UtbWl0ZXJsaW1pdD0iMTAiLz4KICAgIDxwYXRoIGQ9Ik0xNTIuNjQgNDYuMzY3czIuODkxLTI4LjU0NCAxMy44OTgtMjUuMzIzYzMuNDMzIDEuMDEgMy41NjUgNS4wMTggMS44MDcgOC41MTctMy44NzggNy43MS0xNS43MDUgMTYuODA2LTE1LjcwNSAxNi44MDZaIiBmaWxsPSIjMjMyNDI2Ii8+CiAgICA8cGF0aCBkPSJNMTkxLjI0OSA2MC4zOThjLTMuNTU4LTQuODMtMTcuMjk2LTEuNTk2LTIyLjQyNCA0LjM4NWwtMy44MjItMy44MDYtMTUuOTQ4IDEzLjEzMiAzLjEyNyA0LjAwMXMtMTEuNzA5IDYuNTEtMTEuNDggMTMuNTcxdjEuNjUyTDEyOC4xNDYgODAuNjRsLTEyLjQxOCAxMS43NzNzMTAuMDU1IDIyLjMwNSAxOS45MjkgMjguNDMyYzkuODc1IDYuMTI3IDE5LjczNS0zLjMzOCAyMS4zNDEtNS40MDkgMS42MDUtMi4wNyAzLjgyMi00Ljg3OSAzLjgyMi00Ljg3OXMxNy4xODQgMS43NjQgMjcuNzQtMTEuMTUzYzEwLjU1NS0xMi45MTYgOS41MzQtMjkuNyAyLjY4OS0zOS4wMDZaIiBmaWxsPSIjMURCRjczIi8+CiAgICA8cmVjdCB4PSItLjc1IiB5PSIuNzUiIHdpZHRoPSIxMTMuMjQzIiBoZWlnaHQ9IjM2LjAzMyIgcng9IjE4LjAxNiIgdHJhbnNmb3JtPSJtYXRyaXgoLTEgMCAwIDEgMTIzLjk2NyAzOC42MDUpIiBzdHJva2U9IiMyMzI0MjYiIHN0cm9rZS13aWR0aD0iMS41Ii8+CiAgICA8cGF0aCBkPSJNMTQzLjY5NyA0Ny43MmMyLjU3NCA1LjE0OCA5LjY1MSA2Ljc5MiAxMi44NjggNi45Ny0uMTc4LTEuOTY1LTEuMTc5LTYuMzI2LTMuNzUzLTguMDQyLTIuNTc0LTEuNzE2LTcuMTQ5IDAtOS4xMTUgMS4wNzJaTTQ2LjQ2NiA3NS4wNjVWMzkuNjc3SDI2LjYyOGMtOS43NzMgMC0xNy42OTQgNy45MjItMTcuNjk0IDE3LjY5NCAwIDkuNzcyIDcuOTIxIDE3LjY5NCAxNy42OTQgMTcuNjk0aDE5LjgzOFoiIGZpbGw9IiMyMzI0MjYiLz4KICAgIDxwYXRoIGQ9Ik0xMjguNzI5IDgyLjM5Yy0yLjM0OC0yLjcwNS0xMS45MjQtMTUuNDE5LTEzLjUyOS0xNy43MTktMS42MDUtMi4zLTYuNTM5LTguNTczLTguMDMzLTguMTEzLTEuNDk0LjQ2Ljg2MiA0LjYgMy4zMjIgOC4yNjcgMi40NTkgMy42NjYgMy42NzYgNS4xNzkgMy42NzYgNS4xNzkiIGZpbGw9IiNmZmYiLz4KICAgIDxwYXRoIGQ9Ik0xMjguNzI5IDgyLjM5Yy0yLjM0OC0yLjcwNS0xMS45MjQtMTUuNDE5LTEzLjUyOS0xNy43MTktMS42MDUtMi4zLTYuNTM5LTguNTczLTguMDMzLTguMTEzLTEuNDk0LjQ2Ljg2MiA0LjYgMy4zMjIgOC4yNjcgMi40NTkgMy42NjYgMy42NzYgNS4xNzkgMy42NzYgNS4xNzkiIHN0cm9rZT0iIzIzMjQyNiIgc3Ryb2tlLXdpZHRoPSIxLjUiIHN0cm9rZS1taXRlcmxpbWl0PSIxMCIvPgogICAgPHBhdGggZD0iTTExMC45ODEgNjUuNTdzLTcuMTk5IDMuNDE2LTYuMjU0IDYuMTgzYy45NDUgMi43NjcgMi4zIDcuMTMgNC4wMTcgNy45NCAxLjcxNi44MDggMS43NzguODA4IDIuODY5IDIuNTkyIDEuMDkxIDEuNzg1IDUuMzM3IDEwLjM1OCA1LjMzNyAxMC4zNTgiIGZpbGw9IiNmZmYiLz4KICAgIDxwYXRoIGQ9Ik0xMTAuOTgxIDY1LjU3cy03LjE5OSAzLjQxNi02LjI1NCA2LjE4M2MuOTQ1IDIuNzY3IDIuMyA3LjEzIDQuMDE3IDcuOTQgMS43MTYuODA4IDEuNzc4LjgwOCAyLjg2OSAyLjU5MiAxLjA5MSAxLjc4NSA1LjMzNyAxMC4zNTggNS4zMzcgMTAuMzU4IiBzdHJva2U9IiMyMzI0MjYiIHN0cm9rZS13aWR0aD0iMS41IiBzdHJva2UtbWl0ZXJsaW1pdD0iMTAiLz4KICAgIDxwYXRoIGQ9Ik0xMTYuNzQyIDcwLjU0cy0yLjY2MS0uMjg2LTUuMDcyLS4zNDFjLTIuNDEyLS4wNTYtNC44NjUuOTItNC45NDEgMi4xMjYtLjA3NyAxLjIwNSAyLjQ4MSAxLjQ3NyA0LjM3MSAxLjY3MiAyLjc3OS4yODYgNi40MjEgMi4wOTIgNi40MjEgMi4wOTIiIGZpbGw9IiNmZmYiLz4KICAgIDxwYXRoIGQ9Ik0xMTYuNzQyIDcwLjU0cy0yLjY2MS0uMjg2LTUuMDcyLS4zNDFjLTIuNDEyLS4wNTYtNC44NjUuOTItNC45NDEgMi4xMjYtLjA3NyAxLjIwNSAyLjQ4MSAxLjQ3NyA0LjM3MSAxLjY3MiAyLjc3OS4yODYgNi40MjEgMi4wOTIgNi40MjEgMi4wOTIiIHN0cm9rZT0iIzIzMjQyNiIgc3Ryb2tlLXdpZHRoPSIxLjUiIHN0cm9rZS1taXRlcmxpbWl0PSIxMCIvPgogICAgPHBhdGggZD0ibTY4LjE4MyAxNjQuMDExIDMuMzgyLTUuOTkgOS40NDcgNS42MTEgMi45OTgtNS4zMDctOS40MzMtNS41ODIgMy4zODItNS45OTEgOS40MzYgNS41ODggMTIuNDk2LTIyLjExYTE2LjYwNyAxNi42MDcgMCAwIDEtNS4xMDktOS41OTggMTYuNjkgMTYuNjkgMCAwIDEgMS45Mi0xMC43MjEgMTUuOCAxNS44IDAgMCAxIDQuMDg5LTQuNzg0IDE1Ljg0NyAxNS44NDcgMCAwIDEgNS42MzYtMi44MTMgMTUuNzk4IDE1Ljc5OCAwIDAgMSA2LjI4Mi0uMzkzIDE1LjczMiAxNS43MzIgMCAwIDEgNS45MjUgMi4wOSAxNi42MDMgMTYuNjAzIDAgMCAxIDcuNTIyIDkuOTc3IDE2LjY2MiAxNi42NjIgMCAwIDEtMS41NjMgMTIuNDEzIDE1LjgxNCAxNS44MTQgMCAwIDEtOC4wOSA3LjAzNyAxNS43OTUgMTUuNzk1IDAgMCAxLTEwLjcxNS4yNjNsLTI0LjY4NCA0My42NjgtNS45MDktMy40OTYgMi40Mi00LjI4LTkuNDMyLTUuNTgyWm00NC44ODQtMzYuNzM3YTkuMTQgOS4xNCAwIDAgMCA1LjYxOC00LjM2OSA5LjYwNSA5LjYwNSAwIDAgMCAuOTAzLTcuMTU3IDkuNTcyIDkuNTcyIDAgMCAwLTQuMzM5LTUuNzUyIDkuMDk0IDkuMDk0IDAgMCAwLTcuMDM2LS45ODEgOS4xNSA5LjE1IDAgMCAwLTUuNjEzIDQuMzY4IDkuNjA1IDkuNjA1IDAgMCAwLS45MDMgNy4xNTcgOS41NzIgOS41NzIgMCAwIDAgNC4zMzkgNS43NTIgOS4wOTQgOS4wOTQgMCAwIDAgNy4wMzEuOTgyWiIgZmlsbD0iIzIzMjQyNiIvPgo8L3N2Zz4K\"></figure><section><h1 data-identifier=\"title\">It needs a human touch</h1><article><p data-identifier=\"content\">Complete the task and we'll get you right back into Fiverr.</p><br><div id=\"px-captcha\">Loading challenge</div><noscript><div style=\"position:fixed;top:0;left:0\" width=\"1\" height=\"1\"><img src=\"https://collector-pxk3bezzfo.perimeterx.net/api/v1/collector/noScript.gif?appId=PXK3bezZfO\"></div><div style=\"position:fixed;top:0;left:0\" width=\"1\" height=\"1\"><img src=\"https://collector-pxk3bezzfo.perimeterx.net/api/v1/collector/pxPixel.gif?appId=PXK3bezZfO\"></div></noscript><script>var define;\n",
      "var e;function t(e){return(t=\"function\"==typeof Symbol&&\"symbol\"==typeof Symbol.iterator?function(e){return typeof e}:function(e){return e&&\"function\"==typeof Symbol&&e.constructor===Symbol&&e!==Symbol.prototype?\"symbol\":typeof e})(e)}window.onerror=function(e,t,n,r,o){var i,a,c=(o=o||{}).stack||(new Error).stack;return null===(i=(a=navigator).sendBeacon)||void 0===i||i.call(a,\"https://relay.fiverr.com/v3/relay\",JSON.stringify({context:{page:{url:document.location.href,name:\"challenge-page\"}},log:[{level:\"error\",pipeline:\"UNHANDLED_ERRORS_PIPELINE\",message:e,file:t,line:n,column:r,stack:c,name:o.name}]})),!1},parcelRequire=function(n,r,o,i){var a,c=\"function\"==typeof parcelRequire&&parcelRequire,l=\"function\"==typeof require&&require;function u(e,t){if(!r[e]){if(!n[e]){var o=\"function\"==typeof parcelRequire&&parcelRequire;if(!t&&o)return o(e,!0);if(c)return c(e,!0);if(l&&\"string\"==typeof e)return l(e);var i=new Error(\"Cannot find module '\"+e+\"'\");throw i.code=\"MODULE_NOT_FOUND\",i}f.resolve=function(t){return n[e][1][t]||t},f.cache={};var a=r[e]=new u.Module(e);n[e][0].call(a.exports,f,a,a.exports,this)}return r[e].exports;function f(e){return u(f.resolve(e))}}u.isParcelRequire=!0,u.Module=function(e){this.id=e,this.bundle=u,this.exports={}},u.modules=n,u.cache=r,u.parent=c,u.register=function(e,t){n[e]=[function(e,n){n.exports=t},{}]};for(var f=0;f<o.length;f++)try{u(o[f])}catch(n){a||(a=n)}if(o.length){var s=u(o[o.length-1]);\"object\"==(\"undefined\"==typeof exports?\"undefined\":t(exports))&&\"undefined\"!=typeof module?module.exports=s:\"function\"==typeof e&&e.amd&&e(function(){return s})}if(parcelRequire=u,a)throw a;return u}({WK3E:[function(e,t,n){\"use strict\";Object.defineProperty(n,\"__esModule\",{value:!0}),n.getMeta=function(e){var t=document.querySelector('meta[name=\"'.concat(e,'\"]'));return t&&t.getAttribute(\"content\")}},{}],et6N:[function(e,t,n){\"use strict\";Object.defineProperty(n,\"__esModule\",{value:!0}),n.getTLD=function(){var e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:\"fiverr\",t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:document.domain,n=new RegExp(\"\".concat(t,\"$\")),i=o.find(function(e){return n.test(e)});if(i)return i;var a=(null==t?void 0:t.split(\".\"))||[];return a.includes(e)?a.reduceRight(function(t,n,o){return a.length-o>4?[r]:t[0]===e?t:(t.unshift(n),t)},[]).join(\".\"):r};var r=\"fiverr.com\",o=[\"dev.fiverr.com\",r]},{}],OpPT:[function(e,t,n){\"use strict\";Object.defineProperty(n,\"__esModule\",{value:!0}),n.SEPARATOR=n.REPEAT_SOLVE_COOKIE_NAME=n.EXONERATION_EXPIRATION=void 0,n.all=function(){var e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:document.cookie;try{return Object.fromEntries(e.split(\";\").map(function(e){return e.split(\"=\").map(function(e){return e.trim()})}))}catch(t){return{}}},n.set=function(e,t,n){document.cookie=[[e,t].join(\"=\"),[\"expires\",new Date(n).toUTCString()].join(\"=\"),[\"max-age\",Math.floor((n-Date.now())/1e3)].join(\"=\"),[\"domain\",(0,r.getTLD)()].join(\"=\"),\"path=/\",\"samesite=none\",\"secure\"].join(\"; \")};var r=e(\"../getTLD\");n.EXONERATION_EXPIRATION=9e5,n.REPEAT_SOLVE_COOKIE_NAME=\"_pxcrs\",n.SEPARATOR=\"%3A\"},{\"../getTLD\":\"et6N\"}],arge:[function(e,t,n){\"use strict\";Object.defineProperty(n,\"__esModule\",{value:!0}),n.log=function(e){c&&e.length&&window.fetch&&(e.forEach(function(e){return Object.assign(e,f)}),window.fetch(c,{method:\"POST\",mode:\"cors\",credentials:\"same-origin\",headers:{\"Content-type\":\"application/json\",\"X-Requested-With\":\"XMLHttpRequest\",Accept:\"application/json\"},body:JSON.stringify({logs:e,meta:l}),keepalive:!0}))};var r=e(\"../getMeta\"),o=e(\"../cookie\");function i(e,t){return function(e){if(Array.isArray(e))return e}(e)||function(e,t){var n=null==e?null:\"undefined\"!=typeof Symbol&&e[Symbol.iterator]||e[\"@@iterator\"];if(null!=n){var r,o,i,a,c=[],l=!0,u=!1;try{if(i=(n=n.call(e)).next,0===t){if(Object(n)!==n)return;l=!1}else for(;!(l=(r=i.call(n)).done)&&(c.push(r.value),c.length!==t);l=!0);}catch(e){u=!0,o=e}finally{try{if(!l&&null!=n.return&&(a=n.return(),Object(a)!==a))return}finally{if(u)throw o}}return c}}(e,t)||function(e,t){if(e){if(\"string\"==typeof e)return a(e,t);var n={}.toString.call(e).slice(8,-1);return\"Object\"===n&&e.constructor&&(n=e.constructor.name),\"Map\"===n||\"Set\"===n?Array.from(e):\"Arguments\"===n||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n)?a(e,t):void 0}}(e,t)||function(){throw new TypeError(\"Invalid attempt to destructure non-iterable instance.\\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.\")}()}function a(e,t){(null==t||t>e.length)&&(t=e.length);for(var n=0,r=Array(t);n<t;n++)r[n]=e[n];return r}var c=(0,r.getMeta)(\"logs_api\"),l={facility:(0,r.getMeta)(\"facility\")},u=(0,o.all)(document.cookie),f=function(){try{return Object.fromEntries([[\"referrer\",document.referrer],[\"vid\",u._pxvid],[\"u_guid\",u.u_guid],[\"uuid\",void 0],[\"source\",document.location.href],[\"query\",window.location.search]].filter(function(e){var t=i(e,2);return t[0],t[1]}))}catch(r){return{}}}()},{\"../getMeta\":\"WK3E\",\"../cookie\":\"OpPT\"}],B0nX:[function(e,t,n){\"use strict\";Object.defineProperty(n,\"__esModule\",{value:!0}),n.failedToDisplayChallenge=function(e){var t,n=e.selector,o=e.limit,i=document.querySelector(n);if(!i)throw new Error(\"Failed to find px-captcha container\");var a=setTimeout(u,o);function c(){\"hidden\"===document.visibilityState&&u()}function l(){var e;clearTimeout(a),null===(e=t)||void 0===e||e.disconnect(),document.removeEventListener(\"visibilitychange\",c)}function u(){var e,t,n;if(i){null===(e=(t=window).fetch)||void 0===e||e.call(t,\"https://relay.fiverr.com/v3/pixels/count/static_pages.block_page.captcha_missing\",{keepalive:!0});var o=Math.round((null===(n=window.performance)||void 0===n?void 0:n.now())/1e3);(0,r.log)([{message:\"Failed to display challenge after a few seconds.\",seconds:o,level:\"info\",code:\"FAILED_CHALLENGE_DISPLAY\",details:{container:i.innerHTML?i.innerHTML.substring(0,500):\"[Empty]\"}}]);var a=document.createTextNode(\"Error. Failed to display challenge. Please contact support.\");try{var c=i.childNodes.item(0);c&&c.nodeType===document.TEXT_NODE&&c.replaceWith(a)}catch(u){i.appendChild(a)}l()}}(t=new MutationObserver(function(e,t){e.some(function(e){return\"childList\"===e.type})&&l()})).observe(i,{attributes:!0,childList:!0,subtree:!0}),document.addEventListener(\"visibilitychange\",c,{once:!0})};var r=e(\"../log\")},{\"../log\":\"arge\"}],PSQD:[function(e,t,n){\"use strict\";Object.defineProperty(n,\"__esModule\",{value:!0}),n.default=function(e){var t=[];setTimeout(function(){window.location.reload()});try{var n=(0,o.all)(document.cookie)[o.REPEAT_SOLVE_COOKIE_NAME];if(\"string\"==typeof n){var a=i(n.split(o.SEPARATOR).map(Number),2),c=a[0],l=a[1];c&&l&&(0,o.set)(o.REPEAT_SOLVE_COOKIE_NAME,[c+1,l].join(o.SEPARATOR),l+o.EXONERATION_EXPIRATION)}else(0,o.set)(o.REPEAT_SOLVE_COOKIE_NAME,[1,Date.now()].join(o.SEPARATOR),Date.now()+o.EXONERATION_EXPIRATION)}catch(u){setTimeout(function(){throw u})}\"function\"==typeof window.fetch&&window.fetch([\"https://relay.fiverr.com/v3/pixels/count/static_pages.block_page.\",e?\"captcha_solved\":\"captcha_failed\"].join(\"\"),{keepalive:!0}),e||t.push({message:\"Challenge result was invalid\",level:\"warn\",interval:0,code:\"CHALLENGE_FAILURE\",file:\"inCaptchaSuccess\"}),(0,r.log)(t)};var r=e(\"../log\"),o=e(\"../cookie\");function i(e,t){return function(e){if(Array.isArray(e))return e}(e)||function(e,t){var n=null==e?null:\"undefined\"!=typeof Symbol&&e[Symbol.iterator]||e[\"@@iterator\"];if(null!=n){var r,o,i,a,c=[],l=!0,u=!1;try{if(i=(n=n.call(e)).next,0===t){if(Object(n)!==n)return;l=!1}else for(;!(l=(r=i.call(n)).done)&&(c.push(r.value),c.length!==t);l=!0);}catch(e){u=!0,o=e}finally{try{if(!l&&null!=n.return&&(a=n.return(),Object(a)!==a))return}finally{if(u)throw o}}return c}}(e,t)||function(e,t){if(e){if(\"string\"==typeof e)return a(e,t);var n={}.toString.call(e).slice(8,-1);return\"Object\"===n&&e.constructor&&(n=e.constructor.name),\"Map\"===n||\"Set\"===n?Array.from(e):\"Arguments\"===n||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n)?a(e,t):void 0}}(e,t)||function(){throw new TypeError(\"Invalid attempt to destructure non-iterable instance.\\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.\")}()}function a(e,t){(null==t||t>e.length)&&(t=e.length);for(var n=0,r=Array(t);n<t;n++)r[n]=e[n];return r}},{\"../log\":\"arge\",\"../cookie\":\"OpPT\"}],dgrr:[function(e,t,n){\"use strict\";Object.defineProperty(n,\"__esModule\",{value:!0}),n.onPageView=function(){try{var e=(0,r.all)(document.cookie)[r.REPEAT_SOLVE_COOKIE_NAME];if(\"string\"==typeof e){var t=i(e.split(r.SEPARATOR).map(Number),2),n=t[0],a=t[1],c=Date.now()-a;c<r.EXONERATION_EXPIRATION&&(0,o.log)([{message:\"Revisiting block page after challenge solve: \".concat(n,\" in \").concat(Math.round(c/1e3),\" seconds.\"),level:\"warn\",interval:c,code:\"REPEAT_CHALLENGE_SOLVE\"}])}}catch(l){setTimeout(function(){throw l})}};var r=e(\"../cookie\"),o=e(\"../log\");function i(e,t){return function(e){if(Array.isArray(e))return e}(e)||function(e,t){var n=null==e?null:\"undefined\"!=typeof Symbol&&e[Symbol.iterator]||e[\"@@iterator\"];if(null!=n){var r,o,i,a,c=[],l=!0,u=!1;try{if(i=(n=n.call(e)).next,0===t){if(Object(n)!==n)return;l=!1}else for(;!(l=(r=i.call(n)).done)&&(c.push(r.value),c.length!==t);l=!0);}catch(e){u=!0,o=e}finally{try{if(!l&&null!=n.return&&(a=n.return(),Object(a)!==a))return}finally{if(u)throw o}}return c}}(e,t)||function(e,t){if(e){if(\"string\"==typeof e)return a(e,t);var n={}.toString.call(e).slice(8,-1);return\"Object\"===n&&e.constructor&&(n=e.constructor.name),\"Map\"===n||\"Set\"===n?Array.from(e):\"Arguments\"===n||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n)?a(e,t):void 0}}(e,t)||function(){throw new TypeError(\"Invalid attempt to destructure non-iterable instance.\\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.\")}()}function a(e,t){(null==t||t>e.length)&&(t=e.length);for(var n=0,r=Array(t);n<t;n++)r[n]=e[n];return r}},{\"../cookie\":\"OpPT\",\"../log\":\"arge\"}],E7Wx:[function(e,n,r){\"use strict\";Object.defineProperty(r,\"__esModule\",{value:!0}),r.cookies=void 0,Object.defineProperty(r,\"failedToDisplayChallenge\",{enumerable:!0,get:function(){return o.failedToDisplayChallenge}}),Object.defineProperty(r,\"onCaptchaSuccess\",{enumerable:!0,get:function(){return i.default}}),Object.defineProperty(r,\"onPageView\",{enumerable:!0,get:function(){return a.onPageView}});var o=e(\"./failedToDisplayChallenge\"),i=function(e){return e&&e.__esModule?e:{default:e}}(e(\"./onCaptchaSuccess\")),a=e(\"./onPageView\"),c=function(e,n){if(!n&&e&&e.__esModule)return e;if(null===e||\"object\"!=t(e)&&\"function\"!=typeof e)return{default:e};var r=l(n);if(r&&r.has(e))return r.get(e);var o={__proto__:null},i=Object.defineProperty&&Object.getOwnPropertyDescriptor;for(var a in e)if(\"default\"!==a&&{}.hasOwnProperty.call(e,a)){var c=i?Object.getOwnPropertyDescriptor(e,a):null;c&&(c.get||c.set)?Object.defineProperty(o,a,c):o[a]=e[a]}return o.default=e,r&&r.set(e,o),o}(e(\"./cookie\"));function l(e){if(\"function\"!=typeof WeakMap)return null;var t=new WeakMap,n=new WeakMap;return(l=function(e){return e?n:t})(e)}r.cookies=c},{\"./failedToDisplayChallenge\":\"B0nX\",\"./onCaptchaSuccess\":\"PSQD\",\"./onPageView\":\"dgrr\",\"./cookie\":\"OpPT\"}],QCba:[function(e,t,n){\"use strict\";var r,o=e(\"./lib/index\"),i=null===(r=document.querySelector('meta[name=\"pxAppId\"]'))||void 0===r?void 0:r.getAttribute(\"content\");window._pxOnCaptchaSuccess=o.onCaptchaSuccess,window._pxAppId=i,window._pxJsClientSrc=\"https://client.perimeterx.net/\".concat(i,\"/main.min.js\"),window._pxHostUrl=\"https://collector-\".concat(i,\".perimeterx.net\"),window._pxFirstPartyEnabled=!0,(0,o.onPageView)(),(0,o.failedToDisplayChallenge)({selector:\"#px-captcha\",limit:1e4})},{\"./lib/index\":\"E7Wx\"}]},{},[\"QCba\"]);</script><script src=\"https://captcha.px-cdn.net/PXK3bezZfO/captcha.js?a=c&amp;m=0&amp;cors=1\" crossorigin></script><br><div class=\"quickfix\" data-identifier=\"quickfix\"><h2>Quick fixes</h2><ul><li>Disable any browser extensions that could be interfering with the website. This includes Ad blockers, privacy extensions, or VPNs that may modify web traffic.</li><li>Clear your browser's cache and cookies. Outdated or corrupt cache data can cause issues with how the webpage loads and operates.</li><li>Make sure Javascript is enabled in your browser and update your browser to the latest version.</li></ul></div><noscript><p>To access Fiverr, please make sure JavaScript is enabled on your browser.</p></noscript><br><code>ERRCODE PXCR10002539</code><small class=\"details\"><p>\n",
      "Request details:\n",
      "<ul>\n",
      "<li>ip: <kbd>94.188.131.43</kbd></li>\n",
      "<li>traceId: <kbd>d11744dc86e24585b2908ef590448cef</kbd></li>\n",
      "</ul>\n",
      "</p>\n",
      "<script>\n",
      "</script></small></article></section></main><script type=\"application/json\" data-role=\"translations\">{\"en\":{\"title\":\"It needs a human touch\",\"content\":\"Complete the task and weÃ¢Â€Â™ll get you right back into Fiverr.\",\"quickfixes\":\"<h2>Quick fixes</h2><ul><li>Disable any browser extensions that could be interfering with the website. This includes Ad blockers, privacy extensions, or VPNs that may modify web traffic.</li><li>Clear your browserÃ¢Â€Â™s cache and cookies. Outdated or corrupt cache data can cause issues with how the webpage loads and operates.</li><li>Make sure Javascript is enabled in your browser and update your browser to the latest version.</li></ul>\"},\"nl\":{\"title\":\"Er is een menselijke touch nodig\",\"content\":\"Voltooi de taak en we brengen je meteen terug naar Fiverr.\",\"quickfixes\":\"<h2>Snelle oplossingen</h2><ul><li>Schakel alle browser-extensies uit die de website kunnen verstoren. Dit omvat adblockers, privacy-extensies of VPNÃ¢Â€Â™s die het webverkeer kunnen wijzigen.</li><li>Wis de browsercache en cookies. Verouderde of beschadigde cache-gegevens kunnen problemen veroorzaken met de manier waarop de webpagina wordt geladen en werkt.</li><li>Zorg ervoor dat Javascript is ingeschakeld in je browser en werk de browser bij naar de nieuwste versie.</li></ul>\"},\"de\":{\"title\":\"Ein Klick genÃƒÂ¼gt\",\"content\":\"SchlieÃƒÂŸe die Aufgabe ab und wir bringen dich direkt wieder zu Fiverr.\",\"quickfixes\":\"<h2>SofortlÃƒÂ¶sungen</h2><ul><li>Deaktiviere alle Browsererweiterungen, die die Website beeintrÃƒÂ¤chtigen kÃƒÂ¶nnten. Dazu gehÃƒÂ¶ren Anzeigenblocker, Datenschutzerweiterungen oder VPNs, die den Web-Traffic ÃƒÂ¤ndern kÃƒÂ¶nnen.</li><li>LÃƒÂ¶sche den Cache und die Cookies deines Browsers. Veraltete oder beschÃƒÂ¤digte Cache-Daten kÃƒÂ¶nnen Probleme beim Laden und Betrieb der Webseite verursachen.</li><li>Stelle sicher, dass Javascript in deinem Browser aktiviert ist, und aktualisiere ihn auf die neueste Version.</li></ul>\"},\"it\":{\"title\":\"Ci serve il tocco di una persona in carne ed ossa.\",\"content\":\"Completa lÃ¢Â€Â™attivitÃƒÂ  e ti riporteremo su Fiverr.\",\"quickfixes\":\"<h2>Soluzioni rapide</h2><ul><li>Disattiva tutte le estensioni del Browser che potrebbero interferire con il sito web. CiÃƒÂ² include ad blocker, estensioni per la privacy o VPN che possono modificare il traffico web.</li><li>Svuota la cache e i cookie del tuo Browser. I dati della cache obsoleti o corrotti possono causare problemi con il caricamento e il funzionamento della pagina web.</li><li>Assicurati che Javascript sia abilitato nel tuo Browser e aggiorna il tuo Browser allÃ¢Â€Â™ultima versione.</li></ul>\"},\"es\":{\"title\":\"Prueba que no eres un robot\",\"content\":\"Completa el trabajo para poder volver a Fiverr.\",\"quickfixes\":\"<h2>Arreglos rÃƒÂ¡pidos</h2><ul><li>Desactiva las extensiones del navegador que puedan interferir con el sitio web. Esto incluye bloqueadores de anuncios, extensiones de privacidad o VPN que pueden modificar el trÃƒÂ¡fico web.</li><li>Borra la memoria cachÃƒÂ© y las cookies del navegador. Los datos de cachÃƒÂ© obsoletos o corruptos pueden causar problemas en cÃƒÂ³mo se carga y funciona la pÃƒÂ¡gina web.</li><li>AsegÃƒÂºrate de que Javascript estÃƒÂ© habilitado en el navegador y actualiza el navegador a la ÃƒÂºltima versiÃƒÂ³n.</li></ul>\"},\"fr\":{\"title\":\"Montrez nous que vous nÃ¢Â€Â™ÃƒÂªtes pas un robot.\",\"content\":\"Terminez la tÃƒÂ¢che et nous vous dirigerons directement sur Fiverr.\",\"quickfixes\":\"<h2>Solutions rapides</h2><ul><li> DÃƒÂ©sactivez toutes les extensions de navigateur qui pourraient interfÃƒÂ©rer avec le site web. Cela inclut les bloqueurs de publicitÃƒÂ©s, les extensions de confidentialitÃƒÂ© ou les VPN qui peuvent modifier le trafic Web.</li><li>Videz le cache et les cookies de votre navigateur. Des donnÃƒÂ©es de cache obsolÃƒÂ¨tes ou corrompues peuvent entraÃƒÂ®ner des problÃƒÂ¨mes de chargement et de fonctionnement de la page Web.</li><li>Assurez-vous que Javascript est activÃƒÂ© dans votre navigateur et mettez ÃƒÂ  jour votre navigateur avec la derniÃƒÂ¨re version.</li></ul>\"},\"pt\":{\"title\":\"DÃƒÂª seu toque pessoal\",\"content\":\"Conclua a tarefa, e nÃƒÂ³s o redirecionaremos de volta ÃƒÂ  Fiverr.\",\"quickfixes\":\"<h2>CorreÃƒÂ§ÃƒÂµes rÃƒÂ¡pidas</h2><ul><li>Desative quaisquer extensÃƒÂµes do navegador que possam estar interferindo no site. Isso inclui bloqueadores de anÃƒÂºncios, extensÃƒÂµes de privacidade ou VPNs que podem modificar o trÃƒÂ¡fego da web.</li><li>Limpe o cache e os cookies de seu navegador. Dados de cache desatualizados ou corrompidos podem causar problemas no carregamento e operaÃƒÂ§ÃƒÂ£o da pÃƒÂ¡gina.</li><li>Certifique-se de que o Javascript esteja habilitado em seu navegador e atualize-o para a versÃƒÂ£o mais recente.</li></ul>\"}}</script><script>function t(t,e){return i(t)||o(t,e)||n(t,e)||r()}function r(){throw new TypeError(\"Invalid attempt to destructure non-iterable instance.\\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.\")}function n(t,r){if(t){if(\"string\"==typeof t)return e(t,r);var n={}.toString.call(t).slice(8,-1);return\"Object\"===n&&t.constructor&&(n=t.constructor.name),\"Map\"===n||\"Set\"===n?Array.from(t):\"Arguments\"===n||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n)?e(t,r):void 0}}function e(t,r){(null==r||r>t.length)&&(r=t.length);for(var n=0,e=Array(r);n<r;n++)e[n]=t[n];return e}function o(t,r){var n=null==t?null:\"undefined\"!=typeof Symbol&&t[Symbol.iterator]||t[\"@@iterator\"];if(null!=n){var e,o,i,a,u=[],c=!0,l=!1;try{if(i=(n=n.call(t)).next,0===r){if(Object(n)!==n)return;c=!1}else for(;!(c=(e=i.call(n)).done)&&(u.push(e.value),u.length!==r);c=!0);}catch(t){l=!0,o=t}finally{try{if(!c&&null!=n.return&&(a=n.return(),Object(a)!==a))return}finally{if(l)throw o}}return u}}function i(t){if(Array.isArray(t))return t}!function(){Array.from(document.querySelectorAll('script[data-role=\"translations\"]')).forEach(function(r){var n,e=function(t){try{return JSON.parse(t)}catch(r){return}}(r.textContent)||{};function o(t){var r=t.split(\"-\").shift();return document.querySelector(\"html\").setAttribute(\"lang\",t),!!e[r]&&(Object.entries(e[r]).forEach(i),!0)}function i(r){var n=t(r,3),e=n[0],o=n[1],i=n[2],a=void 0===i?document.querySelector('[data-identifier=\"'.concat(e,'\"]')):i;a&&o&&a.innerHTML.trim()!==o&&(a.innerHTML=o)}(null===(n=document.domain)||void 0===n?void 0:n.replace(/.fiverr.com$/,\"\").split(\".\")).some(o)||(o(navigator.language),fetch(\"https://httpbin.org/headers\").then(function(t){return t.json()}).then(function(t){return t.headers[\"Accept-Language\"]}).then(function(t){return null==t?void 0:t.split(\",\").shift()}).then(o).catch(function(){return\"nothing\"}))})}();</script></body></html>\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T11:52:39.789629Z",
     "start_time": "2025-05-15T11:52:34.842678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chat_with_document(\n",
    "    document=terms_of_use,\n",
    "    user_query=questions[0],\n",
    "    model_id=model_id\n",
    ")"
   ],
   "id": "e1034cf621c4cfaa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response text:\n",
      "Relevant clauses:\n",
      "[1] \"The website does not share user information with third parties without explicit consent, except as required by law or to protect the rights, property, or safety of the company or others.\"\n",
      "\n",
      "Answer:\n",
      "Based on the provided Terms of Use, the website explicitly states that it does not share user information with third parties without explicit consent. [1] This means that your personal information is generally kept confidential and is not disclosed to external entities unless you have given your permission, or unless the disclosure is legally mandated or necessary to protect the rights, property, or safety of the company or others. It's important to note that this clause is a general statement and does not cover all possible scenarios or specific practices that might be detailed in other parts of the Terms of Use or in the website's privacy policy. For a comprehensive understanding of how your data is used and shared, including with third parties, it would be advisable to review the website's privacy policy, if available.\n",
      "Usage:\n",
      "{\n",
      "  \"inputTokens\": 15,\n",
      "  \"outputTokens\": 200,\n",
      "  \"totalTokens\": 17481,\n",
      "  \"cacheReadInputTokens\": 0,\n",
      "  \"cacheWriteInputTokens\": 17266\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T11:53:12.616608Z",
     "start_time": "2025-05-15T11:53:08.216488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chat_with_document(\n",
    "    document=terms_of_use,\n",
    "    user_query=questions[1],\n",
    "    model_id=model_id\n",
    ")"
   ],
   "id": "5870c4d0cc1d8a71",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response text:\n",
      "Relevant clauses:\n",
      "[1] \"This document does not contain any specific information regarding GDPR compliance.\"\n",
      "\n",
      "Answer:\n",
      "Based on the provided Terms of Use document, there is no explicit mention of GDPR compliance. The document does not provide any information about how the website handles personal data, user rights under GDPR, or any specific measures taken to ensure compliance with the General Data Protection Regulation [1]. \n",
      "\n",
      "If GDPR compliance is a concern, it would be advisable to review the website's privacy policy or contact the website's support team for detailed information on their data protection practices and GDPR compliance status. It is important to note that the absence of information in the Terms of Use does not necessarily mean non-compliance, but rather that the specific details regarding GDPR are not addressed in this document.\n",
      "Usage:\n",
      "{\n",
      "  \"inputTokens\": 10,\n",
      "  \"outputTokens\": 157,\n",
      "  \"totalTokens\": 17434,\n",
      "  \"cacheReadInputTokens\": 17267,\n",
      "  \"cacheWriteInputTokens\": 0\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 35
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
